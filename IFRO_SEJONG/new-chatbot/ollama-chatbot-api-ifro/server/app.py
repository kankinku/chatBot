try:
    from fastapi import FastAPI
    from pydantic import BaseModel
    FASTAPI_AVAILABLE = True
except Exception:
    FASTAPI_AVAILABLE = False

import json
import os
import logging
from datetime import datetime
from pathlib import Path
from typing import List

from unifiedpdf.config import PipelineConfig
from unifiedpdf.facade import UnifiedPDFPipeline
from unifiedpdf.types import Chunk

# ë¡œê¹… ì„¤ì • - Docker Desktopì—ì„œ í™•ì¸í•˜ê¸° ì‰½ë„ë¡ ê°œì„ 
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('logs/chatbot_conversations.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ì§ˆë¬¸/ë‹µë³€ ì „ìš© ë¡œê±° ìƒì„±
qa_logger = logging.getLogger('qa_conversations')
qa_handler = logging.FileHandler('logs/qa_detailed.log', encoding='utf-8')
qa_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
qa_logger.addHandler(qa_handler)
qa_logger.setLevel(logging.INFO)

# uvicorn access ë¡œê·¸ ë ˆë²¨ ì¡°ì • (health check ë¡œê·¸ ì¤„ì´ê¸°)
import logging
uvicorn_logger = logging.getLogger("uvicorn.access")
uvicorn_logger.setLevel(logging.ERROR)  # ERROR ë ˆë²¨ë¡œ ì„¤ì •í•˜ì—¬ health check ë¡œê·¸ ì™„ì „íˆ ì œê±°

# uvicorn ë¡œê±°ë„ ì¡°ì •
uvicorn_main_logger = logging.getLogger("uvicorn")
uvicorn_main_logger.setLevel(logging.WARNING)

def log_conversation(question: str, answer: str, confidence: float, sources: list, metrics: dict):
    """ì±„íŒ… ëŒ€í™”ë¥¼ ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡ - Docker Desktopì—ì„œ í™•ì¸í•˜ê¸° ì‰½ë„ë¡ ê°œì„ """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # ìƒì„¸í•œ ì§ˆë¬¸/ë‹µë³€ ë¡œê·¸ (qa_detailed.log)
    qa_logger.info("=" * 80)
    qa_logger.info(f"ğŸ¤– ì§ˆë¬¸: {question}")
    qa_logger.info(f"âœ… ë‹µë³€: {answer}")
    qa_logger.info(f"ğŸ“Š ì‹ ë¢°ë„: {confidence:.2f} | ì†ŒìŠ¤ ìˆ˜: {len(sources)} | Fallback: {metrics.get('fallback_used', False)}")
    qa_logger.info("=" * 80)
    
    # JSONL í˜•ì‹ìœ¼ë¡œ ë¡œê·¸ íŒŒì¼ì— ì¶”ê°€ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "answer": answer,
        "confidence": confidence,
        "sources_count": len(sources),
        "metrics": metrics,
        "sources": sources
    }
    
    log_file = Path("logs/conversations.jsonl")
    log_file.parent.mkdir(exist_ok=True)
    
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
    
    # ê°„ë‹¨í•œ ìš”ì•½ ë¡œê·¸ (chatbot_conversations.log)
    logger.info(f"ğŸ’¬ Q&A ì™„ë£Œ | ì§ˆë¬¸: {question[:50]}... | ë‹µë³€ê¸¸ì´: {len(answer)} | ì‹ ë¢°ë„: {confidence:.2f}")


def _load_corpus(path: str) -> List[Chunk]:
    p = Path(path)
    chunks: List[Chunk] = []
    if not p.exists():
        return chunks
    with p.open("r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            obj = json.loads(line)
            chunks.append(
                Chunk(
                    doc_id=obj.get("doc_id", obj.get("filename", "doc")),
                    filename=obj.get("filename", "doc"),
                    page=obj.get("page"),
                    start_offset=int(obj.get("start", 0)),
                    length=int(obj.get("length", len(obj.get("text", "")))),
                    text=obj.get("text", ""),
                    extra=obj.get("extra", {}),
                )
            )
    return chunks


if FASTAPI_AVAILABLE:
    app = FastAPI()
    cfg = PipelineConfig()
    corpus_path = str(Path("data/corpus_v1.jsonl"))
    pipe = UnifiedPDFPipeline(_load_corpus(corpus_path), cfg)
    _warmed = False
    # Simple in-memory aggregator
    AGG = {"requests_total": 0, "no_answer_total": 0}

    class AskRequest(BaseModel):
        question: str
        mode: str = "accuracy"
        k: str = "auto"
    class BatchRequest(BaseModel):
        items: list
        mode: str = "accuracy"

    @app.get("/healthz")
    def healthz():
        # health checkëŠ” ë¡œê·¸ ì¶œë ¥í•˜ì§€ ì•ŠìŒ (ë„ˆë¬´ ë§ì´ ë‚˜ì™€ì„œ)
        return {"status": "ok", "warmed": _warmed}

    @app.get("/status")
    def status():
        """AI ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸"""
        # Ollama ëª¨ë¸ ìƒíƒœë„ í™•ì¸
        model_status = "unknown"
        try:
            import urllib.request
            import json
            ollama_host = os.getenv('OLLAMA_HOST', 'ollama')
            url = f"http://{ollama_host}:11434/api/tags"
            with urllib.request.urlopen(url, timeout=5) as resp:
                data = json.loads(resp.read().decode("utf-8"))
                models = data.get("models", [])
                model_name = cfg.model_name
                if any(m.get("name") == model_name for m in models):
                    model_status = "available"
                else:
                    model_status = "not_found"
        except Exception:
            model_status = "error"
            
        return {
            "model_loaded": _warmed,
            "total_pdfs": len(pipe.corpus) if hasattr(pipe, 'corpus') else 0,
            "total_chunks": len(pipe.corpus) if hasattr(pipe, 'corpus') else 0,
            "ai_available": _warmed,
            "warmed": _warmed,
            "model_status": model_status,
            "model_name": cfg.model_name
        }

    @app.post("/api/ask")
    def api_ask(req: AskRequest):
        try:
            # ì§ˆë¬¸ ìˆ˜ì‹  ë¡œê·¸ - Docker Desktopì—ì„œ ëª…í™•íˆ ë³´ì´ë„ë¡ ê°œì„ 
            logger.info(f"ğŸ“¥ ì§ˆë¬¸ ìˆ˜ì‹  | ëª¨ë“œ: {req.mode} | ê¸¸ì´: {len(req.question)}ì")
            logger.info(f"ğŸ“ ì§ˆë¬¸ ë‚´ìš©: {req.question}")
            
            res = pipe.ask(req.question, mode=req.mode)
            AGG["requests_total"] += 1
            AGG["no_answer_total"] += int(res.metrics.get("no_answer", 0))
            
            # ì†ŒìŠ¤ ì •ë³´ ì¤€ë¹„
            sources = [
                {
                    "filename": s.chunk.filename,
                    "page": s.chunk.page,
                    "start": s.chunk.start_offset,
                    "length": s.chunk.length,
                    "calibrated_conf": s.calibrated_conf,
                }
                for s in res.sources
            ]
            
            # ë‹µë³€ ìƒì„± ì™„ë£Œ ë¡œê·¸
            logger.info(f"ğŸ“¤ ë‹µë³€ ìƒì„± ì™„ë£Œ | ì‹ ë¢°ë„: {res.confidence:.2f} | ì†ŒìŠ¤: {len(sources)}ê°œ | Fallback: {res.fallback_used}")
            logger.info(f"ğŸ“„ ë‹µë³€ ë‚´ìš©: {res.text}")
            
            # ëŒ€í™” ë¡œê·¸ ê¸°ë¡
            log_conversation(
                question=req.question,
                answer=res.text,
                confidence=res.confidence,
                sources=sources,
                metrics=res.metrics
            )
            
            return {
                "answer": res.text,
                "confidence": res.confidence,
                "sources": sources,
                "metrics": res.metrics,
                "fallback_used": res.fallback_used,
            }
        except Exception as e:
            logger.error(f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì˜¤ë¥˜: '{req.question}' - {str(e)}")
            raise

    @app.post("/api/qa/batch")
    def api_batch(req: BatchRequest):
        out = []
        for it in req.items:
            q = it.get("question", "")
            res = pipe.ask(q, mode=req.mode)
            AGG["requests_total"] += 1
            AGG["no_answer_total"] += int(res.metrics.get("no_answer", 0))
            
            # ì†ŒìŠ¤ ì •ë³´ ì¤€ë¹„
            sources = [
                {
                    "filename": s.chunk.filename,
                    "page": s.chunk.page,
                    "start": s.chunk.start_offset,
                    "length": s.chunk.length,
                    "calibrated_conf": s.calibrated_conf,
                }
                for s in res.sources
            ]
            
            # ëŒ€í™” ë¡œê·¸ ê¸°ë¡
            log_conversation(
                question=q,
                answer=res.text,
                confidence=res.confidence,
                sources=sources,
                metrics=res.metrics
            )
            
            out.append({
                "id": it.get("id"),
                "question": q,
                "answer": res.text,
                "confidence": res.confidence,
                "metrics": res.metrics,
                "fallback_used": res.fallback_used,
            })
        return {"results": out, "config_hash": cfg.config_hash()}

    @app.get("/metrics")
    def metrics():
        # Prometheus text exposition format (very small set)
        lines = []
        lines.append(f"unifiedpdf_requests_total {AGG['requests_total']}")
        lines.append(f"unifiedpdf_no_answer_total {AGG['no_answer_total']}")
        lines.append(f"unifiedpdf_config_info{{config_hash=\"{cfg.config_hash()}\"}} 1")
        return "\n".join(lines)

    @app.on_event("startup")
    def _warm_start():
        global _warmed
        try:
            # Ollama ì—°ê²° ë° ëª¨ë¸ ìë™ Pull ê¸°ëŠ¥ ì¶”ê°€
            import urllib.request
            import json
            import time
            
            ollama_host = os.getenv('OLLAMA_HOST', 'ollama')
            
            # 1. Ollama ì„œë²„ ì—°ê²° í™•ì¸
            url = f"http://{ollama_host}:11434/api/tags"
            with urllib.request.urlopen(url, timeout=10) as resp:
                data = json.loads(resp.read().decode("utf-8"))
                models = data.get("models", [])
                
                # 2. í•„ìš”í•œ ëª¨ë¸ í™•ì¸ ë° ìë™ Pull
                model_name = cfg.model_name
                model_exists = any(m.get("name") == model_name for m in models)
                
                if not model_exists:
                    print(f"Model '{model_name}' not found. Pulling via Ollama API...")
                    # ëª¨ë¸ Pull ìš”ì²­
                    pull_url = f"http://{ollama_host}:11434/api/pull"
                    pull_data = {"name": model_name}
                    pull_req = urllib.request.Request(
                        pull_url, 
                        data=json.dumps(pull_data).encode("utf-8"),
                        headers={"Content-Type": "application/json"}
                    )
                    
                    try:
                        with urllib.request.urlopen(pull_req, timeout=300) as pull_resp:
                            pull_result = json.loads(pull_resp.read().decode("utf-8"))
                            print(f"Model pull initiated: {pull_result}")
                    except Exception as pull_e:
                        print(f"Model pull failed: {pull_e}")
                
                # 3. ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸ (ìµœëŒ€ 60ì´ˆ ëŒ€ê¸°)
                for attempt in range(30):  # 30ë²ˆ ì‹œë„ (2ì´ˆ ê°„ê²©)
                    try:
                        with urllib.request.urlopen(url, timeout=10) as resp:
                            data = json.loads(resp.read().decode("utf-8"))
                            models = data.get("models", [])
                            if any(m.get("name") == model_name for m in models):
                                print(f"Model '{model_name}' is now available.")
                                break
                    except Exception:
                        pass
                    time.sleep(2)
                else:
                    print(f"Model '{model_name}' not available after pull attempt.")
                    _warmed = False
                    return
                
                # 4. ëª¨ë¸ ì›œì—… (ì‹¤ì œ ë©”ëª¨ë¦¬ ë¡œë”© ë° keep_alive ì„¤ì •)
                print(f"Warming up model '{model_name}'...")
                warmup_url = f"http://{ollama_host}:11434/api/generate"
                warmup_data = {
                    "model": model_name,
                    "prompt": "Hello",  # ê°„ë‹¨í•œ ì›œì—… í”„ë¡¬í”„íŠ¸
                    "stream": False,
                    "keep_alive": "24h",  # 24ì‹œê°„ ë™ì•ˆ ë©”ëª¨ë¦¬ì— ìœ ì§€
                    "options": {
                        "temperature": 0.0,
                        "num_predict": 1  # ìµœì†Œí•œì˜ í† í°ë§Œ ìƒì„±
                    }
                }
                
                try:
                    warmup_req = urllib.request.Request(
                        warmup_url,
                        data=json.dumps(warmup_data).encode("utf-8"),
                        headers={"Content-Type": "application/json"}
                    )
                    with urllib.request.urlopen(warmup_req, timeout=60) as warmup_resp:
                        warmup_result = json.loads(warmup_resp.read().decode("utf-8"))
                        print(f"Model warmup completed: {warmup_result.get('response', '')[:50]}...")
                        print(f"Model '{model_name}' is now loaded in memory and will stay warm for 24h")
                        _warmed = True
                except Exception as warmup_e:
                    print(f"Model warmup failed: {warmup_e}")
                    _warmed = False
                    
        except Exception as e:
            print(f"Warm start failed: {e}")
            _warmed = False
else:
    app = None  # Placeholder; FastAPI not installed
