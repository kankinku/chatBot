"""
ë²¤ì¹˜ë§ˆí¬ ì „ì— ëª‡ ê°œ ìƒ˜í”Œë§Œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import json
from rag_query import RAGSystem
from evaluation_metrics import EvaluationMetrics


def test_sample_questions(num_samples=3):
    """ìƒ˜í”Œ ì§ˆë¬¸ìœ¼ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    
    # QA ë°ì´í„° ë¡œë“œ
    with open("qa.json", 'r', encoding='utf-8') as f:
        qa_data = json.load(f)
    
    # ì²˜ìŒ num_samplesê°œë§Œ ì„ íƒ
    sample_data = qa_data[:num_samples]
    
    print("=" * 80)
    print(f"ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ({num_samples}ê°œ ì§ˆë¬¸)")
    print("=" * 80)
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print("\nRAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    rag = RAGSystem()
    evaluator = EvaluationMetrics()
    
    # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
    for idx, qa in enumerate(sample_data):
        question = qa['question']
        expected_answer = qa['answer']
        keywords = qa.get('accepted_keywords', [])
        
        print(f"\n{'='*80}")
        print(f"[{idx + 1}/{len(sample_data)}] ì§ˆë¬¸: {question}")
        print(f"{'='*80}")
        
        try:
            answer, docs, metas = rag.query(question, top_k=3)
            
            # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
            contexts = docs if docs else []
            
            # í‰ê°€ ìˆ˜í–‰
            eval_results = evaluator.evaluate_answer(answer, expected_answer, keywords, contexts)
            
            print(f"\nðŸ“Š í‰ê°€ ì ìˆ˜:")
            print(f"  ê¸°ë³¸ Score (v5):      {eval_results['basic_score']*100:6.1f}%")
            print(f"  ë„ë©”ì¸ íŠ¹í™”:          {eval_results['domain_score']*100:6.1f}%")
            print(f"  RAG ì¢…í•©:             {eval_results['rag_overall']*100:6.1f}%")
            print(f"\n  ìƒì„¸:")
            print(f"    í‚¤ì›Œë“œ ì •í™•ë„:      {eval_results['keyword']['accuracy']*100:6.1f}%")
            print(f"    í† í° F1:            {eval_results['token_overlap']['f1']*100:6.1f}%")
            print(f"    ìˆ«ìž ì •í™•ë„:        {eval_results['numeric']['accuracy']*100:6.1f}%")
            print(f"    ë‹¨ìœ„ ì •í™•ë„:        {eval_results['unit']['accuracy']*100:6.1f}%")
            print(f"    Faithfulness:       {eval_results['faithfulness']*100:6.1f}%")
            print(f"    Answer Correctness: {eval_results['answer_correctness']*100:6.1f}%")
            print(f"    Context Precision:  {eval_results['context_precision']*100:6.1f}%")
            print(f"    ROUGE-L:            {eval_results['text_similarity']['rouge_l']*100:6.1f}%")
            
            print(f"\nìƒì„±ëœ ë‹µë³€:")
            print(f"{answer}")
            
            print(f"\nê¸°ëŒ€ ë‹µë³€:")
            print(f"{expected_answer}")
            
            print(f"\nê¸°ëŒ€ í‚¤ì›Œë“œ: {', '.join(keywords)}")
            
            if eval_results['keyword']['matched_keywords']:
                print(f"ë§¤ì¹­ëœ í‚¤ì›Œë“œ: {', '.join(eval_results['keyword']['matched_keywords'])}")
            else:
                print("ë§¤ì¹­ëœ í‚¤ì›Œë“œ: ì—†ìŒ")
            
        except Exception as e:
            print(f"\nì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    print(f"\n{'='*80}")
    print("ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("ì „ì²´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´: python benchmark.py")
    print("=" * 80)


if __name__ == "__main__":
    import sys
    
    num_samples = 3
    if len(sys.argv) > 1:
        try:
            num_samples = int(sys.argv[1])
        except ValueError:
            print("ì‚¬ìš©ë²•: python test_sample.py [ìƒ˜í”Œ ìˆ˜]")
            sys.exit(1)
    
    test_sample_questions(num_samples)

