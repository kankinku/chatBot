"""
qa.jsonì˜ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì‚¬ìš©í•œ RAG ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬
ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œë¥¼ í†µí•´ ì¢…í•©ì ì¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""
import json
import time
from datetime import datetime
from rag_query import RAGSystem
from evaluation_metrics import EvaluationMetrics


def load_qa_data(qa_file="qa.json"):
    """QA ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    with open(qa_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def run_benchmark(rag_system, qa_data, output_file="benchmark_results.json"):
    """ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    print("=" * 80)
    print("RAG ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    print("=" * 80)
    print(f"ì´ {len(qa_data)}ê°œì˜ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n")
    
    results = []
    total_time = 0
    evaluator = EvaluationMetrics()
    
    for idx, qa in enumerate(qa_data):
        question_id = qa['id']
        question = qa['question']
        expected_answer = qa['answer']
        keywords = qa.get('accepted_keywords', [])
        
        print(f"\n[{idx + 1}/{len(qa_data)}] ID: {question_id}")
        print(f"ì§ˆë¬¸: {question}")
        
        # ì‹œê°„ ì¸¡ì • ì‹œì‘
        start_time = time.time()
        
        try:
            # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë‹µë³€ ìƒì„±
            generated_answer, docs, metas = rag_system.query(question, top_k=3)
            elapsed_time = time.time() - start_time
            
            # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê²€ìƒ‰ëœ ë¬¸ì„œë“¤)
            contexts = docs if docs else []
            
            # ì¢…í•© í‰ê°€ ìˆ˜í–‰
            eval_results = evaluator.evaluate_answer(
                generated_answer,
                expected_answer,
                keywords,
                contexts
            )
            
            print(f"ë‹µë³€ ìƒì„± ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
            print(f"ê¸°ë³¸ Score: {eval_results['basic_score']*100:.1f}%")
            print(f"ë„ë©”ì¸ íŠ¹í™”: {eval_results['domain_score']*100:.1f}%")
            print(f"RAG ì¢…í•©: {eval_results['rag_overall']*100:.1f}%")
            print(f"  - Faithfulness: {eval_results['faithfulness']*100:.1f}%")
            print(f"  - Answer Correctness: {eval_results['answer_correctness']*100:.1f}%")
            
            result = {
                "id": question_id,
                "question": question,
                "expected_answer": expected_answer,
                "generated_answer": generated_answer,
                "keywords": keywords,
                "elapsed_time": elapsed_time,
                "retrieved_sources": [meta['source'] for meta in metas],
                "success": True,
                
                # í‰ê°€ ì§€í‘œ
                "evaluation": {
                    # ì¢…í•© ì ìˆ˜
                    "basic_score": eval_results['basic_score'],
                    "domain_score": eval_results['domain_score'],
                    "rag_overall": eval_results['rag_overall'],
                    
                    # í‚¤ì›Œë“œ
                    "keyword_accuracy": eval_results['keyword']['accuracy'],
                    "keyword_matched": eval_results['keyword']['matched_keywords'],
                    
                    # í† í°
                    "token_f1": eval_results['token_overlap']['f1'],
                    "token_precision": eval_results['token_overlap']['precision'],
                    "token_recall": eval_results['token_overlap']['recall'],
                    
                    # ìˆ«ì & ë‹¨ìœ„
                    "numeric_accuracy": eval_results['numeric']['accuracy'],
                    "numeric_matched": eval_results['numeric']['matched_numbers'],
                    "unit_accuracy": eval_results['unit']['accuracy'],
                    "unit_matched": eval_results['unit']['matched_units'],
                    
                    # RAG ì§€í‘œ
                    "faithfulness": eval_results['faithfulness'],
                    "answer_correctness": eval_results['answer_correctness'],
                    "context_precision": eval_results['context_precision'],
                    
                    # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„
                    "bleu_2": eval_results['text_similarity']['bleu_2'],
                    "rouge_l": eval_results['text_similarity']['rouge_l'],
                    "exact_match": eval_results['text_similarity']['exact_match'],
                    "contains_match": eval_results['text_similarity']['contains_match']
                }
            }
            
            total_time += elapsed_time
            
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            elapsed_time = time.time() - start_time
            
            result = {
                "id": question_id,
                "question": question,
                "expected_answer": expected_answer,
                "generated_answer": None,
                "keywords": keywords,
                "elapsed_time": elapsed_time,
                "retrieved_sources": [],
                "success": False,
                "error": str(e),
                "evaluation": {
                    "basic_score": 0.0,
                    "domain_score": 0.0,
                    "rag_overall": 0.0,
                    "keyword_accuracy": 0.0,
                    "token_f1": 0.0,
                    "numeric_accuracy": 0.0,
                    "unit_accuracy": 0.0,
                    "faithfulness": 0.0,
                    "answer_correctness": 0.0,
                    "context_precision": 0.0
                }
            }
            
            total_time += elapsed_time
        
        results.append(result)
    
    # ì „ì²´ í†µê³„ ê³„ì‚°
    valid_results = [r for r in results if r['success']]
    success_count = len(valid_results)
    success_rate = (success_count / len(qa_data) * 100) if qa_data else 0
    avg_time = total_time / len(qa_data) if qa_data else 0
    
    # í‰ê°€ ì§€í‘œ í‰ê·  ê³„ì‚°
    if valid_results:
        avg_basic = sum(r['evaluation']['basic_score'] for r in valid_results) / len(valid_results)
        avg_domain = sum(r['evaluation']['domain_score'] for r in valid_results) / len(valid_results)
        avg_rag = sum(r['evaluation']['rag_overall'] for r in valid_results) / len(valid_results)
        
        avg_keyword = sum(r['evaluation']['keyword_accuracy'] for r in valid_results) / len(valid_results)
        avg_token_f1 = sum(r['evaluation']['token_f1'] for r in valid_results) / len(valid_results)
        avg_numeric = sum(r['evaluation']['numeric_accuracy'] for r in valid_results) / len(valid_results)
        avg_unit = sum(r['evaluation']['unit_accuracy'] for r in valid_results) / len(valid_results)
        
        avg_faithfulness = sum(r['evaluation']['faithfulness'] for r in valid_results) / len(valid_results)
        avg_correctness = sum(r['evaluation']['answer_correctness'] for r in valid_results) / len(valid_results)
        avg_context_prec = sum(r['evaluation']['context_precision'] for r in valid_results) / len(valid_results)
        
        avg_bleu = sum(r['evaluation']['bleu_2'] for r in valid_results) / len(valid_results)
        avg_rouge = sum(r['evaluation']['rouge_l'] for r in valid_results) / len(valid_results)
    else:
        avg_basic = avg_domain = avg_rag = 0.0
        avg_keyword = avg_token_f1 = avg_numeric = avg_unit = 0.0
        avg_faithfulness = avg_correctness = avg_context_prec = 0.0
        avg_bleu = avg_rouge = 0.0
    
    summary = {
        "timestamp": datetime.now().isoformat(),
        "total_questions": len(qa_data),
        "successful_answers": success_count,
        "failed_answers": len(qa_data) - success_count,
        "success_rate": success_rate,
        "total_time": total_time,
        "average_time_per_question": avg_time,
        
        # í‰ê°€ ì§€í‘œ í‰ê· 
        "average_scores": {
            # ì¢…í•© ì ìˆ˜
            "basic_score": avg_basic,
            "domain_score": avg_domain,
            "rag_overall": avg_rag,
            
            # ìƒì„¸ ì§€í‘œ
            "keyword_accuracy": avg_keyword,
            "token_f1": avg_token_f1,
            "numeric_accuracy": avg_numeric,
            "unit_accuracy": avg_unit,
            
            # RAG ì§€í‘œ
            "faithfulness": avg_faithfulness,
            "answer_correctness": avg_correctness,
            "context_precision": avg_context_prec,
            
            # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„
            "bleu_2": avg_bleu,
            "rouge_l": avg_rouge
        }
    }
    
    # ê²°ê³¼ ì €ì¥
    output_data = {
        "summary": summary,
        "results": results
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    # ìš”ì•½ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)
    print(f"\nğŸ“Š ê¸°ë³¸ ì •ë³´")
    print(f"  ì´ ì§ˆë¬¸ ìˆ˜: {len(qa_data)}ê°œ")
    print(f"  ì„±ê³µ: {success_count}ê°œ | ì‹¤íŒ¨: {len(qa_data) - success_count}ê°œ")
    print(f"  ì„±ê³µë¥ : {success_rate:.1f}%")
    
    print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½:")
    print(f"\n1ï¸âƒ£  ê¸°ë³¸ Score (v5):        {avg_basic*100:6.1f}%")
    print(f"2ï¸âƒ£  ë„ë©”ì¸ íŠ¹í™” ì¢…í•©:        {avg_domain*100:6.1f}%")
    print(f"    - ìˆ«ì ì •í™•ë„:          {avg_numeric*100:6.1f}%")
    print(f"    - ë‹¨ìœ„ ì •í™•ë„:          {avg_unit*100:6.1f}%")
    print(f"3ï¸âƒ£  RAG í•µì‹¬ ì§€í‘œ:")
    print(f"    - Faithfulness:         {avg_faithfulness*100:6.1f}%")
    print(f"    - Answer Correctness:   {avg_correctness*100:6.1f}%")
    print(f"    - Context Precision:    {avg_context_prec*100:6.1f}%")
    print(f"4ï¸âƒ£  í•™ìˆ  í‘œì¤€:")
    print(f"    - Token F1:             {avg_token_f1*100:6.1f}%")
    print(f"    - ROUGE-L:              {avg_rouge*100:6.1f}%")
    
    print(f"\nâ±ï¸  ì„±ëŠ¥")
    print(f"  ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"  í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_time:.2f}ì´ˆ")
    print(f"\nê²°ê³¼ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("=" * 80)
    
    return output_data


def main():
    # QA ë°ì´í„° ë¡œë“œ
    print("QA ë°ì´í„° ë¡œë“œ ì¤‘...")
    qa_data = load_qa_data("qa.json")
    print(f"{len(qa_data)}ê°œì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n")
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    rag = RAGSystem()
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    results = run_benchmark(rag, qa_data)
    
    print("\në²¤ì¹˜ë§ˆí¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()

