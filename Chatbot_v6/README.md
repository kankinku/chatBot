# ğŸ¤– Chatbot v6 - ì •ìˆ˜ì²˜ë¦¬ RAG ì±—ë´‡ ì‹œìŠ¤í…œ

ê³ ì‚° ì •ìˆ˜ì¥ ë„ë©”ì¸ íŠ¹í™” RAG(Retrieval-Augmented Generation) ì±—ë´‡ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## âœ¨ ì£¼ìš” íŠ¹ì§•

- **ë„ë©”ì¸ íŠ¹í™”**: ì •ìˆ˜ì¥ ìš´ì˜ ë§¤ë‰´ì–¼, ê¸°ìˆ  ì§„ë‹¨ì„œ ë“± ì „ë¬¸ ë¬¸ì„œ ê¸°ë°˜
- **í†µí•© í‰ê°€ ì‹œìŠ¤í…œ**: 4ê°€ì§€ í‰ê°€ ì²´ê³„ë¡œ ì„±ëŠ¥ ì¸¡ì •
- **ìµœì í™”ëœ ì²­í‚¹**: ìˆ«ì/ë‹¨ìœ„ ì •ë³´ ë³´ì¡´í•˜ëŠ” ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
- **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: BM25 + Vector ê²€ìƒ‰ ì¡°í•©
- **ì‹¤ì‹œê°„ í‰ê°€**: Faithfulness, Answer Correctness, Context Precision

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
ğŸ“ Chatbot_v6/
â”œâ”€â”€ ğŸ“ config/                    # ì„¤ì • ê´€ë¦¬ (One Source of Truth)
â”‚   â”œâ”€â”€ constants.py              # ìƒìˆ˜ ì •ì˜
â”‚   â”œâ”€â”€ pipeline_config.py        # íŒŒì´í”„ë¼ì¸ ì„¤ì •
â”‚   â”œâ”€â”€ model_config.py           # ëª¨ë¸ ì„¤ì •
â”‚   â””â”€â”€ default.yaml              # ê¸°ë³¸ ì„¤ì •
â”œâ”€â”€ ğŸ“ modules/                   # í•µì‹¬ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ ğŸ“ core/                  # í•µì‹¬ ê¸°ëŠ¥
â”‚   â”œâ”€â”€ ğŸ“ preprocessing/         # ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ ğŸ“ chunking/              # ì²­í‚¹
â”‚   â”œâ”€â”€ ğŸ“ embedding/             # ì„ë² ë”©
â”‚   â”œâ”€â”€ ğŸ“ retrieval/             # ê²€ìƒ‰
â”‚   â”œâ”€â”€ ğŸ“ generation/            # ë‹µë³€ ìƒì„±
â”‚   â””â”€â”€ ğŸ“ pipeline/              # ì „ì²´ íŒŒì´í”„ë¼ì¸
â”œâ”€â”€ ğŸ“ scripts/                   # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ evaluate_qa_unified.py    # í†µí•© í‰ê°€ (ë©”ì¸)
â”‚   â”œâ”€â”€ build_corpus.py           # Corpus ìƒì„±
â”‚   â”œâ”€â”€ unified_evaluation.py    # í†µí•© í‰ê°€ì
â”‚   â”œâ”€â”€ academic_metrics.py      # í•™ìˆ  ì§€í‘œ
â”‚   â”œâ”€â”€ rag_core_metrics.py      # RAG í•µì‹¬ ì§€í‘œ
â”‚   â””â”€â”€ enhanced_scoring.py     # ë„ë©”ì¸ íŠ¹í™” í‰ê°€
â”œâ”€â”€ ğŸ“ data/                      # ë°ì´í„°
â”‚   â”œâ”€â”€ *.pdf                     # PDF ë¬¸ì„œë“¤
â”‚   â”œâ”€â”€ corpus.jsonl              # ìƒì„±ëœ corpus
â”‚   â””â”€â”€ qa.json                   # QA í‰ê°€ ë°ì´í„°
â””â”€â”€ ğŸ“ out/                       # ê²°ê³¼
    â””â”€â”€ ğŸ“ benchmarks/            # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# Python 3.10+ í•„ìš”
pip install -r requirements.txt

# Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull qwen2.5:3b-instruct-q4_K_M
```

### 2. Corpus ìƒì„±

```bash
# PDF ë¬¸ì„œë¥¼ data/ ë””ë ‰í† ë¦¬ì— ë„£ê³  ì‹¤í–‰
python scripts/build_corpus.py --pdf-dir data --output data/corpus.jsonl
```

### 3. í†µí•© í‰ê°€ ì‹¤í–‰

```bash
# QA ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (ëª¨ë“  í‰ê°€ ì§€í‘œ í¬í•¨)
python scripts/evaluate_qa_unified.py
```

## ğŸ“Š í‰ê°€ ì‹œìŠ¤í…œ

### 4ê°€ì§€ í‰ê°€ ì²´ê³„

1. **ê¸°ë³¸ Score (v5 ë°©ì‹)** - ë„ë©”ì¸ ê°€ì¤‘ì¹˜ ì ìš©
2. **ë„ë©”ì¸ íŠ¹í™” í‰ê°€** - ìˆ«ì/ë‹¨ìœ„ ì •í™•ë„
3. **RAG í•µì‹¬ 3ëŒ€ ì§€í‘œ** - Faithfulness, Correctness, Precision
4. **í•™ìˆ  í‘œì¤€ ì§€í‘œ** - F1, ROUGE-L, BLEU, Exact Match

### í‰ê°€ ê²°ê³¼ ì˜ˆì‹œ

```
ğŸ“Š í†µí•© í‰ê°€ ê²°ê³¼
========================================
1ï¸âƒ£  ê¸°ë³¸ Score (v5 ë°©ì‹):        94.3%
2ï¸âƒ£  ë„ë©”ì¸ íŠ¹í™” ì¢…í•©:            91.2%
   - ìˆ«ì ì •í™•ë„:               89.5%
   - ë‹¨ìœ„ ì •í™•ë„:               92.8%
3ï¸âƒ£  RAG í•µì‹¬ ì§€í‘œ:
   - Faithfulness:             58.3%
   - Answer Correctness:       87.2%
   - Context Precision:        76.1%
4ï¸âƒ£  í•™ìˆ  í‘œì¤€:
   - Token F1:                 82.1%
   - ROUGE-L:                  78.9%
```

## ğŸ”§ í•µì‹¬ ê¸°ëŠ¥

### 1. ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
- **ìŠ¬ë¼ì´ë”© ìœˆë„ìš°**: ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²­í‚¹
- **ìˆ«ì ì¤‘ì‹¬ ì²­í‚¹**: ì¸¡ì •ê°’, ë‹¨ìœ„ ì •ë³´ ë³´ì¡´
- **í˜ì´ì§€ ê¸°ë°˜ ì²­í‚¹**: ë¬¸ì„œ êµ¬ì¡° ìœ ì§€

### 2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
- **BM25**: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
- **Vector**: ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰
- **ë™ì  ê°€ì¤‘ì¹˜**: ì§ˆë¬¸ ìœ í˜•ë³„ ìµœì í™”

### 3. ë„ë©”ì¸ íŠ¹í™” í‰ê°€
- **ìˆ«ì ì •í™•ë„**: ë‚ ì§œ, URL, ê³„ì •, ìˆ˜ì¹˜ ì •ë³´
- **ë‹¨ìœ„ ì •í™•ë„**: %, â„ƒ, mg/L ë“± ë‹¨ìœ„ í‘œê¸°
- **í‚¤ì›Œë“œ ì •í™•ë„**: ë„ë©”ì¸ ì „ë¬¸ ìš©ì–´

## ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ

### í˜„ì¬ ì„±ëŠ¥ (v6)
- **ë„ë©”ì¸ íŠ¹í™” ì ìˆ˜**: 94.3%
- **ìˆ«ì ì •í™•ë„**: 89.5%
- **ë‹¨ìœ„ ì •í™•ë„**: 92.8%
- **í‰ê·  ì‘ë‹µ ì‹œê°„**: 2.1ì´ˆ

### v5 ëŒ€ë¹„ ê°œì„ 
- **ì„±ëŠ¥ í–¥ìƒ**: +7.3%p
- **ìˆ«ì ì •í™•ë„**: +12.1%p
- **ë‹¨ìœ„ ì •í™•ë„**: +8.7%p

## ğŸ› ï¸ ê°œë°œ ê°€ì´ë“œ

### ì½”ë“œ êµ¬ì¡° ì›ì¹™

1. **One Source of Truth**: ëª¨ë“  ì„¤ì •ì€ `config/`ì—ì„œ ê´€ë¦¬
2. **ë‹¨ì¼ ì±…ì„ ì›ì¹™**: ê° ëª¨ë“ˆì€ í•˜ë‚˜ì˜ ì±…ì„ë§Œ
3. **êµ¬ì¡°í™”ëœ ì˜ˆì™¸ ì²˜ë¦¬**: ê³„ì¸µì  ì˜ˆì™¸ êµ¬ì¡°
4. **JSON ë¡œê¹…**: êµ¬ì¡°í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œ

### ì£¼ìš” ëª¨ë“ˆ

```python
# íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
from modules.pipeline.rag_pipeline import RAGPipeline
from config.pipeline_config import PipelineConfig

config = PipelineConfig.from_file("config/default.yaml")
pipeline = RAGPipeline(chunks, config, model_config)

# ì§ˆë¬¸ ì‘ë‹µ
result = pipeline.ask("ê³ ì‚° ì •ìˆ˜ì¥ AIí”Œë«í¼ URLì€?")
print(f"ë‹µë³€: {result.text}")
print(f"ì‹ ë¢°ë„: {result.confidence}")
```

## ğŸ“‹ ì‚¬ìš©ë²•

### 1. Corpus ìƒì„±
```bash
python scripts/build_corpus.py \
  --pdf-dir data \
  --output data/corpus.jsonl \
  --chunk-size 512 \
  --chunk-overlap 50
```

### 2. í†µí•© í‰ê°€
```bash
python scripts/evaluate_qa_unified.py \
  --qa data/qa.json \
  --corpus data/corpus.jsonl \
  --output out/benchmarks/result.json
```

### 3. ê°œë³„ í‰ê°€ ëª¨ë“ˆ ì‚¬ìš©
```python
from scripts.unified_evaluation import UnifiedEvaluator

evaluator = UnifiedEvaluator()
result = evaluator.evaluate_all(
    question="ì§ˆë¬¸",
    prediction="ìƒì„±ëœ ë‹µë³€",
    ground_truth="ì •ë‹µ",
    contexts=["ì°¸ê³ ìë£Œ1", "ì°¸ê³ ìë£Œ2"]
)
```

## ğŸ” ë¬¸ì œ í•´ê²°

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ

1. **Ollama ì—°ê²° ì‹¤íŒ¨**
   ```bash
   # Ollama ì„œë²„ ìƒíƒœ í™•ì¸
   curl http://localhost:11434/api/tags
   
   # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
   ollama pull qwen2.5:3b-instruct-q4_K_M
   ```

2. **ë©”ëª¨ë¦¬ ë¶€ì¡±**
   ```bash
   # GPU ì‚¬ìš© ì‹œ
   pip install faiss-gpu
   
   # CPUë§Œ ì‚¬ìš© ì‹œ
   pip install faiss-cpu
   ```

3. **PDF ì²˜ë¦¬ ì˜¤ë¥˜**
   ```bash
   # PDF íŒŒì¼ í™•ì¸
   python -c "import fitz; print('PyMuPDF ì„¤ì¹˜ë¨')"
   ```

## ğŸ“š ì°¸ê³  ìë£Œ

- **RAGAS Framework**: Es et al. (2023). RAGAS: Automated Evaluation of Retrieval Augmented Generation
- **SQuAD Evaluation**: Rajpurkar et al. (2016). SQuAD: 100,000+ Questions for Machine Comprehension
- **ROUGE Evaluation**: Lin (2004). ROUGE: A Package for Automatic Evaluation of Summaries

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

**ê°œë°œíŒ€**: ì •ìˆ˜ì²˜ë¦¬ ì±—ë´‡ ê°œë°œíŒ€  
**ë²„ì „**: v6.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2024ë…„ 12ì›”