"""
FastAPI Application - API ì„œë²„

RESTful API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from fastapi import FastAPI, HTTPException, Request, APIRouter
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import uuid
import time

from config.pipeline_config import PipelineConfig
from config.environment import get_env_config
from config.model_config import LLMModelConfig
from modules.core.logger import setup_logging, get_logger
from modules.core.exceptions import ChatbotException
from modules.pipeline.rag_pipeline import RAGPipeline
from modules.core.types import Chunk
from modules.analysis.question_analyzer import QuestionAnalyzer
from modules.generation.ollama_manager import OllamaManager
from modules.document.loader import DocumentLoader

# ë¡œê¹… ì„¤ì •
env_config = get_env_config()
setup_logging(
    log_dir=env_config.log_dir,
    log_level=env_config.log_level,
    log_format=env_config.log_format,
)

logger = get_logger(__name__)

# FastAPI ì•±
app = FastAPI(
    title="Chatbot v6 API",
    description="ì •ìˆ˜ì²˜ë¦¬ ì±—ë´‡ API (4ê°€ì§€ ì›ì¹™ ì¤€ìˆ˜)",
    version="6.0.0",
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” ì œí•œ í•„ìš”
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ íŒŒì´í”„ë¼ì¸ (ì´ˆê¸°í™”ëŠ” startupì—ì„œ)
pipeline: Optional[RAGPipeline] = None
question_analyzer: Optional[QuestionAnalyzer] = None

# ì¸ì‚¬ë§ ì‘ë‹µ ë¦¬ìŠ¤íŠ¸
GREETING_RESPONSES = [
    "ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹ ì •ìˆ˜ì²˜ë¦¬ AI ì±—ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
    "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ì •ìˆ˜ì²˜ë¦¬ ê´€ë ¨ ì§ˆë¬¸ì´ë‚˜ ë¬¸ì„œ ê²€ìƒ‰ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
    "ì•ˆë…•í•˜ì„¸ìš”! ğŸŒŠ ì •ìˆ˜ì¥ ìš´ì˜, ìˆ˜ì§ˆ ê´€ë¦¬, ì‹œìŠ¤í…œ ì •ë³´ ë“± ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!",
    "ë°˜ê°‘ìŠµë‹ˆë‹¤! ğŸ¤– AI ê¸°ë°˜ìœ¼ë¡œ ì •ìˆ˜ì²˜ë¦¬ ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤.",
    "ì•ˆë…•í•˜ì„¸ìš”! ğŸ’§ ì •ìˆ˜ì²˜ë¦¬ ê¸°ìˆ , ê³µì • ì •ë³´, ë§¤ë‰´ì–¼ ê²€ìƒ‰ ë“±ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤.",
]

# API ë¼ìš°í„° ìƒì„± (/api í”„ë¦¬í”½ìŠ¤ìš©)
api_router = APIRouter(prefix="/api")


# Request/Response ëª¨ë¸
class QuestionRequest(BaseModel):
    question: str
    top_k: int = 50


class Source(BaseModel):
    text: str
    score: float
    rank: int
    filename: str
    page: Optional[int] = None


class AnswerResponse(BaseModel):
    answer: str
    confidence: float
    sources: List[Source]
    metrics: Dict[str, Any]


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global pipeline, question_analyzer
    
    logger.info("API server starting up")
    
    try:
        # Ollama ì„œë²„ ì—°ê²° ë° ëª¨ë¸ ìë™ ì„¤ì¹˜
        logger.info("Checking Ollama server and model availability...")
        ollama_url = f"http://{LLMModelConfig().host}:{LLMModelConfig().port}"
        ollama_manager = OllamaManager(base_url=ollama_url)
        
        # Ollama ì„œë²„ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 60ì´ˆ)
        max_wait_time = 60
        wait_interval = 2
        waited = 0
        
        while waited < max_wait_time:
            if ollama_manager.check_ollama_running():
                logger.info("Ollama server is running")
                break
            logger.info(f"Waiting for Ollama server... ({waited}/{max_wait_time}s)")
            time.sleep(wait_interval)
            waited += wait_interval
        else:
            logger.warning("Ollama server is not responding. LLM features may not work.")
        
        # ëª¨ë¸ ìë™ ì„¤ì¹˜
        if ollama_manager.check_ollama_running():
            model_config = LLMModelConfig()
            model_name = model_config.model_name
            logger.info(f"Ensuring model availability: {model_name}")
            
            if ollama_manager.ensure_model_available(model_name):
                logger.info(f"Model {model_name} is ready")
            else:
                logger.warning(f"Failed to ensure model {model_name}. LLM features may not work.")
        
        # ì„¤ì • ë¡œë“œ
        config_path = project_root / "config" / "default.yaml"
        pipeline_config = PipelineConfig.from_file(config_path)
        
        # ë„ë©”ì¸ ì‚¬ì „ ê²½ë¡œ ì„¤ì •
        domain_dict_path = project_root / "data" / "domain_dictionary.json"
        
        # QuestionAnalyzer ì´ˆê¸°í™”
        question_analyzer = QuestionAnalyzer(
            domain_dict_path=str(domain_dict_path) if domain_dict_path.exists() else None
        )
        
        # PDF ë¬¸ì„œ ìë™ ë¡œë“œ ë° ì„ë² ë”©
        data_dir = project_root / "data"
        chunks = []
        
        try:
            logger.info(f"Loading documents from: {data_dir}")
            doc_loader = DocumentLoader(str(data_dir))
            
            # PDF íŒŒì¼ í™•ì¸
            pdf_files = list(data_dir.glob("*.pdf")) + list(data_dir.glob("*.PDF"))
            
            if pdf_files:
                logger.info(f"Found {len(pdf_files)} PDF file(s): {[f.name for f in pdf_files]}")
                try:
                    chunks = doc_loader.load_from_directory(use_cache=True)
                    logger.info(f"Loaded {len(chunks)} chunks from {len(pdf_files)} document(s)")
                except Exception as e:
                    logger.error(f"Failed to load documents: {e}", exc_info=True)
                    logger.warning("Falling back to dummy chunks")
                    chunks = []
            else:
                logger.warning(f"No PDF files found in {data_dir}")
        
        except Exception as e:
            logger.error(f"Document loading error: {e}", exc_info=True)
            logger.warning("Falling back to dummy chunks")
            chunks = []
        
        # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ë”ë¯¸ ì²­í¬ ì‚¬ìš©
        if not chunks:
            logger.warning("No chunks loaded, using dummy chunks")
            chunks = [
                Chunk(
                    doc_id="demo",
                    filename="demo.pdf",
                    page=1,
                    start_offset=0,
                    length=100,
                    text="ë¬¸ì„œê°€ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ data/ ë””ë ‰í† ë¦¬ì— ì¶”ê°€í•´ì£¼ì„¸ìš”.",
                ),
            ]
        
        # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ìë™ ì„ë² ë”© í¬í•¨)
        logger.info("Initializing RAG pipeline with embedding...")
        pipeline = RAGPipeline(
            chunks=chunks,
            pipeline_config=pipeline_config,
        )
        
        logger.info(f"API server started successfully with {len(chunks)} chunks")
    
    except Exception as e:
        logger.error(f"Failed to initialize pipeline: {e}", exc_info=True)
        raise


@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œ ì‹œ ì •ë¦¬"""
    logger.info("API server shutting down")


@app.get("/healthz")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "service": "chatbot-v6",
        "version": "6.0.0",
    }


@app.get("/status")
async def get_status():
    """AI ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    if not pipeline:
        return {
            "status": "initializing",
            "ai_available": False,
            "model_loaded": False,
            "total_pdfs": 0,
            "total_chunks": 0,
        }
    
    # ë¬¸ì„œ í†µê³„
    total_chunks = len(pipeline.chunks) if hasattr(pipeline, 'chunks') else 0
    unique_files = len(set(c.filename for c in pipeline.chunks)) if hasattr(pipeline, 'chunks') else 0
    
    return {
        "status": "ok",
        "ai_available": True,
        "model_loaded": True,
        "total_pdfs": unique_files,
        "total_chunks": total_chunks,
    }


# ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
@app.post("/ask", response_model=AnswerResponse)
async def ask_question(req: Request, request: QuestionRequest):
    """
    ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€
    
    Args:
        request: ì§ˆë¬¸ ìš”ì²­
        
    Returns:
        ë‹µë³€ ì‘ë‹µ
    """
    if not pipeline:
        raise HTTPException(status_code=503, detail="Pipeline not initialized")
    
    start_time = time.time()
    session_id = req.headers.get('X-Session-ID', str(uuid.uuid4()))
    
    try:
        logger.info(
            f"[QUESTION] Received question",
            extra={
                "question": request.question,
                "session_id": session_id,
                "top_k": request.top_k,
                "timestamp": time.time()
            }
        )
        
        # ì¸ì‚¬ë§ ì²´í¬
        if question_analyzer and question_analyzer.is_greeting(request.question):
            logger.info(
                f"[GREETING] Detected, returning preset response",
                extra={
                    "question": request.question,
                    "session_id": session_id
                }
            )
            
            import random
            greeting_response = random.choice(GREETING_RESPONSES)
            processing_time = time.time() - start_time
            
            # ì¸ì‚¬ë§ ì‘ë‹µ ìƒì„±
            from modules.core.types import Answer
            
            answer = Answer(
                text=greeting_response,
                confidence=1.0,
                sources=[],
                metrics={
                    "total_time_ms": int(processing_time * 1000),
                    "is_greeting": True,
                    "llm_used": False,
                }
            )
        else:
            # ì¼ë°˜ ì§ˆë¬¸ - íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            answer = pipeline.ask(
                question=request.question,
                top_k=request.top_k,
            )
            
            processing_time = time.time() - start_time
        
        # ì‘ë‹µ ë³€í™˜ (ì¸ì‚¬ë§ì€ sourcesê°€ ì—†ì„ ìˆ˜ ìˆìŒ)
        sources = [
            Source(
                text=span.chunk.text[:200],
                score=span.score,
                rank=span.rank,
                filename=span.chunk.filename,
                page=span.chunk.page,
            )
            for span in answer.sources[:5]
        ] if answer.sources else []
        
        response = AnswerResponse(
            answer=answer.text,
            confidence=answer.confidence,
            sources=sources,
            metrics={
                **answer.metrics,
                "processing_time": processing_time,
                "session_id": session_id,
            },
        )
        
        logger.info(
            f"[ANSWER] Generated successfully",
            extra={
                "question": request.question,
                "answer": answer.text[:200] + "..." if len(answer.text) > 200 else answer.text,
                "confidence": answer.confidence,
                "processing_time": processing_time,
                "session_id": session_id,
                "sources_count": len(answer.sources),
                "metrics": answer.metrics
            }
        )
        
        return response
    
    except ChatbotException as e:
        logger.error(f"Chatbot error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail={
                "error": e.to_dict(),
                "message": str(e),
            }
        )
    
    except Exception as e:
        logger.error(f"Unexpected error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# /api í”„ë¦¬í”½ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ë“¤ (í”„ë¡ì‹œ ì„œë²„ í˜¸í™˜)
@api_router.post("/ask", response_model=AnswerResponse)
async def ask_question_api(req: Request, request: QuestionRequest):
    """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ (API í”„ë¦¬í”½ìŠ¤ ë²„ì „)"""
    # ê¸°ì¡´ í•¨ìˆ˜ì™€ ë™ì¼í•œ ë¡œì§ ì¬ì‚¬ìš©
    return await ask_question(req, request)


@api_router.get("/healthz")
async def health_check_api():
    """í—¬ìŠ¤ ì²´í¬ (API í”„ë¦¬í”½ìŠ¤ ë²„ì „)"""
    return await health_check()


@api_router.get("/status")
async def get_status_api():
    """AI ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ (API í”„ë¦¬í”½ìŠ¤ ë²„ì „)"""
    return await get_status()


# API ë¼ìš°í„°ë¥¼ ë©”ì¸ ì•±ì— ë“±ë¡
app.include_router(api_router)


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "Chatbot v6 API",
        "version": "6.0.0",
        "description": "ì •ìˆ˜ì²˜ë¦¬ ì±—ë´‡ API (4ê°€ì§€ ì›ì¹™ ì¤€ìˆ˜)",
        "endpoints": {
            "health": "/healthz or /api/healthz",
            "ask": "/ask (POST) or /api/ask (POST)",
            "status": "/status or /api/status",
            "docs": "/docs",
        }
    }


if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8000,
        reload=env_config.debug,
    )

