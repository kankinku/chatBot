"""
FastAPI Application - API ì„œë²„

RESTful API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from fastapi import FastAPI, HTTPException, Request, APIRouter, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import uuid
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor

from config.pipeline_config import PipelineConfig
from config.environment import get_env_config
from config.model_config import LLMModelConfig
from modules.core.logger import setup_logging, get_logger
from modules.core.exceptions import ChatbotException
from modules.pipeline.rag_pipeline import RAGPipeline
from modules.core.types import Chunk
from modules.analysis.question_analyzer import QuestionAnalyzer
from modules.generation.ollama_manager import OllamaManager
from modules.document.loader import DocumentLoader

# ë¡œê¹… ì„¤ì •
env_config = get_env_config()
setup_logging(
    log_dir=env_config.log_dir,
    log_level=env_config.log_level,
    log_format=env_config.log_format,
)

logger = get_logger(__name__)

# FastAPI ì•±
app = FastAPI(
    title="Chatbot v6 API",
    description="ì •ìˆ˜ì²˜ë¦¬ ì±—ë´‡ API (4ê°€ì§€ ì›ì¹™ ì¤€ìˆ˜)",
    version="6.0.0",
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” ì œí•œ í•„ìš”
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ íŒŒì´í”„ë¼ì¸ (ì´ˆê¸°í™”ëŠ” startupì—ì„œ)
pipeline: Optional[RAGPipeline] = None
question_analyzer: Optional[QuestionAnalyzer] = None

# PDF ì²˜ë¦¬ ì‘ì—… ìƒíƒœ ì €ì¥
pdf_processing_status: Dict[str, Any] = {
    "status": "idle",  # idle, processing, completed, error
    "progress": 0,
    "message": "",
    "processed_files": [],
    "skipped_files": [],
    "total_chunks": 0,
    "processing_time_seconds": 0,
}

# ì¸ì‚¬ë§ ì‘ë‹µ ë¦¬ìŠ¤íŠ¸
GREETING_RESPONSES = [
    "ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹ ì •ìˆ˜ì²˜ë¦¬ AI ì±—ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
    "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ì •ìˆ˜ì²˜ë¦¬ ê´€ë ¨ ì§ˆë¬¸ì´ë‚˜ ë¬¸ì„œ ê²€ìƒ‰ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
    "ì•ˆë…•í•˜ì„¸ìš”! ğŸŒŠ ì •ìˆ˜ì¥ ìš´ì˜, ìˆ˜ì§ˆ ê´€ë¦¬, ì‹œìŠ¤í…œ ì •ë³´ ë“± ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!",
    "ë°˜ê°‘ìŠµë‹ˆë‹¤! ğŸ¤– AI ê¸°ë°˜ìœ¼ë¡œ ì •ìˆ˜ì²˜ë¦¬ ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤.",
    "ì•ˆë…•í•˜ì„¸ìš”! ğŸ’§ ì •ìˆ˜ì²˜ë¦¬ ê¸°ìˆ , ê³µì • ì •ë³´, ë§¤ë‰´ì–¼ ê²€ìƒ‰ ë“±ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤.",
]

# API ë¼ìš°í„° ìƒì„± (/api í”„ë¦¬í”½ìŠ¤ìš©)
api_router = APIRouter(prefix="/api")


# Request/Response ëª¨ë¸
class QuestionRequest(BaseModel):
    question: str
    top_k: int = 50


class Source(BaseModel):
    text: str
    score: float
    rank: int
    filename: str
    page: Optional[int] = None


class AnswerResponse(BaseModel):
    answer: str
    confidence: float
    sources: List[Source]
    metrics: Dict[str, Any]


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global pipeline, question_analyzer
    
    logger.info("API server starting up")
    
    try:
        # Ollama ì„œë²„ ì—°ê²° ë° ëª¨ë¸ ìë™ ì„¤ì¹˜
        logger.info("Checking Ollama server and model availability...")
        ollama_url = f"http://{LLMModelConfig().host}:{LLMModelConfig().port}"
        ollama_manager = OllamaManager(base_url=ollama_url)
        
        # Ollama ì„œë²„ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 60ì´ˆ)
        max_wait_time = 60
        wait_interval = 2
        waited = 0
        
        while waited < max_wait_time:
            if ollama_manager.check_ollama_running():
                logger.info("Ollama server is running")
                break
            logger.info(f"Waiting for Ollama server... ({waited}/{max_wait_time}s)")
            time.sleep(wait_interval)
            waited += wait_interval
        else:
            logger.warning("Ollama server is not responding. LLM features may not work.")
        
        # ëª¨ë¸ ìë™ ì„¤ì¹˜
        if ollama_manager.check_ollama_running():
            model_config = LLMModelConfig()
            model_name = model_config.model_name
            logger.info(f"Ensuring model availability: {model_name}")
            
            if ollama_manager.ensure_model_available(model_name):
                logger.info(f"Model {model_name} is ready")
            else:
                logger.warning(f"Failed to ensure model {model_name}. LLM features may not work.")
        
        # ì„¤ì • ë¡œë“œ
        config_path = project_root / "config" / "default.yaml"
        pipeline_config = PipelineConfig.from_file(config_path)
        
        # ë„ë©”ì¸ ì‚¬ì „ ê²½ë¡œ ì„¤ì •
        domain_dict_path = project_root / "data" / "domain_dictionary.json"
        
        # QuestionAnalyzer ì´ˆê¸°í™”
        question_analyzer = QuestionAnalyzer(
            domain_dict_path=str(domain_dict_path) if domain_dict_path.exists() else None
        )
        
        # PDF ë¬¸ì„œ ìë™ ë¡œë“œ ë° ì„ë² ë”©
        data_dir = project_root / "data"
        chunks = []
        
        try:
            logger.info(f"Loading documents from: {data_dir}")
            doc_loader = DocumentLoader(str(data_dir))
            
            # PDF íŒŒì¼ í™•ì¸
            pdf_files = list(data_dir.glob("*.pdf")) + list(data_dir.glob("*.PDF"))
            
            if pdf_files:
                logger.info(f"Found {len(pdf_files)} PDF file(s): {[f.name for f in pdf_files]}")
                try:
                    chunks = doc_loader.load_from_directory(use_cache=True)
                    logger.info(f"Loaded {len(chunks)} chunks from {len(pdf_files)} document(s)")
                except Exception as e:
                    logger.error(f"Failed to load documents: {e}", exc_info=True)
                    logger.warning("Falling back to dummy chunks")
                    chunks = []
            else:
                logger.warning(f"No PDF files found in {data_dir}")
        
        except Exception as e:
            logger.error(f"Document loading error: {e}", exc_info=True)
            logger.warning("Falling back to dummy chunks")
            chunks = []
        
        # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ë”ë¯¸ ì²­í¬ ì‚¬ìš©
        if not chunks:
            logger.warning("No chunks loaded, using dummy chunks")
            chunks = [
                Chunk(
                    doc_id="demo",
                    filename="demo.pdf",
                    page=1,
                    start_offset=0,
                    length=100,
                    text="ë¬¸ì„œê°€ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ data/ ë””ë ‰í† ë¦¬ì— ì¶”ê°€í•´ì£¼ì„¸ìš”.",
                ),
            ]
        
        # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ìë™ ì„ë² ë”© í¬í•¨)
        logger.info("Initializing RAG pipeline with embedding... (evaluation mode enabled)")
        pipeline = RAGPipeline(
            chunks=chunks,
            pipeline_config=pipeline_config,
            evaluation_mode=True,  # í‰ê°€ ëª¨ë“œ í™œì„±í™”
        )
        
        logger.info(f"API server started successfully with {len(chunks)} chunks")
    
    except Exception as e:
        logger.error(f"Failed to initialize pipeline: {e}", exc_info=True)
        raise


@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œ ì‹œ ì •ë¦¬"""
    logger.info("API server shutting down")


@app.get("/healthz")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "service": "chatbot-v6",
        "version": "6.0.0",
    }


@app.get("/status")
async def get_status():
    """AI ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    if not pipeline:
        return {
            "status": "initializing",
            "ai_available": False,
            "model_loaded": False,
            "total_pdfs": 0,
            "total_chunks": 0,
        }
    
    # ë¬¸ì„œ í†µê³„
    total_chunks = len(pipeline.chunks) if hasattr(pipeline, 'chunks') else 0
    unique_files = len(set(c.filename for c in pipeline.chunks)) if hasattr(pipeline, 'chunks') else 0
    
    return {
        "status": "ok",
        "ai_available": True,
        "model_loaded": True,
        "total_pdfs": unique_files,
        "total_chunks": total_chunks,
    }


class ProcessPDFsResponse(BaseModel):
    success: bool
    message: str
    processed_files: List[str]
    skipped_files: List[str]
    total_chunks: int
    processing_time_seconds: float


def process_pdfs_background():
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ PDF ì²˜ë¦¬ (ë³‘ë ¬ ì„ë² ë”© í¬í•¨)"""
    global pipeline, pdf_processing_status
    
    try:
        pdf_processing_status["status"] = "processing"
        pdf_processing_status["progress"] = 0
        pdf_processing_status["message"] = "PDF ì²˜ë¦¬ ì‹œì‘..."
        
        if not pipeline:
            pdf_processing_status["status"] = "error"
            pdf_processing_status["message"] = "Pipeline not initialized"
            return
        
        start_time = time.time()
        data_dir = project_root / "data"
        cache_path = data_dir / "chunks_cache.pkl"
        
        # ê¸°ì¡´ ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ í™•ì¸ (ìºì‹œì—ì„œ)
        processed_files = set()
        if cache_path.exists():
            try:
                import pickle
                with open(cache_path, 'rb') as f:
                    cached_data = pickle.load(f)
                    cached_chunks = cached_data.get("chunks", [])
                processed_files = {c.filename for c in cached_chunks if isinstance(c, Chunk)}
                logger.info(f"Found {len(processed_files)} already processed files in cache")
            except Exception as e:
                logger.warning(f"Failed to load cache for comparison: {e}")
        
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ PDF íŒŒì¼ ëª©ë¡
        pdf_files = list(data_dir.glob("*.pdf")) + list(data_dir.glob("*.PDF"))
        
        # ì²˜ë¦¬ë˜ì§€ ì•Šì€ íŒŒì¼ í•„í„°ë§
        new_files = [f for f in pdf_files if f.name not in processed_files]
        skipped_files = [f.name for f in pdf_files if f.name in processed_files]
        
        pdf_processing_status["progress"] = 10
        pdf_processing_status["message"] = f"{len(new_files)}ê°œ ìƒˆ PDF íŒŒì¼ ë°œê²¬"
        
        if not new_files:
            pdf_processing_status["status"] = "completed"
            pdf_processing_status["message"] = f"ì²˜ë¦¬í•  ìƒˆ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼: {len(skipped_files)}ê°œ)"
            pdf_processing_status["skipped_files"] = skipped_files
            pdf_processing_status["total_chunks"] = len(pipeline.chunks) if hasattr(pipeline, 'chunks') else 0
            pdf_processing_status["processing_time_seconds"] = time.time() - start_time
            return
        
        logger.info(f"Processing {len(new_files)} new PDF file(s): {[f.name for f in new_files]}")
        
        # DocumentLoaderë¡œ ìƒˆ íŒŒì¼ë“¤ ë³‘ë ¬ ì²˜ë¦¬
        pdf_processing_status["progress"] = 20
        pdf_processing_status["message"] = "PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."
        
        doc_loader = DocumentLoader(str(data_dir))
        new_chunks = []
        
        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            for pdf_path in new_files:
                future = executor.submit(doc_loader._load_pdf, pdf_path)
                futures.append((future, pdf_path))
            
            for i, (future, pdf_path) in enumerate(futures):
                try:
                    chunks = future.result()
                    new_chunks.extend(chunks)
                    logger.info(f"Processed {pdf_path.name}: {len(chunks)} chunks")
                    
                    progress = 20 + int((i + 1) / len(futures) * 30)
                    pdf_processing_status["progress"] = progress
                    pdf_processing_status["message"] = f"PDF ì²˜ë¦¬ ì¤‘... ({i+1}/{len(futures)}): {pdf_path.name}"
                except Exception as e:
                    logger.error(f"Failed to process {pdf_path.name}: {e}", exc_info=True)
                    continue
        
        pdf_processing_status["progress"] = 50
        pdf_processing_status["message"] = "ì„ë² ë”© ì¤€ë¹„ ì¤‘..."
        
        # ê¸°ì¡´ ì²­í¬ì™€ ìƒˆ ì²­í¬ ë³‘í•© í›„ ìºì‹œ ì €ì¥
        if new_chunks:
            # ê¸°ì¡´ ìºì‹œì—ì„œ ì²­í¬ ë¡œë“œ
            existing_chunks = []
            if hasattr(pipeline, 'chunks'):
                existing_chunks = pipeline.chunks
            
            # ëª¨ë“  ì²­í¬ ë³‘í•©
            all_chunks_for_cache = existing_chunks + new_chunks
            
            # ìºì‹œì— ì €ì¥
            doc_loader.chunks = all_chunks_for_cache
            doc_loader.save_to_cache(str(cache_path))
            logger.info(f"Cache updated: {len(existing_chunks)} existing + {len(new_chunks)} new = {len(all_chunks_for_cache)} total chunks")
        
        if not new_chunks:
            pdf_processing_status["status"] = "error"
            pdf_processing_status["message"] = "ìƒˆ PDF íŒŒì¼ì—ì„œ ì²­í¬ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."
            return
        
        # ê¸°ì¡´ ì²­í¬ì™€ ìƒˆ ì²­í¬ ë³‘í•©
        if hasattr(pipeline, 'chunks'):
            all_chunks = pipeline.chunks + new_chunks
        else:
            all_chunks = new_chunks
        
        # íŒŒì´í”„ë¼ì¸ ì¬ì´ˆê¸°í™” (ìƒˆ ì²­í¬ í¬í•¨) - ì„ë² ë”©ì€ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹œ ìë™ìœ¼ë¡œ ìˆ˜í–‰ë¨
        pdf_processing_status["progress"] = 60
        pdf_processing_status["message"] = "ì„ë² ë”© ë° ë²¡í„° ì €ì¥ ì¤‘... (ì´ ì‘ì—…ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"
        
        from config.pipeline_config import PipelineConfig
        config_path = project_root / "config" / "default.yaml"
        pipeline_config = PipelineConfig.from_file(config_path)
        
        # íŒŒì´í”„ë¼ì¸ ì¬ì´ˆê¸°í™” (ì„ë² ë”© ìë™ ìˆ˜í–‰)
        pipeline = RAGPipeline(
            chunks=all_chunks,
            pipeline_config=pipeline_config,
        )
        
        processing_time = time.time() - start_time
        
        # ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        pdf_processing_status["status"] = "completed"
        pdf_processing_status["progress"] = 100
        pdf_processing_status["message"] = f"{len(new_files)}ê°œ PDF íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ"
        pdf_processing_status["processed_files"] = [f.name for f in new_files]
        pdf_processing_status["skipped_files"] = skipped_files
        pdf_processing_status["total_chunks"] = len(all_chunks)
        pdf_processing_status["processing_time_seconds"] = processing_time
        
        logger.info(
            f"Successfully processed {len(new_files)} PDF file(s)",
            extra={
                "processed_files": [f.name for f in new_files],
                "total_chunks": len(all_chunks),
                "new_chunks": len(new_chunks),
                "processing_time": processing_time
            }
        )
        
    except Exception as e:
        logger.error(f"Failed to process PDFs: {e}", exc_info=True)
        pdf_processing_status["status"] = "error"
        pdf_processing_status["message"] = f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@app.post("/api/process-pdfs")
async def start_process_pdfs(background_tasks: BackgroundTasks):
    """PDF ì²˜ë¦¬ ì‹œì‘ (ë¹„ë™ê¸°)"""
    global pdf_processing_status
    
    # ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì´ë©´ ì‹œì‘í•˜ì§€ ì•ŠìŒ
    if pdf_processing_status["status"] == "processing":
        return {
            "success": True,
            "message": "ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤.",
            "status": "processing"
        }
    
    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘
    background_tasks.add_task(process_pdfs_background)
    
    return {
        "success": True,
        "message": "PDF ì²˜ë¦¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "status": "processing"
    }


@app.get("/api/process-pdfs/status")
async def get_process_pdfs_status():
    """PDF ì²˜ë¦¬ ìƒíƒœ í™•ì¸"""
    return pdf_processing_status


# ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
@app.post("/ask", response_model=AnswerResponse)
async def ask_question(req: Request, request: QuestionRequest):
    """
    ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€
    
    Args:
        request: ì§ˆë¬¸ ìš”ì²­
        
    Returns:
        ë‹µë³€ ì‘ë‹µ
    """
    if not pipeline:
        raise HTTPException(status_code=503, detail="Pipeline not initialized")
    
    start_time = time.time()
    session_id = req.headers.get('X-Session-ID', str(uuid.uuid4()))
    
    try:
        logger.info(
            f"[QUESTION] Received question",
            extra={
                "question": request.question,
                "session_id": session_id,
                "top_k": request.top_k,
                "timestamp": time.time()
            }
        )
        
        # ì¸ì‚¬ë§ ì²´í¬
        if question_analyzer and question_analyzer.is_greeting(request.question):
            logger.info(
                f"[GREETING] Detected, returning preset response",
                extra={
                    "question": request.question,
                    "session_id": session_id
                }
            )
            
            import random
            greeting_response = random.choice(GREETING_RESPONSES)
            processing_time = time.time() - start_time
            
            # ì¸ì‚¬ë§ ì‘ë‹µ ìƒì„±
            from modules.core.types import Answer
            
            answer = Answer(
                text=greeting_response,
                confidence=1.0,
                sources=[],
                metrics={
                    "total_time_ms": int(processing_time * 1000),
                    "is_greeting": True,
                    "llm_used": False,
                }
            )
        else:
            # ì¼ë°˜ ì§ˆë¬¸ - íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            answer = pipeline.ask(
                question=request.question,
                top_k=request.top_k,
            )
            
            processing_time = time.time() - start_time
        
        # ì‘ë‹µ ë³€í™˜ (ì¸ì‚¬ë§ì€ sourcesê°€ ì—†ì„ ìˆ˜ ìˆìŒ)
        sources = [
            Source(
                text=span.chunk.text[:200],
                score=span.score,
                rank=span.rank,
                filename=span.chunk.filename,
                page=span.chunk.page,
            )
            for span in answer.sources[:5]
        ] if answer.sources else []
        
        response = AnswerResponse(
            answer=answer.text,
            confidence=answer.confidence,
            sources=sources,
            metrics={
                **answer.metrics,
                "processing_time": processing_time,
                "session_id": session_id,
            },
        )
        
        logger.info(
            f"[ANSWER] Generated successfully",
            extra={
                "question": request.question,
                "answer": answer.text[:200] + "..." if len(answer.text) > 200 else answer.text,
                "confidence": answer.confidence,
                "processing_time": processing_time,
                "session_id": session_id,
                "sources_count": len(answer.sources),
                "metrics": answer.metrics
            }
        )
        
        return response
    
    except ChatbotException as e:
        logger.error(f"Chatbot error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail={
                "error": e.to_dict(),
                "message": str(e),
            }
        )
    
    except Exception as e:
        logger.error(f"Unexpected error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# /api í”„ë¦¬í”½ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ë“¤ (í”„ë¡ì‹œ ì„œë²„ í˜¸í™˜)
@api_router.post("/ask", response_model=AnswerResponse)
async def ask_question_api(req: Request, request: QuestionRequest):
    """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ (API í”„ë¦¬í”½ìŠ¤ ë²„ì „)"""
    # ê¸°ì¡´ í•¨ìˆ˜ì™€ ë™ì¼í•œ ë¡œì§ ì¬ì‚¬ìš©
    return await ask_question(req, request)


@api_router.get("/healthz")
async def health_check_api():
    """í—¬ìŠ¤ ì²´í¬ (API í”„ë¦¬í”½ìŠ¤ ë²„ì „)"""
    return await health_check()


@api_router.get("/status")
async def get_status_api():
    """AI ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ (API í”„ë¦¬í”½ìŠ¤ ë²„ì „)"""
    return await get_status()


# API ë¼ìš°í„°ë¥¼ ë©”ì¸ ì•±ì— ë“±ë¡
app.include_router(api_router)


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "Chatbot v6 API",
        "version": "6.0.0",
        "description": "ì •ìˆ˜ì²˜ë¦¬ ì±—ë´‡ API (4ê°€ì§€ ì›ì¹™ ì¤€ìˆ˜)",
        "endpoints": {
            "health": "/healthz or /api/healthz",
            "ask": "/ask (POST) or /api/ask (POST)",
            "status": "/status or /api/status",
            "docs": "/docs",
        }
    }


if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8000,
        reload=env_config.debug,
    )

