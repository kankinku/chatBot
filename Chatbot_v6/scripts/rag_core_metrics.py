#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG ì‹œìŠ¤í…œ í•µì‹¬ í‰ê°€ ì§€í‘œ (RAGAs Framework ê¸°ë°˜)

3ëŒ€ í•µì‹¬ ì§€í‘œ:
1. Faithfulness (ì¶©ì‹¤ì„±) - í™˜ê° ë°©ì§€
2. Answer Correctness (ë‹µë³€ ì •í™•ë„) - ì‚¬ì‹¤ì  ì¼ì¹˜
3. Context Precision (ë¬¸ë§¥ ì •ë°€ë„) - ê²€ìƒ‰ íš¨ìœ¨ì„±
"""

import re
from typing import List, Dict, Any, Set
from collections import Counter


class RAGCoreMetrics:
    """RAG ì‹œìŠ¤í…œ í•µì‹¬ í‰ê°€ ì§€í‘œ"""
    
    @staticmethod
    def tokenize(text: str) -> List[str]:
        """í† í¬ë‚˜ì´ì €"""
        tokens = re.findall(r'[\w]+', text.lower())
        return [t for t in tokens if len(t) > 1]
    
    # ============================================================
    # 1. Faithfulness (ì¶©ì‹¤ì„±) - í™˜ê° ë°©ì§€
    # ============================================================
    
    @staticmethod
    def faithfulness(answer: str, contexts: List[str]) -> Dict[str, Any]:
        """
        Faithfulness (ì¶©ì‹¤ì„±/ê·¼ê±°ì„±)
        
        ë‹µë³€ì˜ ëª¨ë“  ì‚¬ì‹¤ì´ ì°¸ê³  ìžë£Œ(contexts)ì— ì˜í•´ ë’·ë°›ì¹¨ë˜ëŠ” ì •ë„ë¥¼ ì¸¡ì •.
        í™˜ê°(hallucination) ë°©ì§€ì˜ í•µì‹¬ ì§€í‘œ.
        
        í‰ê°€ ì§ˆë¬¸: "ë‹µë³€ì´ ìžë£Œ ë°–ì˜ ê±°ì§“ë§ì„ í–ˆë‚˜?"
        
        Args:
            answer: ìƒì„±ëœ ë‹µë³€
            contexts: ì°¸ê³  ìžë£Œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            {
                'score': float (0~1),
                'supported_claims': int,
                'total_claims': int,
                'support_ratio': float
            }
            
        Reference:
            Es et al. (2023). RAGAS: Automated Evaluation of 
            Retrieval Augmented Generation. arXiv:2309.15217
        """
        if not answer or not contexts:
            return {
                'score': 0.0,
                'supported_claims': 0,
                'total_claims': 0,
                'support_ratio': 0.0
            }
        
        # ë‹µë³€ì„ ë¬¸ìž¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (claimìœ¼ë¡œ ê°„ì£¼)
        answer_sentences = [s.strip() for s in re.split(r'[.!?]\s+', answer) if len(s.strip()) > 10]
        
        if not answer_sentences:
            return {
                'score': 0.0,
                'supported_claims': 0,
                'total_claims': 0,
                'support_ratio': 0.0
            }
        
        # ëª¨ë“  contextsë¥¼ í•˜ë‚˜ë¡œ í•©ì¹¨
        combined_context = ' '.join(contexts).lower()
        context_tokens = set(RAGCoreMetrics.tokenize(combined_context))
        
        # ê° ë¬¸ìž¥(claim)ì´ contextì—ì„œ ì§€ì§€ë˜ëŠ”ì§€ í™•ì¸
        supported = 0
        
        for sentence in answer_sentences:
            sentence_tokens = set(RAGCoreMetrics.tokenize(sentence))
            
            # ë¬¸ìž¥ì˜ ì£¼ìš” í† í°ë“¤ì´ contextì— ìžˆëŠ”ì§€ í™•ì¸
            if sentence_tokens:
                overlap_ratio = len(sentence_tokens & context_tokens) / len(sentence_tokens)
                
                # í† í°ì˜ 70% ì´ìƒì´ contextì— ìžˆìœ¼ë©´ ì§€ì§€ë¨ìœ¼ë¡œ ê°„ì£¼
                if overlap_ratio >= 0.7:
                    supported += 1
        
        total_claims = len(answer_sentences)
        support_ratio = supported / total_claims
        
        # Faithfulness score
        score = support_ratio
        
        return {
            'score': round(score, 4),
            'supported_claims': supported,
            'total_claims': total_claims,
            'support_ratio': round(support_ratio, 4)
        }
    
    # ============================================================
    # 2. Answer Correctness (ë‹µë³€ ì •í™•ë„) - ì‚¬ì‹¤ì  ì¼ì¹˜
    # ============================================================
    
    @staticmethod
    def answer_correctness(answer: str, ground_truth: str) -> Dict[str, Any]:
        """
        Answer Correctness (ë‹µë³€ ì •í™•ë„)
        
        ìƒì„±ëœ ë‹µë³€ì´ ì •ë‹µ(Ground Truth)ê³¼ ì˜ë¯¸ì /ì‚¬ì‹¤ì ìœ¼ë¡œ 
        ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ì¸¡ì •.
        
        í‰ê°€ ì§ˆë¬¸: "ë‹µë³€ì´ ì •ë‹µê³¼ ì‚¬ì‹¤ìƒ ë™ì¼í•œê°€?"
        
        Args:
            answer: ìƒì„±ëœ ë‹µë³€
            ground_truth: ì •ë‹µ
            
        Returns:
            {
                'score': float (0~1),
                'semantic_similarity': float,
                'factual_correctness': float
            }
            
        Reference:
            Es et al. (2023). RAGAS: Automated Evaluation of 
            Retrieval Augmented Generation. arXiv:2309.15217
        """
        if not answer or not ground_truth:
            return {
                'score': 0.0,
                'semantic_similarity': 0.0,
                'factual_correctness': 0.0
            }
        
        answer_tokens = set(RAGCoreMetrics.tokenize(answer))
        truth_tokens = set(RAGCoreMetrics.tokenize(ground_truth))
        
        if not answer_tokens or not truth_tokens:
            return {
                'score': 0.0,
                'semantic_similarity': 0.0,
                'factual_correctness': 0.0
            }
        
        # 1. ì˜ë¯¸ì  ìœ ì‚¬ë„ (Token overlap ê¸°ë°˜)
        common_tokens = answer_tokens & truth_tokens
        semantic_similarity = len(common_tokens) / len(truth_tokens)
        
        # 2. ì‚¬ì‹¤ì  ì •í™•ë„ (íŠ¹ížˆ ìˆ«ìž/ë‹¨ìœ„ ì¤‘ì‹¬)
        # ì •ë‹µì˜ ìˆ«ìž ì¶”ì¶œ
        truth_nums = set(re.findall(r'\d+(?:[.,]\d+)?', ground_truth))
        answer_nums = set(re.findall(r'\d+(?:[.,]\d+)?', answer))
        
        # ì •ë‹µì˜ ë‹¨ìœ„ ì¶”ì¶œ
        truth_units = set(re.findall(r'[%Â°â„ƒãŽŽ]+|(?:mg|ppm|rpm|kwh|kg)/[lL]?', ground_truth, re.IGNORECASE))
        answer_units = set(re.findall(r'[%Â°â„ƒãŽŽ]+|(?:mg|ppm|rpm|kwh|kg)/[lL]?', answer, re.IGNORECASE))
        
        # ìˆ«ìž ì¼ì¹˜ìœ¨
        num_match = 0.0
        if truth_nums:
            num_match = len(truth_nums & answer_nums) / len(truth_nums)
        else:
            num_match = 1.0  # ìˆ«ìžê°€ ì—†ìœ¼ë©´ ë§Œì 
        
        # ë‹¨ìœ„ ì¼ì¹˜ìœ¨
        unit_match = 0.0
        if truth_units:
            unit_match = len(truth_units & answer_units) / len(truth_units)
        else:
            unit_match = 1.0  # ë‹¨ìœ„ê°€ ì—†ìœ¼ë©´ ë§Œì 
        
        # ì‚¬ì‹¤ì  ì •í™•ë„: ìˆ«ìž(50%) + ë‹¨ìœ„(30%) + í† í°(20%)
        factual_correctness = 0.5 * num_match + 0.3 * unit_match + 0.2 * semantic_similarity
        
        # ìµœì¢… ì ìˆ˜: ì˜ë¯¸ ìœ ì‚¬ë„(40%) + ì‚¬ì‹¤ì  ì •í™•ë„(60%)
        final_score = 0.4 * semantic_similarity + 0.6 * factual_correctness
        
        return {
            'score': round(final_score, 4),
            'semantic_similarity': round(semantic_similarity, 4),
            'factual_correctness': round(factual_correctness, 4),
            'details': {
                'num_match_ratio': round(num_match, 4),
                'unit_match_ratio': round(unit_match, 4),
                'truth_nums': list(truth_nums),
                'answer_nums': list(answer_nums)
            }
        }
    
    # ============================================================
    # 3. Context Precision (ë¬¸ë§¥ ì •ë°€ë„) - ê²€ìƒ‰ íš¨ìœ¨ì„±
    # ============================================================
    
    @staticmethod
    def context_precision(
        question: str,
        contexts: List[str],
        answer: str,
        ground_truth: str
    ) -> Dict[str, Any]:
        """
        Context Precision (ë¬¸ë§¥ ì •ë°€ë„)
        
        ê²€ìƒ‰ëœ ìžë£Œ(contexts) ì¤‘ ì‹¤ì œë¡œ ë‹µë³€ì— ì‚¬ìš©ëœ/í•„ìš”í•œ ìžë£Œì˜ ë¹„ìœ¨.
        ê²€ìƒ‰ íš¨ìœ¨ì„±ì„ ì¸¡ì •.
        
        í‰ê°€ ì§ˆë¬¸: "ì—‰ëš±í•œ ìžë£Œë¥¼ ê°€ì ¸ì™€ì„œ í—·ê°ˆë¦¬ì§€ ì•Šì•˜ë‚˜?"
        
        Args:
            question: ì§ˆë¬¸
            contexts: ê²€ìƒ‰ëœ ìžë£Œ ë¦¬ìŠ¤íŠ¸
            answer: ìƒì„±ëœ ë‹µë³€
            ground_truth: ì •ë‹µ
            
        Returns:
            {
                'score': float (0~1),
                'relevant_contexts': int,
                'total_contexts': int,
                'precision': float
            }
            
        Reference:
            Es et al. (2023). RAGAS: Automated Evaluation of 
            Retrieval Augmented Generation. arXiv:2309.15217
        """
        if not contexts:
            return {
                'score': 0.0,
                'relevant_contexts': 0,
                'total_contexts': 0,
                'precision': 0.0
            }
        
        # ì§ˆë¬¸ê³¼ ì •ë‹µì˜ í‚¤ í† í° ì¶”ì¶œ
        question_tokens = set(RAGCoreMetrics.tokenize(question))
        truth_tokens = set(RAGCoreMetrics.tokenize(ground_truth))
        answer_tokens = set(RAGCoreMetrics.tokenize(answer))
        
        # ê´€ë ¨ì„± ìžˆëŠ” í† í° = ì§ˆë¬¸ + ì •ë‹µ + ë‹µë³€ì˜ í•©ì§‘í•©
        relevant_tokens = question_tokens | truth_tokens | answer_tokens
        
        # ê° contextê°€ ê´€ë ¨ì„±ì´ ìžˆëŠ”ì§€ í‰ê°€
        relevant_count = 0
        
        for context in contexts:
            context_tokens = set(RAGCoreMetrics.tokenize(context))
            
            if not context_tokens:
                continue
            
            # Contextê°€ ê´€ë ¨ í† í°ê³¼ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ì§€
            overlap = len(context_tokens & relevant_tokens) / len(context_tokens)
            
            # 30% ì´ìƒ ê²¹ì¹˜ë©´ ê´€ë ¨ì„± ìžˆìŒìœ¼ë¡œ ê°„ì£¼
            if overlap >= 0.3:
                relevant_count += 1
        
        total_contexts = len(contexts)
        precision = relevant_count / total_contexts
        
        return {
            'score': round(precision, 4),
            'relevant_contexts': relevant_count,
            'total_contexts': total_contexts,
            'precision': round(precision, 4)
        }
    
    # ============================================================
    # ì¢…í•© í‰ê°€
    # ============================================================
    
    @staticmethod
    def evaluate_all(
        question: str,
        answer: str,
        ground_truth: str,
        contexts: List[str]
    ) -> Dict[str, Any]:
        """
        RAG ì‹œìŠ¤í…œ í•µì‹¬ 3ëŒ€ ì§€í‘œ ì¢…í•© í‰ê°€
        
        Returns:
            {
                'faithfulness': {...},
                'answer_correctness': {...},
                'context_precision': {...},
                'overall_score': float
            }
        """
        # 1. Faithfulness
        faith = RAGCoreMetrics.faithfulness(answer, contexts)
        
        # 2. Answer Correctness
        correctness = RAGCoreMetrics.answer_correctness(answer, ground_truth)
        
        # 3. Context Precision
        precision = RAGCoreMetrics.context_precision(question, contexts, answer, ground_truth)
        
        # ì¢…í•© ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
        # Faithfulness(40%) + Answer Correctness(40%) + Context Precision(20%)
        overall = (
            0.4 * faith['score'] +
            0.4 * correctness['score'] +
            0.2 * precision['score']
        )
        
        return {
            'faithfulness': faith,
            'answer_correctness': correctness,
            'context_precision': precision,
            'overall_score': round(overall, 4)
        }


def main():
    """í…ŒìŠ¤íŠ¸"""
    import sys
    sys.stdout.reconfigure(encoding='utf-8')
    
    print("=" * 70)
    print("RAG ì‹œìŠ¤í…œ í•µì‹¬ í‰ê°€ ì§€í‘œ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    question = "AI í”Œëž«í¼ì˜ ê¸°ë³¸ ê´€ë¦¬ìž ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ëŠ”?"
    answer = "ê¸°ë³¸ ê´€ë¦¬ìž ê³„ì •ì˜ ì•„ì´ë””ëŠ” KWATERì´ê³  ë¹„ë°€ë²ˆí˜¸ë„ KWATERìž…ë‹ˆë‹¤."
    ground_truth = "ê¸°ë³¸ ê´€ë¦¬ìž ê³„ì •ì˜ ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ëŠ” ëª¨ë‘ KWATERìž…ë‹ˆë‹¤."
    contexts = [
        "ê¸°ë³¸ ê´€ë¦¬ìž ê³„ì •: ì•„ì´ë”” KWATER, ë¹„ë°€ë²ˆí˜¸ KWATER",
        "AI í”Œëž«í¼ ì ‘ì† ì£¼ì†ŒëŠ” http://waio-portal-vip:10011 ìž…ë‹ˆë‹¤.",
        "ì‹œìŠ¤í…œì€ 4ê°€ì§€ ì˜ì—­ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤."
    ]
    
    print(f"\nì§ˆë¬¸: {question}")
    print(f"ë‹µë³€: {answer}")
    print(f"ì •ë‹µ: {ground_truth}")
    print(f"ì°¸ê³  ìžë£Œ ìˆ˜: {len(contexts)}ê°œ")
    
    print("\n" + "=" * 70)
    
    # í‰ê°€ ì‹¤í–‰
    metrics = RAGCoreMetrics()
    results = metrics.evaluate_all(question, answer, ground_truth, contexts)
    
    print("\nðŸ“Š RAG í•µì‹¬ í‰ê°€ ê²°ê³¼:")
    print()
    print(f"1ï¸âƒ£  Faithfulness (ì¶©ì‹¤ì„±):        {results['faithfulness']['score']*100:>6.1f}%")
    print(f"   - ìžë£Œ ê¸°ë°˜ ë¬¸ìž¥: {results['faithfulness']['supported_claims']}/{results['faithfulness']['total_claims']}")
    print(f"   - í™˜ê° ë°©ì§€ ì„±ê³µ: {results['faithfulness']['support_ratio']*100:.1f}%")
    print()
    print(f"2ï¸âƒ£  Answer Correctness (ë‹µë³€ ì •í™•ë„): {results['answer_correctness']['score']*100:>6.1f}%")
    print(f"   - ì˜ë¯¸ ìœ ì‚¬ë„: {results['answer_correctness']['semantic_similarity']*100:.1f}%")
    print(f"   - ì‚¬ì‹¤ì  ì •í™•ë„: {results['answer_correctness']['factual_correctness']*100:.1f}%")
    print()
    print(f"3ï¸âƒ£  Context Precision (ë¬¸ë§¥ ì •ë°€ë„):  {results['context_precision']['score']*100:>6.1f}%")
    print(f"   - ê´€ë ¨ ìžë£Œ: {results['context_precision']['relevant_contexts']}/{results['context_precision']['total_contexts']}")
    print(f"   - ê²€ìƒ‰ íš¨ìœ¨ì„±: {results['context_precision']['precision']*100:.1f}%")
    print()
    print("=" * 70)
    print(f"ðŸ† ì¢…í•© ì ìˆ˜:                     {results['overall_score']*100:>6.1f}%")
    print("=" * 70)
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()



