#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•™ìˆ  ë…¼ë¬¸ìš© í‰ê°€ ì§€í‘œ êµ¬í˜„

í‘œì¤€ í‰ê°€ ì§€í‘œ: Token F1, ROUGE-L, BLEU, Exact Match
"""

import re
from typing import List, Dict, Any, Tuple
from collections import Counter


class AcademicMetrics:
    """í•™ìˆ  ë…¼ë¬¸ìš© í‘œì¤€ í‰ê°€ ì§€í‘œ"""
    
    @staticmethod
    def normalize_text(text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        text = text.lower().strip()
        text = re.sub(r'\s+', ' ', text)
        return text
    
    @staticmethod
    def tokenize(text: str) -> List[str]:
        """í† í¬ë‚˜ì´ì € (í•œê¸€ ì§€ì›)"""
        tokens = re.findall(r'[\w]+', text)
        return [t for t in tokens if t]
    
    # ============================================================
    # 1. Exact Match (EM) - SQuAD í‘œì¤€
    # ============================================================
    
    @staticmethod
    def exact_match(pred: str, gold: str) -> float:
        """
        Exact Match (ì™„ì „ ì¼ì¹˜)
        
        SQuAD, KorQuAD ë“±ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê°€ì¥ ì—„ê²©í•œ ì§€í‘œ
        
        Returns:
            1.0 if exact match, else 0.0
            
        Reference:
            Rajpurkar et al. (2016). SQuAD: 100,000+ Questions for 
            Machine Comprehension of Text. EMNLP 2016.
        """
        pred_norm = AcademicMetrics.normalize_text(pred)
        gold_norm = AcademicMetrics.normalize_text(gold)
        return 1.0 if pred_norm == gold_norm else 0.0
    
    # ============================================================
    # 2. Token F1 Score - SQuAD í‘œì¤€
    # ============================================================
    
    @staticmethod
    def token_f1_score(pred: str, gold: str) -> Dict[str, float]:
        """
        Token-level F1 Score
        
        í† í° ë‹¨ìœ„ ì •ë°€ë„(Precision)ì™€ ì¬í˜„ìœ¨(Recall)ì˜ ì¡°í™”í‰ê· 
        SQuAD, KorQuADì˜ ì£¼ìš” í‰ê°€ ì§€í‘œ
        
        Returns:
            {'precision': float, 'recall': float, 'f1': float}
            
        Reference:
            Rajpurkar et al. (2016). SQuAD: 100,000+ Questions for 
            Machine Comprehension of Text. EMNLP 2016.
        """
        pred_tokens = AcademicMetrics.tokenize(AcademicMetrics.normalize_text(pred))
        gold_tokens = AcademicMetrics.tokenize(AcademicMetrics.normalize_text(gold))
        
        if not pred_tokens or not gold_tokens:
            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}
        
        common = Counter(pred_tokens) & Counter(gold_tokens)
        num_same = sum(common.values())
        
        if num_same == 0:
            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}
        
        precision = num_same / len(pred_tokens)
        recall = num_same / len(gold_tokens)
        f1 = 2 * (precision * recall) / (precision + recall)
        
        return {
            'precision': round(precision, 4),
            'recall': round(recall, 4),
            'f1': round(f1, 4)
        }
    
    # ============================================================
    # 3. ROUGE-L - ìš”ì•½ í‰ê°€ í‘œì¤€
    # ============================================================
    
    @staticmethod
    def _lcs_length(x: List[str], y: List[str]) -> int:
        """ìµœì¥ ê³µí†µ ë¶€ë¶„ìˆ˜ì—´ (LCS) ê¸¸ì´"""
        m, n = len(x), len(y)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if x[i - 1] == y[j - 1]:
                    dp[i][j] = dp[i - 1][j - 1] + 1
                else:
                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])
        
        return dp[m][n]
    
    @staticmethod
    def rouge_l(pred: str, gold: str) -> Dict[str, float]:
        """
        ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation)
        
        ìµœì¥ ê³µí†µ ë¶€ë¶„ìˆ˜ì—´(LCS) ê¸°ë°˜ í‰ê°€
        ìˆœì„œë¥¼ ê³ ë ¤í•œ ìœ ì‚¬ë„ ì¸¡ì •
        
        Returns:
            {'precision': float, 'recall': float, 'f1': float}
            
        Reference:
            Lin, C. Y. (2004). ROUGE: A Package for Automatic Evaluation 
            of Summaries. ACL Workshop 2004.
        """
        pred_tokens = AcademicMetrics.tokenize(AcademicMetrics.normalize_text(pred))
        gold_tokens = AcademicMetrics.tokenize(AcademicMetrics.normalize_text(gold))
        
        if not pred_tokens or not gold_tokens:
            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}
        
        lcs_len = AcademicMetrics._lcs_length(pred_tokens, gold_tokens)
        
        if lcs_len == 0:
            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}
        
        precision = lcs_len / len(pred_tokens)
        recall = lcs_len / len(gold_tokens)
        f1 = 2 * (precision * recall) / (precision + recall)
        
        return {
            'precision': round(precision, 4),
            'recall': round(recall, 4),
            'f1': round(f1, 4)
        }
    
    # ============================================================
    # 4. BLEU - ê¸°ê³„ë²ˆì—­ í‘œì¤€
    # ============================================================
    
    @staticmethod
    def bleu_n(pred: str, gold: str, n: int = 2) -> float:
        """
        BLEU-n (Bilingual Evaluation Understudy)
        
        n-gram ì •ë°€ë„ ê¸°ë°˜ í‰ê°€
        ê¸°ê³„ë²ˆì—­ ë° í…ìŠ¤íŠ¸ ìƒì„± í‰ê°€ì˜ í‘œì¤€
        
        Args:
            n: n-gram í¬ê¸° (ê¸°ë³¸ 2)
        
        Returns:
            BLEU-n score (0.0 ~ 1.0)
            
        Reference:
            Papineni et al. (2002). BLEU: a Method for Automatic 
            Evaluation of Machine Translation. ACL 2002.
        """
        pred_tokens = AcademicMetrics.tokenize(AcademicMetrics.normalize_text(pred))
        gold_tokens = AcademicMetrics.tokenize(AcademicMetrics.normalize_text(gold))
        
        if len(pred_tokens) < n or len(gold_tokens) < n:
            return 0.0
        
        # n-gram ìƒì„±
        pred_ngrams = [tuple(pred_tokens[i:i+n]) for i in range(len(pred_tokens) - n + 1)]
        gold_ngrams = [tuple(gold_tokens[i:i+n]) for i in range(len(gold_tokens) - n + 1)]
        
        pred_counter = Counter(pred_ngrams)
        gold_counter = Counter(gold_ngrams)
        
        # Clipped counts
        overlap = pred_counter & gold_counter
        num_match = sum(overlap.values())
        num_pred = len(pred_ngrams)
        
        if num_pred == 0:
            return 0.0
        
        return round(num_match / num_pred, 4)
    
    # ============================================================
    # 5. ì¢…í•© í‰ê°€
    # ============================================================
    
    @staticmethod
    def evaluate_all(pred: str, gold: str) -> Dict[str, Any]:
        """
        ëª¨ë“  í•™ìˆ  ì§€í‘œë¡œ í‰ê°€
        
        Returns:
            Dict with all academic metrics
        """
        return {
            'exact_match': AcademicMetrics.exact_match(pred, gold),
            'token_f1': AcademicMetrics.token_f1_score(pred, gold),
            'rouge_l': AcademicMetrics.rouge_l(pred, gold),
            'bleu_1': AcademicMetrics.bleu_n(pred, gold, n=1),
            'bleu_2': AcademicMetrics.bleu_n(pred, gold, n=2),
        }


def main():
    """í…ŒìŠ¤íŠ¸"""
    import sys
    sys.stdout.reconfigure(encoding='utf-8')
    
    print("=" * 70)
    print("í•™ìˆ  ë…¼ë¬¸ìš© í‰ê°€ ì§€í‘œ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ
    pred = "ë°œì£¼ê¸°ê´€ì€ í•œêµ­ìˆ˜ìì›ê³µì‚¬ì´ë©°, ì‚¬ì—…ëª…ì€ ê¸ˆê°• ìœ ì—­ ìŠ¤ë§ˆíŠ¸ ì •ìˆ˜ì¥ì…ë‹ˆë‹¤."
    gold = "ë°œì£¼ê¸°ê´€ì€ í•œêµ­ìˆ˜ìì›ê³µì‚¬ì´ë©°, ì‚¬ì—…ëª…ì€ ê¸ˆê°• ìœ ì—­ ë‚¨ë¶€ ìŠ¤ë§ˆíŠ¸ ì •ìˆ˜ì¥ í™•ëŒ€ êµ¬ì¶• ìš©ì—­ì…ë‹ˆë‹¤."
    
    print(f"\nì˜ˆì¸¡: {pred}")
    print(f"ì •ë‹µ: {gold}")
    print("\n" + "=" * 70)
    
    metrics = AcademicMetrics.evaluate_all(pred, gold)
    
    print("\nğŸ“Š í•™ìˆ  í‰ê°€ ê²°ê³¼:")
    print(f"\n1. Exact Match (SQuAD)")
    print(f"   Score: {metrics['exact_match']:.3f}")
    
    print(f"\n2. Token F1 (SQuAD)")
    print(f"   Precision: {metrics['token_f1']['precision']:.3f}")
    print(f"   Recall:    {metrics['token_f1']['recall']:.3f}")
    print(f"   F1:        {metrics['token_f1']['f1']:.3f}")
    
    print(f"\n3. ROUGE-L (ACL 2004)")
    print(f"   Precision: {metrics['rouge_l']['precision']:.3f}")
    print(f"   Recall:    {metrics['rouge_l']['recall']:.3f}")
    print(f"   F1:        {metrics['rouge_l']['f1']:.3f}")
    
    print(f"\n4. BLEU (ACL 2002)")
    print(f"   BLEU-1: {metrics['bleu_1']:.3f}")
    print(f"   BLEU-2: {metrics['bleu_2']:.3f}")
    
    print("\n" + "=" * 70)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 70)


if __name__ == "__main__":
    main()

