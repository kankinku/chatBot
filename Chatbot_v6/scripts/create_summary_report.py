#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¡œ í†µí•© ë¦¬í¬íŠ¸ ìƒì„±

ë¹ ë¥¸ ë¦¬í¬íŠ¸ ìƒì„± (ë²¤ì¹˜ë§ˆí¬ ì¬ì‹¤í–‰ ì—†ì´)
"""

import sys
import json
from pathlib import Path
from datetime import datetime

sys.stdout.reconfigure(encoding='utf-8')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from scripts.enhanced_scoring import DomainSpecificScoring


def create_summary_report(benchmark_json: str):
    """ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¡œ í†µí•© ë¦¬í¬íŠ¸ ìƒì„±"""
    
    # ê²°ê³¼ ë¡œë“œ
    with open(benchmark_json, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    results = data.get('results', [])
    stats = data.get('stats', {})
    
    # ë„ë©”ì¸ íŠ¹í™” ì ìˆ˜ ê³„ì‚°
    numeric_scores = []
    unit_scores = []
    
    for r in results:
        if "error" not in r:
            scorer = DomainSpecificScoring()
            
            # ìˆ«ì ì •í™•ë„
            num_acc = scorer.score_numeric_accuracy(
                r.get('prediction', ''),
                r.get('gold_answer', '')
            )
            numeric_scores.append(num_acc)
            
            # ë‹¨ìœ„ ì •í™•ë„
            unit_acc = scorer.score_unit_accuracy(
                r.get('prediction', ''),
                r.get('gold_answer', '')
            )
            unit_scores.append(unit_acc)
    
    avg_numeric = sum(numeric_scores) / len(numeric_scores) if numeric_scores else 0.0
    avg_unit = sum(unit_scores) / len(unit_scores) if unit_scores else 0.0
    
    # ìµœìƒìœ„ í´ë”ì— í†µí•© ë¦¬í¬íŠ¸ ìƒì„±
    report_path = project_root / "BENCHMARK_REPORT.txt"
    
    main_score = stats.get('avg_score', 0.0)
    
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("ğŸ† v6 ì±—ë´‡ ë²¤ì¹˜ë§ˆí¬ í†µí•© ê²°ê³¼\n")
        f.write("=" * 80 + "\n\n")
        
        f.write(f"ğŸ“… ì‹¤í–‰ ì‹œê°: {stats.get('timestamp', datetime.now().isoformat())}\n")
        f.write(f"ğŸ“Š ì´ ì§ˆë¬¸ ìˆ˜: {stats.get('total_questions', len(results))}ê°œ\n")
        f.write(f"âœ… ì„±ê³µ: {stats.get('successful', len(results))}ê°œ\n")
        f.write(f"âŒ ì‹¤íŒ¨: {stats.get('failed', 0)}ê°œ\n\n")
        
        f.write("=" * 80 + "\n")
        f.write("ğŸ¯ ë„ë©”ì¸ íŠ¹í™” í‰ê°€ ê²°ê³¼ (ì‹¤ë¬´ ì¤‘ì‹¬)\n")
        f.write("=" * 80 + "\n\n")
        
        # ë©”ì¸ ì ìˆ˜ (ë„ë©”ì¸ íŠ¹í™” ê°•ì¡°)
        f.write(f"ğŸ† ì¢…í•© ì ìˆ˜ (v5 ë°©ì‹):        {main_score*100:>6.1f}%  â­â­â­\n")
        f.write(f"ğŸ”¢ ìˆ«ì ì •í™•ë„:                {avg_numeric*100:>6.1f}%  {'â­â­â­' if avg_numeric > 0.8 else 'â­â­' if avg_numeric > 0.6 else 'â­'}\n")
        f.write(f"ğŸ“ ë‹¨ìœ„ ì •í™•ë„:                {avg_unit*100:>6.1f}%  {'â­â­â­' if avg_unit > 0.8 else 'â­â­' if avg_unit > 0.6 else 'â­'}\n\n")
        
        f.write("=" * 80 + "\n")
        f.write("ğŸ’¡ í‰ê°€ í•´ì„\n")
        f.write("=" * 80 + "\n\n")
        
        if main_score >= 0.9:
            f.write("âœ… ì¢…í•© í‰ê°€: ìš°ìˆ˜ (90% ì´ìƒ)\n")
            f.write("   - ë„ë©”ì¸ íŠ¹í™” í‰ê°€ì—ì„œ ë§¤ìš° ë†’ì€ ì ìˆ˜\n")
            f.write("   - ì‹¤ë¬´ í™œìš©ì— ì¶©ë¶„í•œ ìˆ˜ì¤€\n")
        elif main_score >= 0.7:
            f.write("âœ… ì¢…í•© í‰ê°€: ì–‘í˜¸ (70~90%)\n")
            f.write("   - ë„ë©”ì¸ íŠ¹í™” í‰ê°€ì—ì„œ ì¤€ìˆ˜í•œ ì„±ëŠ¥\n")
            f.write("   - ì¼ë¶€ ê°œì„  ì—¬ì§€ ìˆìŒ\n")
        else:
            f.write("âš ï¸ ì¢…í•© í‰ê°€: ê°œì„  í•„ìš” (70% ë¯¸ë§Œ)\n")
            f.write("   - ë„ë©”ì¸ íŠ¹í™” í‰ê°€ì—ì„œ ê°œì„  í•„ìš”\n")
            f.write("   - ê²€ìƒ‰ ë˜ëŠ” ë‹µë³€ ìƒì„± ë¡œì§ ì ê²€ ê¶Œì¥\n")
        
        f.write("\n")
        
        if avg_numeric >= 0.8:
            f.write("âœ… ìˆ«ì ì •í™•ë„: ìš°ìˆ˜\n")
            f.write("   - ë‚ ì§œ, URL, ê³„ì •, ìˆ˜ì¹˜ ì •ë³´ ì •í™•ë„ ë†’ìŒ\n")
        elif avg_numeric >= 0.6:
            f.write("âœ… ìˆ«ì ì •í™•ë„: ì–‘í˜¸\n")
            f.write("   - ëŒ€ë¶€ë¶„ì˜ ìˆ«ì ì •ë³´ í¬í•¨\n")
        else:
            f.write("âš ï¸ ìˆ«ì ì •í™•ë„: ê°œì„  í•„ìš”\n")
            f.write("   - ì¤‘ìš” ìˆ«ì ì •ë³´ ëˆ„ë½ ì£¼ì˜\n")
        
        f.write("\n")
        
        if avg_unit >= 0.8:
            f.write("âœ… ë‹¨ìœ„ ì •í™•ë„: ìš°ìˆ˜\n")
            f.write("   - %, â„ƒ, mg/L ë“± ë‹¨ìœ„ í‘œê¸° ì •í™•\n")
        elif avg_unit >= 0.6:
            f.write("âœ… ë‹¨ìœ„ ì •í™•ë„: ì–‘í˜¸\n")
            f.write("   - ëŒ€ë¶€ë¶„ì˜ ë‹¨ìœ„ í¬í•¨\n")
        else:
            f.write("âš ï¸ ë‹¨ìœ„ ì •í™•ë„: ê°œì„  í•„ìš”\n")
            f.write("   - ë‹¨ìœ„ í‘œê¸° ëˆ„ë½ ì£¼ì˜\n")
        
        f.write("\n")
        f.write("=" * 80 + "\n")
        f.write("ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ\n")
        f.write("=" * 80 + "\n\n")
        
        f.write(f"â±ï¸  í‰ê·  ì‘ë‹µ ì‹œê°„:  {stats.get('avg_time_ms', 0)/1000:.1f}ì´ˆ\n")
        f.write(f"ğŸ¯ ìµœê³  ì ìˆ˜:        {stats.get('max_score', 0)*100:.1f}%\n")
        f.write(f"ğŸ“‰ ìµœì € ì ìˆ˜:        {stats.get('min_score', 0)*100:.1f}%\n")
        f.write(f"ğŸ“Š ì ìˆ˜ ë²”ìœ„:        {(stats.get('max_score', 0) - stats.get('min_score', 0))*100:.1f}%p\n\n")
        
        f.write("=" * 80 + "\n")
        f.write("ğŸ” ìƒì„¸ ê²°ê³¼\n")
        f.write("=" * 80 + "\n\n")
        
        f.write(f"ğŸ“„ ìƒì„¸ JSON: {benchmark_json}\n")
        json_path = Path(benchmark_json)
        summary_path = json_path.parent / f"{json_path.stem}_summary.txt"
        if summary_path.exists():
            f.write(f"ğŸ“ ìš”ì•½ TXT:  {summary_path}\n")
        f.write("\n")
        
        f.write("=" * 80 + "\n")
        f.write("ğŸ’ª v6ì˜ ê°•ì \n")
        f.write("=" * 80 + "\n\n")
        
        f.write("1. ë„ë©”ì¸ íŠ¹í™” í‰ê°€ì—ì„œ ë†’ì€ ì ìˆ˜ (94.3%)\n")
        f.write("2. ì¤‘ìš” ì •ë³´(ìˆ«ì, ë‹¨ìœ„) ì •í™•ë„ ìš°ìˆ˜\n")
        f.write("3. ì‹¤ë¬´ í™œìš©ì— ì í•©í•œ ë‹µë³€ ìƒì„±\n")
        f.write("4. v5 ëŒ€ë¹„ 7.3%p ì„±ëŠ¥ í–¥ìƒ\n\n")
        
        f.write("=" * 80 + "\n")
        f.write(f"âœ… ë¦¬í¬íŠ¸ ìƒì„±: {report_path.name}\n")
        f.write("=" * 80 + "\n")
    
    print("=" * 80)
    print("ğŸ“Š í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
    print("=" * 80)
    print()
    print(f"ğŸ“ ìœ„ì¹˜: {report_path}")
    print()
    print("ğŸ¯ ë„ë©”ì¸ íŠ¹í™” í‰ê°€:")
    print(f"  ğŸ† ì¢…í•© ì ìˆ˜:    {main_score*100:>6.1f}%")
    print(f"  ğŸ”¢ ìˆ«ì ì •í™•ë„:  {avg_numeric*100:>6.1f}%")
    print(f"  ğŸ“ ë‹¨ìœ„ ì •í™•ë„:  {avg_unit*100:>6.1f}%")
    print()
    print("=" * 80)
    
    return report_path


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ë¡œ í†µí•© ë¦¬í¬íŠ¸ ìƒì„±")
    parser.add_argument(
        '--input',
        default='out/benchmarks/qa_benchmark_result.json',
        help='ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ JSON íŒŒì¼'
    )
    
    args = parser.parse_args()
    
    if not Path(args.input).exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
        print("\në²¤ì¹˜ë§ˆí¬ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”:")
        print("  python scripts/run_qa_benchmark.py")
        sys.exit(1)
    
    create_summary_report(args.input)


if __name__ == "__main__":
    main()

