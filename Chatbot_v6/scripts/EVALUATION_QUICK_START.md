# í‰ê°€ ì‹œìŠ¤í…œ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ âš¡

5ë¶„ ì•ˆì— í‰ê°€ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

---

## ğŸš€ ê°€ì¥ ë¹ ë¥¸ ë°©ë²•

### 1ë‹¨ê³„: í†µí•© í‰ê°€ ëª¨ë“ˆ import

```python
from scripts.unified_evaluation import UnifiedEvaluator

evaluator = UnifiedEvaluator()
```

### 2ë‹¨ê³„: í‰ê°€ ì‹¤í–‰

```python
results = evaluator.evaluate_all(
    question="ì§ˆë¬¸",
    prediction="ìƒì„±ëœ ë‹µë³€",
    ground_truth="ì •ë‹µ",
    contexts=["ì°¸ê³  ìë£Œ 1", "ì°¸ê³  ìë£Œ 2"],  # ì„ íƒ ì‚¬í•­
    keywords=["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"]          # ì„ íƒ ì‚¬í•­
)
```

### 3ë‹¨ê³„: ê²°ê³¼ í™•ì¸

```python
evaluator.print_results(results)
```

ë! ğŸ‰

---

## ğŸ“Š 4ê°€ì§€ í‰ê°€ ì²´ê³„

| í‰ê°€ | íŒŒì¼ | ì£¼ìš” ì§€í‘œ | ì‚¬ìš© ì‹œê¸° |
|-----|------|----------|----------|
| **1. ê¸°ë³¸ Score (v5)** | `run_qa_benchmark.py` | ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì¢…í•© ì ìˆ˜ | ì‹¤ë¬´ ì„±ëŠ¥ |
| **2. ë„ë©”ì¸ íŠ¹í™”** | `enhanced_scoring.py` | ìˆ«ì/ë‹¨ìœ„ ì •í™•ë„ | ê¸°ìˆ  ì •ë³´ |
| **3. RAG í•µì‹¬ 3ëŒ€** | `rag_core_metrics.py` | Faithfulness, Correctness, Precision | ë…¼ë¬¸ ì‘ì„± |
| **4. í•™ìˆ  í‘œì¤€** | `academic_metrics.py` | F1, ROUGE, BLEU, EM | íƒ€ ì‹œìŠ¤í…œ ë¹„êµ |

---

## ğŸ’¡ ì£¼ìš” ì§€í‘œ í•œëˆˆì— ë³´ê¸°

### 1ï¸âƒ£ ê¸°ë³¸ Score (v5 ë°©ì‹)
```python
# ê°€ì¤‘ì¹˜: ìˆ«ì(1.5) > ë‹¨ìœ„(1.3) > í‚¤ì›Œë“œ(1.0)
score = benchmark.score_answer(prediction, ground_truth, keywords)
# ê²°ê³¼: 0.0 ~ 1.0
```

### 2ï¸âƒ£ ë„ë©”ì¸ íŠ¹í™”
```python
from scripts.enhanced_scoring import DomainSpecificScoring
scorer = DomainSpecificScoring()

# ì¢…í•© í‰ê°€
result = scorer.score_answer_v5_style(pred, gold, keywords)
# {'total_score': 0.95, 'numeric_score': 1.0, 'unit_score': 1.0, ...}

# ìˆ«ìë§Œ
numeric_acc = scorer.score_numeric_accuracy(pred, gold)  # 0.0 ~ 1.0

# ë‹¨ìœ„ë§Œ
unit_acc = scorer.score_unit_accuracy(pred, gold)  # 0.0 ~ 1.0
```

### 3ï¸âƒ£ RAG í•µì‹¬ 3ëŒ€ ì§€í‘œ
```python
from scripts.rag_core_metrics import RAGCoreMetrics

results = RAGCoreMetrics.evaluate_all(question, answer, ground_truth, contexts)
# {
#   'faithfulness': {'score': 0.85, ...},        # ì¶©ì‹¤ì„± (í™˜ê° ë°©ì§€)
#   'answer_correctness': {'score': 0.92, ...},  # ì •í™•ë„
#   'context_precision': {'score': 0.75, ...},   # ê²€ìƒ‰ íš¨ìœ¨ì„±
#   'overall_score': 0.84
# }
```

### 4ï¸âƒ£ í•™ìˆ  í‘œì¤€ ì§€í‘œ
```python
from scripts.academic_metrics import AcademicMetrics

metrics = AcademicMetrics.evaluate_all(pred, gold)
# {
#   'exact_match': 0.0,
#   'token_f1': {'f1': 0.90, ...},
#   'rouge_l': {'f1': 0.83, ...},
#   'bleu_1': 0.88,
#   'bleu_2': 0.75
# }
```

---

## ğŸ¯ ìƒí™©ë³„ ì‚¬ìš©ë²•

### âœ… ì‹¤ë¬´ ì„±ëŠ¥ ì¸¡ì •
```python
evaluator = UnifiedEvaluator()
results = evaluator.evaluate_all(...)
print(f"ì‹¤ë¬´ ì ìˆ˜: {results['summary']['basic_v5_score']*100:.1f}%")
```

### âœ… ë…¼ë¬¸ ì‘ì„±
```python
results = evaluator.evaluate_all(...)
print(f"Faithfulness: {results['summary']['faithfulness']*100:.1f}%")
print(f"Answer Correctness: {results['summary']['answer_correctness']*100:.1f}%")
print(f"Token F1: {results['summary']['token_f1']*100:.1f}%")
```

### âœ… ì—¬ëŸ¬ ì§ˆë¬¸ ë°°ì¹˜ í‰ê°€
```python
qa_pairs = [
    {'question': '...', 'prediction': '...', 'ground_truth': '...', ...},
    {'question': '...', 'prediction': '...', 'ground_truth': '...', ...},
]
batch_results = evaluator.evaluate_batch(qa_pairs)
evaluator.print_results(batch_results)
```

### âœ… ë‘ ë²„ì „ ë¹„êµ
```python
v5_results = evaluator.evaluate_all(..., prediction=v5_answer)
v6_results = evaluator.evaluate_all(..., prediction=v6_answer)

improvement = (
    v6_results['summary']['basic_v5_score'] - 
    v5_results['summary']['basic_v5_score']
) * 100
print(f"ê°œì„ í­: +{improvement:.1f}%p")
```

---

## ğŸ“ ì‹¤í–‰ ì˜ˆì œ

### í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸
```bash
# í†µí•© í‰ê°€ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
python scripts/unified_evaluation.py

# ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python scripts/example_evaluation.py
```

### QA ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
```bash
# ì „ì²´ QA ë°ì´í„° í‰ê°€ (ëª¨ë“  ì§€í‘œ í¬í•¨)
python scripts/run_qa_benchmark.py --qa data/qa.json --corpus data/corpus.jsonl
```

---

## ğŸ“– ë” ìì„¸í•œ ì •ë³´

- **ìƒì„¸ ê°€ì´ë“œ**: `scripts/EVALUATION_GUIDE.md`
- **ì˜ˆì œ ì½”ë“œ**: `scripts/example_evaluation.py`
- **RAG í‰ê°€**: `scripts/rag_core_metrics.py`
- **ë„ë©”ì¸ í‰ê°€**: `scripts/enhanced_scoring.py`
- **í•™ìˆ  ì§€í‘œ**: `scripts/academic_metrics.py`

---

## ğŸ“ ë…¼ë¬¸ ì¸ìš© ì˜ˆì‹œ

### RAG ì‹œìŠ¤í…œ í‰ê°€
```
ë³¸ ì—°êµ¬ì˜ RAG ì±—ë´‡ ì‹œìŠ¤í…œì„ 20ê°œ ì§ˆë¬¸ìœ¼ë¡œ í‰ê°€í•œ ê²°ê³¼,
Faithfulness 85.3%, Answer Correctness 92.1%, Context Precision 75.4%ë¥¼ 
ë‹¬ì„±í•˜ì˜€ë‹¤ (Es et al., 2023).
```

### ì°¸ê³ ë¬¸í—Œ
```
Es, S., James, J., Espinosa-Anke, L., & Schockaert, S. (2023).
RAGAS: Automated Evaluation of Retrieval Augmented Generation.
arXiv preprint arXiv:2309.15217.
```

---

## âš¡ í•µì‹¬ ìš”ì•½

```python
# ğŸ’¡ ì´ê²ƒë§Œ ê¸°ì–µí•˜ì„¸ìš”!
from scripts.unified_evaluation import UnifiedEvaluator

evaluator = UnifiedEvaluator()
results = evaluator.evaluate_all(
    question, prediction, ground_truth, contexts, keywords
)
evaluator.print_results(results)

# ëª¨ë“  ì§€í‘œë¥¼ í•œ ë²ˆì— í™•ì¸! ğŸ‰
```

---

**ì‘ì„±ì¼**: 2025-10-12  
**ë²„ì „**: v6

