#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© í‰ê°€ ëª¨ë“ˆ (Unified Evaluation Module)

ëª¨ë“  í‰ê°€ ì§€í‘œë¥¼ í•œ ë²ˆì— ì‹¤í–‰í•˜ê³  ë¹„êµí•  ìˆ˜ ìˆëŠ” í†µí•© ì¸í„°í˜ì´ìŠ¤
ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ì„œë„ ì‰½ê²Œ ì¬ì‚¬ìš© ê°€ëŠ¥

ì‚¬ìš© ì˜ˆì‹œ:
    from scripts.unified_evaluation import UnifiedEvaluator
    
    evaluator = UnifiedEvaluator()
    results = evaluator.evaluate_all(
        question="AI í”Œë«í¼ì˜ ê¸°ë³¸ ê´€ë¦¬ì ì•„ì´ë””ëŠ”?",
        prediction="ê¸°ë³¸ ê´€ë¦¬ì ì•„ì´ë””ëŠ” KWATERì…ë‹ˆë‹¤.",
        ground_truth="ê¸°ë³¸ ê´€ë¦¬ì ì•„ì´ë””ëŠ” KWATERì…ë‹ˆë‹¤.",
        contexts=["ê´€ë¦¬ì ê³„ì •: KWATER", "ì‹œìŠ¤í…œ ì ‘ì† ì •ë³´..."]
    )
    
    # ëª¨ë“  ì§€í‘œê°€ í¬í•¨ëœ ì¢…í•© ê²°ê³¼
    print(results)
"""

from typing import List, Dict, Any
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from scripts.academic_metrics import AcademicMetrics
from scripts.rag_core_metrics import RAGCoreMetrics
from scripts.enhanced_scoring import DomainSpecificScoring


class UnifiedEvaluator:
    """
    í†µí•© í‰ê°€ì
    
    4ê°€ì§€ í‰ê°€ ì²´ê³„ë¥¼ í•œ ë²ˆì— ì‹¤í–‰:
    1. ê¸°ë³¸ Score (v5 ë°©ì‹) - ë„ë©”ì¸ ê°€ì¤‘ì¹˜
    2. ë„ë©”ì¸ íŠ¹í™” í‰ê°€ - ìˆ«ì/ë‹¨ìœ„ ì •í™•ë„
    3. RAG í•µì‹¬ 3ëŒ€ ì§€í‘œ - Faithfulness, Correctness, Precision
    4. í•™ìˆ  í‘œì¤€ ì§€í‘œ - F1, ROUGE-L, BLEU, Exact Match
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.domain_scorer = DomainSpecificScoring()
        self.academic = AcademicMetrics()
        self.rag = RAGCoreMetrics()
    
    def evaluate_all(
        self,
        question: str,
        prediction: str,
        ground_truth: str,
        contexts: List[str] = None,
        keywords: List[str] = None
    ) -> Dict[str, Any]:
        """
        ëª¨ë“  í‰ê°€ ì§€í‘œë¥¼ í•œ ë²ˆì— ì‹¤í–‰
        
        Args:
            question: ì§ˆë¬¸
            prediction: ìƒì„±ëœ ë‹µë³€
            ground_truth: ì •ë‹µ
            contexts: ì°¸ê³  ìë£Œ ë¦¬ìŠ¤íŠ¸ (RAG í‰ê°€ì— í•„ìš”)
            keywords: í•„ìˆ˜ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì„ íƒ)
            
        Returns:
            {
                'basic_score': Dict,           # ê¸°ë³¸ Score (v5 ë°©ì‹)
                'domain_specific': Dict,       # ë„ë©”ì¸ íŠ¹í™” í‰ê°€
                'rag_metrics': Dict,          # RAG í•µì‹¬ 3ëŒ€ ì§€í‘œ
                'academic_metrics': Dict,      # í•™ìˆ  í‘œì¤€ ì§€í‘œ
                'summary': Dict               # ì£¼ìš” ì ìˆ˜ ìš”ì•½
            }
        """
        contexts = contexts or []
        keywords = keywords or []
        
        # 1. ê¸°ë³¸ Score (v5 ë°©ì‹)
        basic_score = self._evaluate_basic_v5(prediction, ground_truth, keywords)
        
        # 2. ë„ë©”ì¸ íŠ¹í™” í‰ê°€
        domain_specific = self.domain_scorer.score_answer_v5_style(
            prediction,
            ground_truth,
            keywords
        )
        
        # 3. RAG í•µì‹¬ 3ëŒ€ ì§€í‘œ
        rag_metrics = {}
        if contexts:
            rag_metrics = self.rag.evaluate_all(
                question,
                prediction,
                ground_truth,
                contexts
            )
        
        # 4. í•™ìˆ  í‘œì¤€ ì§€í‘œ
        academic_metrics = self.academic.evaluate_all(prediction, ground_truth)
        
        # ì£¼ìš” ì ìˆ˜ ìš”ì•½
        summary = self._create_summary(
            basic_score,
            domain_specific,
            rag_metrics,
            academic_metrics
        )
        
        return {
            'basic_score': basic_score,
            'domain_specific': domain_specific,
            'rag_metrics': rag_metrics,
            'academic_metrics': academic_metrics,
            'summary': summary
        }
    
    def _evaluate_basic_v5(
        self,
        prediction: str,
        gold_answer: str,
        keywords: List[str]
    ) -> Dict[str, Any]:
        """
        ê¸°ë³¸ Score (v5 ë°©ì‹) í‰ê°€
        
        run_qa_benchmark.pyì˜ score_answer ë¡œì§ê³¼ ë™ì¼
        """
        import re
        
        def normalize_text(t: str) -> str:
            return t.strip().lower()
        
        def units_equivalent(u1: str, u2: str) -> bool:
            """ë‹¨ìœ„ ë™ì˜ì–´ ì²´í¬"""
            synonyms = [
                {"mg/l", "ppm"},
                {"ã/l", "ppm", "mg/l"},
                {"â„ƒ", "Â°c", "ë„"},
            ]
            u1_lower = u1.lower().strip()
            u2_lower = u2.lower().strip()
            if u1_lower == u2_lower:
                return True
            for group in synonyms:
                if u1_lower in group and u2_lower in group:
                    return True
            return False
        
        p = normalize_text(prediction)
        g = normalize_text(gold_answer)
        
        # ì •ë‹µì´ "ì—†ìŒ"ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if g in {"ì—†ìŒ", "ì—†ë‹¤", "ì—†ìŠµë‹ˆë‹¤", "none", "no"}:
            score = 1.0 if p.startswith("ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤") or p.startswith("ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤") else 0.0
            return {
                'score': score,
                'type': 'negative_answer',
                'details': {}
            }
        
        # v5 ë¡œì§: numeric + unit + keyword ê°€ì¤‘ì¹˜ ì ìš©
        keywords_set = set(re.findall(r"[\w\-/%Â°â„ƒ]+", g))
        nums = set(re.findall(r"\d+(?:[\.,]\d+)?", g))
        units = set(re.findall(r"[a-z%Â°â„ƒ/ã]+", g, re.IGNORECASE))
        
        hit = 0.0
        total = 0.0
        
        numeric_hit = 0.0
        unit_hit = 0.0
        keyword_hit = 0.0
        
        # numericì— ë†’ì€ ê°€ì¤‘ì¹˜ (1.5)
        if nums:
            total += 1.5
            if any(n in p for n in nums):
                hit += 1.5
                numeric_hit = 1.0
        
        # unitsì— ê°€ì¤‘ì¹˜ (1.3)
        if units:
            total += 1.3
            uh = 0.0
            for u in units:
                if u.lower() in p:
                    uh = 1.3
                    break
            # unit synonym ë§¤í•‘ ì‹œë„
            if uh == 0.0:
                for u in units:
                    for v in ["mg/l", "ppm", "â„ƒ", "Â°c", "ã/l"]:
                        if v in p and units_equivalent(u, v):
                            uh = 1.3
                            break
                    if uh > 0:
                        break
            hit += uh
            if uh > 0:
                unit_hit = 1.0
        
        # general keywords (1.0 ê°€ì¤‘ì¹˜)
        kw = {k for k in keywords_set if k not in nums and k not in units and len(k) >= 2}
        if kw:
            total += 1.0
            if any(k in p for k in kw):
                hit += 1.0
                keyword_hit = 1.0
        
        final_score = (hit / total) if total > 0 else 0.0
        
        return {
            'score': round(final_score, 4),
            'type': 'weighted',
            'numeric_hit': numeric_hit,
            'unit_hit': unit_hit,
            'keyword_hit': keyword_hit,
            'details': {
                'nums': list(nums),
                'units': list(units),
                'keywords': list(kw)[:5]
            }
        }
    
    def _create_summary(
        self,
        basic: Dict,
        domain: Dict,
        rag: Dict,
        academic: Dict
    ) -> Dict[str, Any]:
        """ì£¼ìš” ì ìˆ˜ ìš”ì•½ ìƒì„±"""
        summary = {
            # ê¸°ë³¸ ì ìˆ˜
            'basic_v5_score': basic.get('score', 0.0),
            
            # ë„ë©”ì¸ íŠ¹í™”
            'domain_total_score': domain.get('total_score', 0.0),
            'numeric_accuracy': domain.get('numeric_score', 0.0),
            'unit_accuracy': domain.get('unit_score', 0.0),
            'keyword_accuracy': domain.get('keyword_score', 0.0),
            
            # í•™ìˆ  ì§€í‘œ
            'token_f1': academic.get('token_f1', {}).get('f1', 0.0),
            'rouge_l': academic.get('rouge_l', {}).get('f1', 0.0),
            'bleu_2': academic.get('bleu_2', 0.0),
            'exact_match': academic.get('exact_match', 0.0),
        }
        
        # RAG ì§€í‘œ (ìˆëŠ” ê²½ìš°)
        if rag:
            summary.update({
                'faithfulness': rag.get('faithfulness', {}).get('score', 0.0),
                'answer_correctness': rag.get('answer_correctness', {}).get('score', 0.0),
                'context_precision': rag.get('context_precision', {}).get('score', 0.0),
                'rag_overall': rag.get('overall_score', 0.0),
            })
        
        return summary
    
    def evaluate_batch(
        self,
        qa_pairs: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ QA ìŒì„ ë°°ì¹˜ë¡œ í‰ê°€
        
        Args:
            qa_pairs: QA ìŒ ë¦¬ìŠ¤íŠ¸
                [
                    {
                        'question': str,
                        'prediction': str,
                        'ground_truth': str,
                        'contexts': List[str],  # optional
                        'keywords': List[str]   # optional
                    },
                    ...
                ]
        
        Returns:
            {
                'individual_results': List[Dict],  # ê° ì§ˆë¬¸ë³„ í‰ê°€ ê²°ê³¼
                'aggregated_stats': Dict           # ì „ì²´ í†µê³„
            }
        """
        results = []
        
        for item in qa_pairs:
            result = self.evaluate_all(
                question=item['question'],
                prediction=item['prediction'],
                ground_truth=item['ground_truth'],
                contexts=item.get('contexts', []),
                keywords=item.get('keywords', [])
            )
            results.append(result)
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        stats = self._aggregate_stats(results)
        
        return {
            'individual_results': results,
            'aggregated_stats': stats
        }
    
    def _aggregate_stats(self, results: List[Dict]) -> Dict[str, Any]:
        """ë°°ì¹˜ ê²°ê³¼ì˜ í‰ê·  í†µê³„ ê³„ì‚°"""
        if not results:
            return {}
        
        summaries = [r['summary'] for r in results]
        
        # ê° ì§€í‘œì˜ í‰ê·  ê³„ì‚°
        stats = {}
        for key in summaries[0].keys():
            values = [s.get(key, 0.0) for s in summaries]
            stats[f'avg_{key}'] = sum(values) / len(values) if values else 0.0
            stats[f'min_{key}'] = min(values) if values else 0.0
            stats[f'max_{key}'] = max(values) if values else 0.0
        
        stats['total_evaluated'] = len(results)
        
        return stats
    
    def print_results(self, results: Dict[str, Any], detailed: bool = False):
        """
        í‰ê°€ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥
        
        Args:
            results: evaluate_all() ë˜ëŠ” evaluate_batch()ì˜ ê²°ê³¼
            detailed: ìƒì„¸ ì •ë³´ ì¶œë ¥ ì—¬ë¶€
        """
        print("=" * 80)
        print("ğŸ“Š í†µí•© í‰ê°€ ê²°ê³¼")
        print("=" * 80)
        
        # ë°°ì¹˜ ê²°ê³¼ì¸ ê²½ìš°
        if 'aggregated_stats' in results:
            stats = results['aggregated_stats']
            print(f"\nì´ í‰ê°€ ì§ˆë¬¸ ìˆ˜: {stats['total_evaluated']}ê°œ\n")
            
            print("=" * 80)
            print("í‰ê·  ì ìˆ˜ ìš”ì•½")
            print("=" * 80)
            
            # ì£¼ìš” ì§€í‘œë§Œ ì¶œë ¥
            key_metrics = [
                ('avg_basic_v5_score', 'ê¸°ë³¸ Score (v5 ë°©ì‹)'),
                ('avg_domain_total_score', 'ë„ë©”ì¸ íŠ¹í™” ì¢…í•©'),
                ('avg_numeric_accuracy', 'ìˆ«ì ì •í™•ë„'),
                ('avg_unit_accuracy', 'ë‹¨ìœ„ ì •í™•ë„'),
                ('avg_token_f1', 'Token F1 (SQuAD)'),
                ('avg_rouge_l', 'ROUGE-L'),
                ('avg_faithfulness', 'Faithfulness (ì¶©ì‹¤ì„±)'),
                ('avg_answer_correctness', 'Answer Correctness'),
                ('avg_context_precision', 'Context Precision'),
            ]
            
            for key, label in key_metrics:
                if key in stats:
                    print(f"{label:30s}: {stats[key]*100:6.1f}%")
            
            return
        
        # ë‹¨ì¼ ê²°ê³¼ì¸ ê²½ìš°
        summary = results.get('summary', {})
        
        print("\n1ï¸âƒ£  ê¸°ë³¸ Score (v5 ë°©ì‹)")
        print(f"   ì¢…í•© ì ìˆ˜: {summary.get('basic_v5_score', 0)*100:.1f}%")
        
        print("\n2ï¸âƒ£  ë„ë©”ì¸ íŠ¹í™” í‰ê°€")
        print(f"   ì¢…í•© ì ìˆ˜: {summary.get('domain_total_score', 0)*100:.1f}%")
        print(f"   ìˆ«ì ì •í™•ë„: {summary.get('numeric_accuracy', 0)*100:.1f}%")
        print(f"   ë‹¨ìœ„ ì •í™•ë„: {summary.get('unit_accuracy', 0)*100:.1f}%")
        print(f"   í‚¤ì›Œë“œ ì •í™•ë„: {summary.get('keyword_accuracy', 0)*100:.1f}%")
        
        if summary.get('faithfulness') is not None:
            print("\n3ï¸âƒ£  RAG í•µì‹¬ 3ëŒ€ ì§€í‘œ")
            print(f"   Faithfulness (ì¶©ì‹¤ì„±): {summary.get('faithfulness', 0)*100:.1f}%")
            print(f"   Answer Correctness (ì •í™•ë„): {summary.get('answer_correctness', 0)*100:.1f}%")
            print(f"   Context Precision (ì •ë°€ë„): {summary.get('context_precision', 0)*100:.1f}%")
            print(f"   RAG ì¢…í•© ì ìˆ˜: {summary.get('rag_overall', 0)*100:.1f}%")
        
        print("\n4ï¸âƒ£  í•™ìˆ  í‘œì¤€ ì§€í‘œ")
        print(f"   Token F1: {summary.get('token_f1', 0)*100:.1f}%")
        print(f"   ROUGE-L: {summary.get('rouge_l', 0)*100:.1f}%")
        print(f"   BLEU-2: {summary.get('bleu_2', 0)*100:.1f}%")
        print(f"   Exact Match: {summary.get('exact_match', 0)*100:.1f}%")
        
        if detailed:
            print("\n" + "=" * 80)
            print("ìƒì„¸ ì •ë³´")
            print("=" * 80)
            
            import json
            print(json.dumps(results, indent=2, ensure_ascii=False))
        
        print("\n" + "=" * 80)


def main():
    """ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸"""
    import sys
    sys.stdout.reconfigure(encoding='utf-8')
    
    print("=" * 80)
    print("í†µí•© í‰ê°€ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_cases = [
        {
            'question': 'AI í”Œë«í¼ì˜ ê¸°ë³¸ ê´€ë¦¬ì ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ëŠ”?',
            'prediction': 'ê¸°ë³¸ ê´€ë¦¬ì ê³„ì •ì˜ ì•„ì´ë””ëŠ” KWATERì´ê³  ë¹„ë°€ë²ˆí˜¸ë„ KWATERì…ë‹ˆë‹¤.',
            'ground_truth': 'ê¸°ë³¸ ê´€ë¦¬ì ê³„ì •ì˜ ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ëŠ” ëª¨ë‘ KWATERì…ë‹ˆë‹¤.',
            'contexts': [
                'ê¸°ë³¸ ê´€ë¦¬ì ê³„ì •: ì•„ì´ë”” KWATER, ë¹„ë°€ë²ˆí˜¸ KWATER',
                'AI í”Œë«í¼ ì ‘ì† ì£¼ì†ŒëŠ” http://waio-portal-vip:10011 ì…ë‹ˆë‹¤.',
            ],
            'keywords': ['KWATER', 'ê´€ë¦¬ì', 'ì•„ì´ë””', 'ë¹„ë°€ë²ˆí˜¸']
        },
        {
            'question': 'ìˆ˜ì§ˆ ê¸°ì¤€ ì˜¨ë„ëŠ”?',
            'prediction': 'ìˆ˜ì§ˆ ê¸°ì¤€ ì˜¨ë„ëŠ” 25â„ƒì…ë‹ˆë‹¤.',
            'ground_truth': 'ìˆ˜ì§ˆ ê¸°ì¤€ ì˜¨ë„ëŠ” 25â„ƒì…ë‹ˆë‹¤.',
            'contexts': [
                'ìˆ˜ì§ˆ ê²€ì‚¬ ê¸°ì¤€: ì˜¨ë„ 25â„ƒ, pH 7.0',
            ],
            'keywords': ['ìˆ˜ì§ˆ', 'ì˜¨ë„', '25', 'â„ƒ']
        }
    ]
    
    # í†µí•© í‰ê°€ì ìƒì„±
    evaluator = UnifiedEvaluator()
    
    print("\n[í…ŒìŠ¤íŠ¸ 1] ë‹¨ì¼ í‰ê°€")
    print("-" * 80)
    result = evaluator.evaluate_all(**test_cases[0])
    evaluator.print_results(result)
    
    print("\n\n[í…ŒìŠ¤íŠ¸ 2] ë°°ì¹˜ í‰ê°€")
    print("-" * 80)
    batch_results = evaluator.evaluate_batch(test_cases)
    evaluator.print_results(batch_results)
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()

