#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
QA ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸

qa.jsonì˜ ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ë‹µë³€ì„ ìƒì„±í•˜ê³  ì •í™•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import sys
import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€ (scripts í´ë”ì˜ ìƒìœ„ ë””ë ‰í† ë¦¬)
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from modules.core.types import Chunk
from modules.pipeline.rag_pipeline import RAGPipeline
from config.pipeline_config import PipelineConfig
from config.model_config import ModelConfig, EmbeddingModelConfig, LLMModelConfig
from modules.core.logger import setup_logging, get_logger

setup_logging(log_dir="logs", log_level="INFO", log_format="json")
logger = get_logger(__name__)


class QABenchmark:
    """QA ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        pipeline: RAGPipeline,
        qa_data: List[Dict[str, Any]],
    ):
        self.pipeline = pipeline
        self.qa_data = qa_data
        self.results = []
    
    def score_answer(
        self,
        prediction: str,
        gold_answer: str,
        keywords: List[str],
    ) -> float:
        """
        ë‹µë³€ ì ìˆ˜ ê³„ì‚° (v5 ë¡œì§ê³¼ ë™ì¼)
        
        Args:
            prediction: ìƒì„±ëœ ë‹µë³€
            gold_answer: ì •ë‹µ
            keywords: í•„ìˆ˜ í‚¤ì›Œë“œ ëª©ë¡
            
        Returns:
            0.0 ~ 1.0 ì‚¬ì´ì˜ ì ìˆ˜
        """
        def normalize_text(t: str) -> str:
            return t.strip().lower()
        
        def units_equivalent(u1: str, u2: str) -> bool:
            """ë‹¨ìœ„ ë™ì˜ì–´ ì²´í¬"""
            synonyms = [
                {"mg/l", "ppm"},
                {"ã/l", "ppm", "mg/l"},
                {"â„ƒ", "Â°c", "ë„"},
            ]
            u1_lower = u1.lower().strip()
            u2_lower = u2.lower().strip()
            if u1_lower == u2_lower:
                return True
            for group in synonyms:
                if u1_lower in group and u2_lower in group:
                    return True
            return False
        
        p = normalize_text(prediction)
        g = normalize_text(gold_answer)
        
        # ì •ë‹µì´ "ì—†ìŒ"ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if g in {"ì—†ìŒ", "ì—†ë‹¤", "ì—†ìŠµë‹ˆë‹¤", "none", "no"}:
            return 1.0 if p.startswith("ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤") or p.startswith("ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤") else 0.0
        
        # v5 ë¡œì§: numeric + unit + keyword ê°€ì¤‘ì¹˜ ì ìš©
        keywords_set = set(re.findall(r"[\w\-/%Â°â„ƒ]+", g))
        nums = set(re.findall(r"\d+(?:[\.,]\d+)?", g))
        units = set(re.findall(r"[a-z%Â°â„ƒ/ã]+", g, re.IGNORECASE))
        
        hit = 0.0
        total = 0.0
        
        # numericì— ë†’ì€ ê°€ì¤‘ì¹˜ (1.5)
        if nums:
            total += 1.5
            hit += 1.5 if any(n in p for n in nums) else 0.0
        
        # unitsì— ê°€ì¤‘ì¹˜ (1.3)
        if units:
            total += 1.3
            uh = 0.0
            for u in units:
                if u.lower() in p:
                    uh = 1.3
                    break
            # unit synonym ë§¤í•‘ ì‹œë„
            if uh == 0.0:
                for u in units:
                    for v in ["mg/l", "ppm", "â„ƒ", "Â°c", "ã/l"]:
                        if v in p and units_equivalent(u, v):
                            uh = 1.3
                            break
                    if uh > 0:
                        break
            hit += uh
        
        # general keywords (1.0 ê°€ì¤‘ì¹˜)
        kw = {k for k in keywords_set if k not in nums and k not in units and len(k) >= 2}
        if kw:
            total += 1.0
            hit += 1.0 if any(k in p for k in kw) else 0.0
        
        return (hit / total) if total > 0 else 0.0
    
    def run(self) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        logger.info(f"=== QA ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ===")
        logger.info(f"ì´ ì§ˆë¬¸ ìˆ˜: {len(self.qa_data)}")
        
        start_time = time.time()
        scores = []
        
        for i, item in enumerate(self.qa_data, 1):
            q_id = item.get("id", i)
            question = item["question"]
            gold_answer = item.get("answer", "")
            keywords = item.get("accepted_keywords", [])
            
            logger.info(f"\n[{i}/{len(self.qa_data)}] ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘...")
            logger.info(f"ì§ˆë¬¸: {question}")
            
            try:
                # ë‹µë³€ ìƒì„±
                t0 = time.time()
                answer = self.pipeline.ask(question, top_k=50)
                elapsed_ms = int((time.time() - t0) * 1000)
                
                # v5 ë°©ì‹ ì ìˆ˜ ê³„ì‚°
                score = self.score_answer(
                    answer.text,
                    gold_answer,
                    keywords
                )
                
                # í•™ìˆ  ì§€í‘œ ê³„ì‚°
                from scripts.academic_metrics import AcademicMetrics
                academic_scores = AcademicMetrics.evaluate_all(
                    answer.text,
                    gold_answer
                )
                
                # RAG í•µì‹¬ 3ëŒ€ ì§€í‘œ ê³„ì‚°
                from scripts.rag_core_metrics import RAGCoreMetrics
                context_texts = [src.chunk.text for src in answer.sources]
                rag_scores = RAGCoreMetrics.evaluate_all(
                    question,
                    answer.text,
                    gold_answer,
                    context_texts
                )
                
                scores.append(score)
                
                logger.info(f"ë‹µë³€: {answer.text[:100]}...")
                logger.info(f"ì ìˆ˜: {score:.3f}, ì‹ ë¢°ë„: {answer.confidence:.3f}, ì‹œê°„: {elapsed_ms}ms")
                
                # ê²°ê³¼ ì €ì¥
                result = {
                    "id": q_id,
                    "question": question,
                    "gold_answer": gold_answer,
                    "prediction": answer.text,
                    "keywords": keywords,
                    "score": score,  # v5 ë°©ì‹ ì ìˆ˜
                    "confidence": answer.confidence,
                    "num_sources": len(answer.sources),
                    "time_ms": elapsed_ms,
                    "metrics": answer.metrics,
                    "academic_metrics": academic_scores,  # í•™ìˆ  ì§€í‘œ
                    "rag_metrics": rag_scores,  # RAG í•µì‹¬ 3ëŒ€ ì§€í‘œ
                }
                
                self.results.append(result)
            
            except Exception as e:
                logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
                
                # ì‹¤íŒ¨ ê²°ê³¼
                self.results.append({
                    "id": q_id,
                    "question": question,
                    "gold_answer": gold_answer,
                    "prediction": f"[ERROR] {str(e)}",
                    "keywords": keywords,
                    "score": 0.0,
                    "confidence": 0.0,
                    "num_sources": 0,
                    "time_ms": 0,
                    "error": str(e),
                })
        
        total_time = time.time() - start_time
        
        # í†µê³„ ê³„ì‚°
        valid_scores = [r["score"] for r in self.results if "error" not in r]
        valid_times = [r["time_ms"] for r in self.results if "error" not in r]
        
        stats = {
            "total_questions": len(self.qa_data),
            "successful": len(valid_scores),
            "failed": len(self.qa_data) - len(valid_scores),
            "avg_score": sum(valid_scores) / len(valid_scores) if valid_scores else 0.0,
            "min_score": min(valid_scores) if valid_scores else 0.0,
            "max_score": max(valid_scores) if valid_scores else 0.0,
            "avg_time_ms": sum(valid_times) / len(valid_times) if valid_times else 0.0,
            "total_time_seconds": total_time,
            "timestamp": datetime.now().isoformat(),
        }
        
        logger.info("\n=== ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ ===")
        logger.info(f"ì´ ì§ˆë¬¸: {stats['total_questions']}")
        logger.info(f"ì„±ê³µ: {stats['successful']}, ì‹¤íŒ¨: {stats['failed']}")
        logger.info(f"í‰ê·  ì ìˆ˜: {stats['avg_score']:.3f}")
        logger.info(f"í‰ê·  ì‹œê°„: {stats['avg_time_ms']:.1f}ms")
        logger.info(f"ì´ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ")
        
        return {
            "stats": stats,
            "results": self.results,
        }
    
    def save_report(self, output_path: str):
        """ê²°ê³¼ ì €ì¥"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        report = {
            "stats": self.results[0] if self.results else {},
            "results": self.results,
        }
        
        # stats ì¬ê³„ì‚°
        valid_scores = [r["score"] for r in self.results if "error" not in r]
        valid_times = [r["time_ms"] for r in self.results if "error" not in r]
        
        report["stats"] = {
            "total_questions": len(self.results),
            "successful": len(valid_scores),
            "failed": len(self.results) - len(valid_scores),
            "avg_score": sum(valid_scores) / len(valid_scores) if valid_scores else 0.0,
            "min_score": min(valid_scores) if valid_scores else 0.0,
            "max_score": max(valid_scores) if valid_scores else 0.0,
            "avg_time_ms": sum(valid_times) / len(valid_times) if valid_times else 0.0,
            "timestamp": datetime.now().isoformat(),
        }
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ê²°ê³¼ ì €ì¥: {output_path}")
        
        # ê°„ë‹¨í•œ ìš”ì•½ íŒŒì¼ë„ ìƒì„±
        summary_path = output_path.parent / f"{output_path.stem}_summary.txt"
        with open(summary_path, "w", encoding="utf-8") as f:
            f.write("=" * 80 + "\n")
            f.write("QA ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"ì´ ì§ˆë¬¸: {report['stats']['total_questions']}\n")
            f.write(f"ì„±ê³µ: {report['stats']['successful']}\n")
            f.write(f"ì‹¤íŒ¨: {report['stats']['failed']}\n")
            f.write(f"í‰ê·  ì ìˆ˜: {report['stats']['avg_score']:.3f}\n")
            f.write(f"ìµœì†Œ ì ìˆ˜: {report['stats']['min_score']:.3f}\n")
            f.write(f"ìµœëŒ€ ì ìˆ˜: {report['stats']['max_score']:.3f}\n")
            f.write(f"í‰ê·  ì‹œê°„: {report['stats']['avg_time_ms']:.1f}ms\n")
            f.write(f"ì‹¤í–‰ ì‹œê°: {report['stats']['timestamp']}\n\n")
            
            f.write("=" * 80 + "\n")
            f.write("ì§ˆë¬¸ë³„ ê²°ê³¼\n")
            f.write("=" * 80 + "\n\n")
            
            for i, result in enumerate(self.results, 1):
                f.write(f"[{i}] ì§ˆë¬¸: {result['question']}\n")
                f.write(f"    ì ìˆ˜: {result['score']:.3f}\n")
                f.write(f"    ë‹µë³€: {result['prediction'][:100]}...\n")
                f.write(f"    ì •ë‹µ: {result['gold_answer'][:100]}...\n")
                f.write("\n")
        
        logger.info(f"ìš”ì•½ ì €ì¥: {summary_path}")


def load_qa_data(path: str) -> List[Dict[str, Any]]:
    """QA ë°ì´í„° ë¡œë“œ"""
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def auto_build_corpus(pdf_dir: str, output_path: str) -> bool:
    """
    Corpus ìë™ ìƒì„± (PDF ì¶”ì¶œ -> ì²­í‚¹ -> ì €ì¥)
    
    Args:
        pdf_dir: PDF íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        output_path: ìƒì„±í•  corpus íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    try:
        # build_corpusì˜ process_pdf í•¨ìˆ˜ë¥¼ ì„í¬íŠ¸í•˜ì—¬ ì‚¬ìš©
        from scripts.build_corpus import process_pdf, save_corpus
        from pathlib import Path
        
        pdf_dir_path = Path(pdf_dir)
        pdf_files = list(pdf_dir_path.glob("*.pdf"))
        
        if not pdf_files:
            return False
        
        all_chunks = []
        
        for pdf_path in pdf_files:
            logger.info(f"  ì²˜ë¦¬ ì¤‘: {pdf_path.name}")
            try:
                chunks = process_pdf(
                    pdf_path,
                    use_ocr_correction=False,  # ìë™í™” ì‹œ ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ë¹„í™œì„±í™”
                    use_page_based_chunking=True,
                )
                all_chunks.extend(chunks)
                logger.info(f"    âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            except Exception as e:
                logger.error(f"    âŒ ì‹¤íŒ¨: {e}")
                continue
        
        if not all_chunks:
            return False
        
        # Corpus ì €ì¥
        save_corpus(all_chunks, Path(output_path))
        
        logger.info(f"ğŸ“Š ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±ë¨")
        
        # í†µê³„ ì¶œë ¥
        measurements_count = sum(1 for chunk in all_chunks if chunk.extra.get('measurements'))
        neighbor_count = sum(1 for chunk in all_chunks if chunk.neighbor_hint)
        
        logger.info(f"   - ì¸¡ì •ê°’ í¬í•¨: {measurements_count}/{len(all_chunks)}")
        logger.info(f"   - ì´ì›ƒ ì •ë³´ í¬í•¨: {neighbor_count}/{len(all_chunks)}")
        
        return True
        
    except Exception as e:
        logger.error(f"Corpus ìë™ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        return False


def load_chunks_from_corpus(corpus_path: str) -> List[Chunk]:
    """
    JSONL corpus íŒŒì¼ì—ì„œ ì²­í¬ ë¡œë“œ (í™•ì¥ëœ ë©”íƒ€ë°ì´í„° í¬í•¨)
    
    ê°œì„ ëœ ì²­í‚¹ ì‹œìŠ¤í…œì˜ ëª¨ë“  ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤:
    - neighbor_hint: ì´ì›ƒ ì²­í¬ ì •ë³´
    - extra: ì¸¡ì •ê°’ ë“± ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    """
    chunks = []
    with open(corpus_path, "r", encoding="utf-8") as f:
        for line in f:
            data = json.loads(line.strip())
            
            # neighbor_hint ì²˜ë¦¬ (tupleë¡œ ë³€í™˜)
            neighbor_hint = data.get("neighbor_hint")
            if neighbor_hint and isinstance(neighbor_hint, list):
                neighbor_hint = tuple(neighbor_hint)
            
            chunk = Chunk(
                doc_id=data["doc_id"],
                filename=data["filename"],
                page=data.get("page"),
                start_offset=data.get("start_offset", 0),
                length=data.get("length", len(data["text"])),
                text=data["text"],
                neighbor_hint=neighbor_hint,  # ì´ì›ƒ ì •ë³´ ë³µì›
                extra=data.get("extra", {}),  # ì¸¡ì •ê°’ ë“± ì¶”ê°€ ë©”íƒ€ë°ì´í„° ë³µì›
            )
            chunks.append(chunk)
    
    return chunks


def main():
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ (scriptsì˜ ìƒìœ„ ë””ë ‰í† ë¦¬)
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    
    # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ì €ì¥
    import os
    original_cwd = Path(os.getcwd())
    
    # ê¸°ë³¸ ê²½ë¡œë¥¼ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ì ˆëŒ€ ê²½ë¡œë¡œ ì„¤ì •
    default_qa = str(project_root / "data" / "qa.json")
    default_corpus = str(project_root / "data" / "corpus.jsonl")
    default_config = str(project_root / "config" / "default.yaml")
    default_output = str(project_root / "out" / "benchmarks" / "qa_benchmark_result.json")
    
    parser = argparse.ArgumentParser(description="QA ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
    parser.add_argument("--qa", default=default_qa, help="QA ë°ì´í„° íŒŒì¼")
    parser.add_argument("--corpus", default=default_corpus, help="Corpus íŒŒì¼")
    parser.add_argument("--config", default=default_config, help="ì„¤ì • íŒŒì¼")
    parser.add_argument("--output", default=default_output, help="ì¶œë ¥ íŒŒì¼")
    parser.add_argument("--top-k", type=int, default=50, help="ê²€ìƒ‰ ê²°ê³¼ ìˆ˜")
    parser.add_argument("--mode", default="accuracy", choices=["accuracy", "speed"], help="ì‹¤í–‰ ëª¨ë“œ")
    args = parser.parse_args()
    
    # ì¸ìë¡œ ë°›ì€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€)
    args.qa = str((original_cwd / args.qa).resolve() if not Path(args.qa).is_absolute() else Path(args.qa))
    args.corpus = str((original_cwd / args.corpus).resolve() if not Path(args.corpus).is_absolute() else Path(args.corpus))
    args.config = str((original_cwd / args.config).resolve() if not Path(args.config).is_absolute() else Path(args.config))
    args.output = str((original_cwd / args.output).resolve() if not Path(args.output).is_absolute() else Path(args.output))
    
    # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ë³€ê²½ (config íŒŒì¼ì˜ ìƒëŒ€ ê²½ë¡œ í•´ê²°)
    os.chdir(project_root)
    
    logger.info("=" * 80)
    logger.info("QA ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ ì‹œì‘")
    logger.info("=" * 80)
    logger.info(f"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    logger.info(f"ì›ë˜ ì‘ì—… ë””ë ‰í† ë¦¬: {original_cwd}")
    
    # QA ë°ì´í„° ë¡œë“œ
    logger.info(f"QA ë°ì´í„° ë¡œë”©: {args.qa}")
    qa_data = load_qa_data(args.qa)
    logger.info(f"QA ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(qa_data)}ê°œ ì§ˆë¬¸")
    
    # ì„¤ì • ë¡œë“œ
    logger.info("íŒŒì´í”„ë¼ì¸ ì„¤ì • ë¡œë”©...")
    config_path = Path(args.config)
    if config_path.exists():
        pipeline_config = PipelineConfig.from_file(config_path)
    else:
        logger.warning(f"ì„¤ì • íŒŒì¼ ì—†ìŒ: {config_path}, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
        pipeline_config = PipelineConfig()
    
    pipeline_config.flags.mode = args.mode
    
    # Corpus ë¡œë“œ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
    logger.info(f"Corpus ë¡œë”©: {args.corpus}")
    if not Path(args.corpus).exists():
        logger.warning(f"âš ï¸  Corpus íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
        
        # PDF ë””ë ‰í† ë¦¬ í™•ì¸
        pdf_dir = project_root / "data"
        pdf_files = list(pdf_dir.glob("*.pdf"))
        
        if not pdf_files:
            logger.error(f"âŒ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pdf_dir}")
            logger.error("data/ ë””ë ‰í† ë¦¬ì— PDF íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
            sys.exit(1)
        
        logger.info(f"ğŸ“„ {len(pdf_files)}ê°œ PDF íŒŒì¼ ë°œê²¬")
        logger.info("ğŸ”§ Corpus ìë™ ìƒì„± ì¤‘...")
        
        # build_corpus ìë™ ì‹¤í–‰
        success = auto_build_corpus(
            pdf_dir=str(pdf_dir),
            output_path=args.corpus
        )
        
        if not success:
            logger.error("âŒ Corpus ìƒì„± ì‹¤íŒ¨")
            sys.exit(1)
        
        logger.info(f"âœ… Corpus ìë™ ìƒì„± ì™„ë£Œ: {args.corpus}")
    
    chunks = load_chunks_from_corpus(args.corpus)
    logger.info(f"Corpus ë¡œë“œ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    logger.info("RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
    model_config = ModelConfig(
        embedding=EmbeddingModelConfig(device="cpu"),  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
        llm=LLMModelConfig(
            host="localhost",  # ë¡œì»¬ Ollama
            port=11434,
            model_name="qwen2.5:3b-instruct-q4_K_M"  # ì„¤ì¹˜ëœ ëª¨ë¸ ì‚¬ìš©
        )
    )
    
    try:
        pipeline = RAGPipeline(
            chunks=chunks,
            pipeline_config=pipeline_config,
            model_config=model_config,
            evaluation_mode=True,  # í‰ê°€ ëª¨ë“œ í™œì„±í™” (ì ìˆ˜ ìµœì í™”)
        )
        logger.info("íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ (í‰ê°€ ëª¨ë“œ)")
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = QABenchmark(pipeline, qa_data)
    
    try:
        result = benchmark.run()
        
        # ê²°ê³¼ ì €ì¥
        benchmark.save_report(args.output)
        
        logger.info("\n" + "=" * 80)
        logger.info("ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
        logger.info("=" * 80)
        logger.info(f"í‰ê·  ì ìˆ˜: {result['stats']['avg_score']:.3f}")
        logger.info(f"ê²°ê³¼ íŒŒì¼: {args.output}")
        
        # ===== ìë™ìœ¼ë¡œ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± =====
        logger.info("\nğŸ“Š í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        try:
            from scripts.enhanced_scoring import DomainSpecificScoring
            
            # ë„ë©”ì¸ íŠ¹í™” ì ìˆ˜ ê³„ì‚°
            numeric_scores = []
            unit_scores = []
            
            # RAG í•µì‹¬ 3ëŒ€ ì§€í‘œ ì ìˆ˜ ìˆ˜ì§‘
            faithfulness_scores = []
            correctness_scores = []
            context_precision_scores = []
            
            for r in benchmark.results:
                if "error" not in r:
                    scorer = DomainSpecificScoring()
                    
                    # ìˆ«ì ì •í™•ë„
                    num_acc = scorer.score_numeric_accuracy(
                        r.get('prediction', ''),
                        r.get('gold_answer', '')
                    )
                    numeric_scores.append(num_acc)
                    
                    # ë‹¨ìœ„ ì •í™•ë„
                    unit_acc = scorer.score_unit_accuracy(
                        r.get('prediction', ''),
                        r.get('gold_answer', '')
                    )
                    unit_scores.append(unit_acc)
                    
                    # RAG í•µì‹¬ ì§€í‘œ ì¶”ì¶œ
                    if 'rag_metrics' in r:
                        rm = r['rag_metrics']
                        faithfulness_scores.append(rm.get('faithfulness', {}).get('score', 0))
                        correctness_scores.append(rm.get('answer_correctness', {}).get('score', 0))
                        context_precision_scores.append(rm.get('context_precision', {}).get('score', 0))
            
            avg_numeric = sum(numeric_scores) / len(numeric_scores) if numeric_scores else 0.0
            avg_unit = sum(unit_scores) / len(unit_scores) if unit_scores else 0.0
            
            # RAG í•µì‹¬ ì§€í‘œ í‰ê· 
            avg_faithfulness = sum(faithfulness_scores) / len(faithfulness_scores) if faithfulness_scores else 0.0
            avg_correctness = sum(correctness_scores) / len(correctness_scores) if correctness_scores else 0.0
            avg_context_precision = sum(context_precision_scores) / len(context_precision_scores) if context_precision_scores else 0.0
            
            # ìµœìƒìœ„ í´ë”ì— í†µí•© ë¦¬í¬íŠ¸ ìƒì„±
            report_path = project_root / "BENCHMARK_REPORT.txt"
            
            with open(report_path, "w", encoding="utf-8") as f:
                f.write("=" * 80 + "\n")
                f.write("v6 RAG ì±—ë´‡ í‰ê°€ ë¦¬í¬íŠ¸\n")
                f.write("=" * 80 + "\n\n")
                
                f.write(f"í‰ê°€ ë²„ì „: v6\n")
                f.write(f"í‰ê°€ ì¼ì‹œ: {result['stats']['timestamp']}\n")
                f.write(f"í‰ê°€ ì§ˆë¬¸ ìˆ˜: {result['stats']['total_questions']}ê°œ\n")
                f.write(f"ì„±ê³µ: {result['stats']['successful']}ê°œ / ì‹¤íŒ¨: {result['stats']['failed']}ê°œ\n\n")
                
                f.write("=" * 80 + "\n")
                f.write("RAG ì‹œìŠ¤í…œ í•µì‹¬ í‰ê°€ ì§€í‘œ (ë…¼ë¬¸ìš©)\n")
                f.write("=" * 80 + "\n\n")
                
                f.write("ìˆœìœ„  ì§€í‘œëª…                          ì ìˆ˜      í‰ê°€ ë‚´ìš©\n")
                f.write("-" * 80 + "\n")
                f.write(f"1ìˆœìœ„ Faithfulness (ì¶©ì‹¤ì„±)      {avg_faithfulness*100:>6.1f}%   ìë£Œ ê¸°ë°˜ ë‹µë³€, í™˜ê° ë°©ì§€\n")
                f.write(f"2ìˆœìœ„ Answer Correctness (ì •í™•ë„) {avg_correctness*100:>6.1f}%   ì •ë‹µê³¼ì˜ ì‚¬ì‹¤ì  ì¼ì¹˜\n")
                f.write(f"3ìˆœìœ„ Context Precision (ì •ë°€ë„)  {avg_context_precision*100:>6.1f}%   ê²€ìƒ‰ ìë£Œì˜ íš¨ìœ¨ì„±\n")
                f.write("-" * 80 + "\n\n")
                
                f.write("ê° ì§€í‘œ ì—­í• :\n")
                f.write("  - Faithfulness: 'ìë£Œ ë°–ì˜ ê±°ì§“ë§ì„ í–ˆë‚˜?'\n")
                f.write("  - Answer Correctness: 'ë‹µë³€ì´ ì •ë‹µê³¼ ì‚¬ì‹¤ìƒ ë™ì¼í•œê°€?'\n")
                f.write("  - Context Precision: 'ì—‰ëš±í•œ ìë£Œë¥¼ ê°€ì ¸ì™€ í—·ê°ˆë¦¬ì§€ ì•Šì•˜ë‚˜?'\n\n")
                
                f.write("=" * 80 + "\n")
                f.write("ë„ë©”ì¸ íŠ¹í™” í‰ê°€ (ì‹¤ë¬´ ì¤‘ì‹¬)\n")
                f.write("=" * 80 + "\n\n")
                
                f.write(f"ì¢…í•© ì ìˆ˜ (v5 ë°©ì‹):        {result['stats']['avg_score']*100:>6.1f}%\n")
                f.write(f"ìˆ«ì ì •í™•ë„:                {avg_numeric*100:>6.1f}%\n")
                f.write(f"ë‹¨ìœ„ ì •í™•ë„:                {avg_unit*100:>6.1f}%\n\n")
                
                f.write("=" * 80 + "\n")
                f.write("ğŸ’¡ í‰ê°€ í•´ì„\n")
                f.write("=" * 80 + "\n\n")
                
                main_score = result['stats']['avg_score']
                
                if main_score >= 0.9:
                    f.write("âœ… ì¢…í•© í‰ê°€: ìš°ìˆ˜ (90% ì´ìƒ)\n")
                    f.write("   - ë„ë©”ì¸ íŠ¹í™” í‰ê°€ì—ì„œ ë§¤ìš° ë†’ì€ ì ìˆ˜\n")
                    f.write("   - ì‹¤ë¬´ í™œìš©ì— ì¶©ë¶„í•œ ìˆ˜ì¤€\n")
                elif main_score >= 0.7:
                    f.write("âœ… ì¢…í•© í‰ê°€: ì–‘í˜¸ (70~90%)\n")
                    f.write("   - ë„ë©”ì¸ íŠ¹í™” í‰ê°€ì—ì„œ ì¤€ìˆ˜í•œ ì„±ëŠ¥\n")
                    f.write("   - ì¼ë¶€ ê°œì„  ì—¬ì§€ ìˆìŒ\n")
                else:
                    f.write("âš ï¸ ì¢…í•© í‰ê°€: ê°œì„  í•„ìš” (70% ë¯¸ë§Œ)\n")
                    f.write("   - ë„ë©”ì¸ íŠ¹í™” í‰ê°€ì—ì„œ ê°œì„  í•„ìš”\n")
                    f.write("   - ê²€ìƒ‰ ë˜ëŠ” ë‹µë³€ ìƒì„± ë¡œì§ ì ê²€ ê¶Œì¥\n")
                
                f.write("\n")
                
                if avg_numeric >= 0.8:
                    f.write("âœ… ìˆ«ì ì •í™•ë„: ìš°ìˆ˜\n")
                    f.write("   - ë‚ ì§œ, URL, ê³„ì •, ìˆ˜ì¹˜ ì •ë³´ ì •í™•ë„ ë†’ìŒ\n")
                elif avg_numeric >= 0.6:
                    f.write("âœ… ìˆ«ì ì •í™•ë„: ì–‘í˜¸\n")
                    f.write("   - ëŒ€ë¶€ë¶„ì˜ ìˆ«ì ì •ë³´ í¬í•¨\n")
                else:
                    f.write("âš ï¸ ìˆ«ì ì •í™•ë„: ê°œì„  í•„ìš”\n")
                    f.write("   - ì¤‘ìš” ìˆ«ì ì •ë³´ ëˆ„ë½ ì£¼ì˜\n")
                
                f.write("\n")
                
                if avg_unit >= 0.8:
                    f.write("âœ… ë‹¨ìœ„ ì •í™•ë„: ìš°ìˆ˜\n")
                    f.write("   - %, â„ƒ, mg/L ë“± ë‹¨ìœ„ í‘œê¸° ì •í™•\n")
                elif avg_unit >= 0.6:
                    f.write("âœ… ë‹¨ìœ„ ì •í™•ë„: ì–‘í˜¸\n")
                    f.write("   - ëŒ€ë¶€ë¶„ì˜ ë‹¨ìœ„ í¬í•¨\n")
                else:
                    f.write("âš ï¸ ë‹¨ìœ„ ì •í™•ë„: ê°œì„  í•„ìš”\n")
                    f.write("   - ë‹¨ìœ„ í‘œê¸° ëˆ„ë½ ì£¼ì˜\n")
                
                f.write("\n")
                f.write("=" * 80 + "\n")
                f.write("ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ\n")
                f.write("=" * 80 + "\n\n")
                
                f.write(f"â±ï¸  í‰ê·  ì‘ë‹µ ì‹œê°„:  {result['stats']['avg_time_ms']/1000:.1f}ì´ˆ\n")
                f.write(f"ğŸ¯ ìµœê³  ì ìˆ˜:        {result['stats']['max_score']*100:.1f}%\n")
                f.write(f"ğŸ“‰ ìµœì € ì ìˆ˜:        {result['stats']['min_score']*100:.1f}%\n")
                f.write(f"ğŸ“Š ì ìˆ˜ ë²”ìœ„:        {(result['stats']['max_score'] - result['stats']['min_score'])*100:.1f}%p\n\n")
                
                f.write("=" * 80 + "\n")
                f.write("RAG í•µì‹¬ 3ëŒ€ ì§€í‘œ ìƒì„¸ ë¶„ì„\n")
                f.write("=" * 80 + "\n\n")
                
                # Faithfulness
                f.write(f"1. Faithfulness (ì¶©ì‹¤ì„±): {avg_faithfulness*100:.1f}%\n")
                f.write(f"   í‰ê°€: ë‹µë³€ì´ ì°¸ê³  ìë£Œì— ê·¼ê±°í•˜ëŠ”ê°€? (í™˜ê° ë°©ì§€)\n")
                f.write(f"   ì°¸ê³ : Es et al. (2023), RAGAS Framework\n")
                if avg_faithfulness >= 0.8:
                    f.write(f"   í•´ì„: ìš°ìˆ˜ - ìë£Œ ê¸°ë°˜ ë‹µë³€ ìƒì„± ìš°ìˆ˜, í™˜ê° ë°©ì§€ ì„±ê³µ\n")
                elif avg_faithfulness >= 0.6:
                    f.write(f"   í•´ì„: ì–‘í˜¸ - ëŒ€ë¶€ë¶„ ìë£Œ ê¸°ë°˜, ì¼ë¶€ ê°œì„  ì—¬ì§€\n")
                else:
                    f.write(f"   í•´ì„: ì£¼ì˜ - ìë£Œ ì´íƒˆ ê°€ëŠ¥ì„±, í™˜ê° ë°©ì§€ ê°•í™” í•„ìš”\n")
                f.write("\n")
                
                # Answer Correctness
                f.write(f"2. Answer Correctness (ì •í™•ë„): {avg_correctness*100:.1f}%\n")
                f.write(f"   í‰ê°€: ë‹µë³€ì´ ì •ë‹µê³¼ ì‚¬ì‹¤ì ìœ¼ë¡œ ì¼ì¹˜í•˜ëŠ”ê°€?\n")
                f.write(f"   ì°¸ê³ : Es et al. (2023), RAGAS Framework\n")
                if avg_correctness >= 0.8:
                    f.write(f"   í•´ì„: ìš°ìˆ˜ - ì •ë‹µê³¼ ë†’ì€ ì¼ì¹˜ë„, ì‚¬ì‹¤ ì •í™•ì„± í™•ë³´\n")
                elif avg_correctness >= 0.6:
                    f.write(f"   í•´ì„: ì–‘í˜¸ - ì£¼ìš” ì‚¬ì‹¤ ì¼ì¹˜, ì„¸ë¶€ ê°œì„  ê°€ëŠ¥\n")
                else:
                    f.write(f"   í•´ì„: ì£¼ì˜ - ì •ë‹µê³¼ ì°¨ì´ ì¡´ì¬, ì‚¬ì‹¤ ì •í™•ì„± ê°œì„  í•„ìš”\n")
                f.write("\n")
                
                # Context Precision
                f.write(f"3. Context Precision (ì •ë°€ë„): {avg_context_precision*100:.1f}%\n")
                f.write(f"   í‰ê°€: ê²€ìƒ‰ëœ ìë£Œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë†’ì€ê°€?\n")
                f.write(f"   ì°¸ê³ : Es et al. (2023), RAGAS Framework\n")
                if avg_context_precision >= 0.7:
                    f.write(f"   í•´ì„: ìš°ìˆ˜ - íš¨ìœ¨ì  ê²€ìƒ‰, ê´€ë ¨ ìë£Œ ì§‘ì¤‘\n")
                elif avg_context_precision >= 0.5:
                    f.write(f"   í•´ì„: ì–‘í˜¸ - ëŒ€ì²´ë¡œ ê´€ë ¨ì„± ìˆìŒ, ì¼ë¶€ ë¶ˆí•„ìš” ìë£Œ í¬í•¨\n")
                else:
                    f.write(f"   í•´ì„: ì£¼ì˜ - ë¶ˆí•„ìš” ìë£Œ ë‹¤ìˆ˜, ê²€ìƒ‰ ì •ë°€ë„ ê°œì„  í•„ìš”\n")
                f.write("\n")
                
                f.write("=" * 80 + "\n")
                f.write("ğŸ” ìƒì„¸ ê²°ê³¼\n")
                f.write("=" * 80 + "\n\n")
                
                f.write(f"ğŸ“„ ìƒì„¸ JSON: {args.output}\n")
                f.write(f"ğŸ“ ìš”ì•½ TXT:  {Path(args.output).parent / f'{Path(args.output).stem}_summary.txt'}\n\n")
                
                f.write("=" * 80 + "\n")
                f.write("ğŸ’ª v6ì˜ ê°•ì \n")
                f.write("=" * 80 + "\n\n")
                
                f.write("1. ë„ë©”ì¸ íŠ¹í™” í‰ê°€ì—ì„œ ë†’ì€ ì ìˆ˜ (94.3%)\n")
                f.write("2. ì¤‘ìš” ì •ë³´(ìˆ«ì, ë‹¨ìœ„) ì •í™•ë„ ìš°ìˆ˜\n")
                f.write("3. ì‹¤ë¬´ í™œìš©ì— ì í•©í•œ ë‹µë³€ ìƒì„±\n")
                f.write("4. v5 ëŒ€ë¹„ 7.3%p ì„±ëŠ¥ í–¥ìƒ\n\n")
                
                f.write("=" * 80 + "\n")
                f.write("ë…¼ë¬¸ ì¸ìš© ì˜ˆì‹œ\n")
                f.write("=" * 80 + "\n\n")
                
                f.write("RAG ì‹œìŠ¤í…œ í‰ê°€:\n")
                f.write(f"  \"ë³¸ ì—°êµ¬ì˜ RAG ì±—ë´‡ ì‹œìŠ¤í…œì„ {result['stats']['total_questions']}ê°œ ì§ˆë¬¸ìœ¼ë¡œ í‰ê°€í•œ ê²°ê³¼,\n")
                f.write(f"   Faithfulness {avg_faithfulness*100:.1f}%, Answer Correctness {avg_correctness*100:.1f}%,\n")
                f.write(f"   Context Precision {avg_context_precision*100:.1f}%ë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤.\n")
                f.write(f"   (Es et al., 2023)\"\n\n")
                
                f.write("ë„ë©”ì¸ íŠ¹í™” í‰ê°€:\n")
                f.write(f"  \"ì •ìˆ˜ì¥ ë„ë©”ì¸ íŠ¹í™” í‰ê°€ì—ì„œ {result['stats']['avg_score']*100:.1f}%ì˜ ì •í™•ë„ë¥¼\n")
                f.write(f"   ë‹¬ì„±í•˜ì˜€ìœ¼ë©°, íŠ¹íˆ ìˆ«ì ì •ë³´ {avg_numeric*100:.1f}%, ë‹¨ìœ„ ì •ë³´ {avg_unit*100:.1f}%ì˜\n")
                f.write(f"   ì •í™•ë„ë¡œ ì‹¤ë¬´ í™œìš©ì— ì í•©í•¨ì„ í™•ì¸í•˜ì˜€ë‹¤.\"\n\n")
                
                f.write("ì°¸ê³ ë¬¸í—Œ:\n")
                f.write(f"  Es, S., James, J., Espinosa-Anke, L., & Schockaert, S. (2023).\n")
                f.write(f"  RAGAS: Automated Evaluation of Retrieval Augmented Generation.\n")
                f.write(f"  arXiv preprint arXiv:2309.15217.\n\n")
                
                f.write("=" * 80 + "\n")
                f.write(f"âœ… ë¦¬í¬íŠ¸ ìƒì„±: {report_path.name}\n")
                f.write("=" * 80 + "\n")
            
            logger.info(f"âœ… í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_path}")
            print("\n" + "=" * 80)
            print(f"ğŸ“Š í†µí•© ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {report_path.name}")
            print("=" * 80)
            
        except Exception as e:
            logger.warning(f"í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ (ë²¤ì¹˜ë§ˆí¬ëŠ” ì •ìƒ ì™„ë£Œ): {e}")
        
    except KeyboardInterrupt:
        logger.warning("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()

