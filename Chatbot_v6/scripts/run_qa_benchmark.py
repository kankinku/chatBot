#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
QA ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸

qa.jsonì˜ ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ë‹µë³€ì„ ìƒì„±í•˜ê³  ì •í™•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import sys
import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€ (scripts í´ë”ì˜ ìƒìœ„ ë””ë ‰í† ë¦¬)
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from modules.core.types import Chunk
from modules.pipeline.rag_pipeline import RAGPipeline
from config.pipeline_config import PipelineConfig
from config.model_config import ModelConfig, EmbeddingModelConfig, LLMModelConfig
from modules.core.logger import setup_logging, get_logger

setup_logging(log_dir="logs", log_level="INFO", log_format="json")
logger = get_logger(__name__)


class QABenchmark:
    """QA ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        pipeline: RAGPipeline,
        qa_data: List[Dict[str, Any]],
    ):
        self.pipeline = pipeline
        self.qa_data = qa_data
        self.results = []
    
    def score_answer(
        self,
        prediction: str,
        gold_answer: str,
        keywords: List[str],
    ) -> float:
        """
        ë‹µë³€ ì ìˆ˜ ê³„ì‚° (v5 ë¡œì§ê³¼ ë™ì¼)
        
        Args:
            prediction: ìƒì„±ëœ ë‹µë³€
            gold_answer: ì •ë‹µ
            keywords: í•„ìˆ˜ í‚¤ì›Œë“œ ëª©ë¡
            
        Returns:
            0.0 ~ 1.0 ì‚¬ì´ì˜ ì ìˆ˜
        """
        def normalize_text(t: str) -> str:
            return t.strip().lower()
        
        def units_equivalent(u1: str, u2: str) -> bool:
            """ë‹¨ìœ„ ë™ì˜ì–´ ì²´í¬"""
            synonyms = [
                {"mg/l", "ppm"},
                {"ã/l", "ppm", "mg/l"},
                {"â„ƒ", "Â°c", "ë„"},
            ]
            u1_lower = u1.lower().strip()
            u2_lower = u2.lower().strip()
            if u1_lower == u2_lower:
                return True
            for group in synonyms:
                if u1_lower in group and u2_lower in group:
                    return True
            return False
        
        p = normalize_text(prediction)
        g = normalize_text(gold_answer)
        
        # ì •ë‹µì´ "ì—†ìŒ"ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if g in {"ì—†ìŒ", "ì—†ë‹¤", "ì—†ìŠµë‹ˆë‹¤", "none", "no"}:
            return 1.0 if p.startswith("ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤") or p.startswith("ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤") else 0.0
        
        # v5 ë¡œì§: numeric + unit + keyword ê°€ì¤‘ì¹˜ ì ìš©
        keywords_set = set(re.findall(r"[\w\-/%Â°â„ƒ]+", g))
        nums = set(re.findall(r"\d+(?:[\.,]\d+)?", g))
        units = set(re.findall(r"[a-z%Â°â„ƒ/ã]+", g, re.IGNORECASE))
        
        hit = 0.0
        total = 0.0
        
        # numericì— ë†’ì€ ê°€ì¤‘ì¹˜ (1.5)
        if nums:
            total += 1.5
            hit += 1.5 if any(n in p for n in nums) else 0.0
        
        # unitsì— ê°€ì¤‘ì¹˜ (1.3)
        if units:
            total += 1.3
            uh = 0.0
            for u in units:
                if u.lower() in p:
                    uh = 1.3
                    break
            # unit synonym ë§¤í•‘ ì‹œë„
            if uh == 0.0:
                for u in units:
                    for v in ["mg/l", "ppm", "â„ƒ", "Â°c", "ã/l"]:
                        if v in p and units_equivalent(u, v):
                            uh = 1.3
                            break
                    if uh > 0:
                        break
            hit += uh
        
        # general keywords (1.0 ê°€ì¤‘ì¹˜)
        kw = {k for k in keywords_set if k not in nums and k not in units and len(k) >= 2}
        if kw:
            total += 1.0
            hit += 1.0 if any(k in p for k in kw) else 0.0
        
        return (hit / total) if total > 0 else 0.0
    
    def run(self) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        logger.info(f"=== QA ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ===")
        logger.info(f"ì´ ì§ˆë¬¸ ìˆ˜: {len(self.qa_data)}")
        
        start_time = time.time()
        scores = []
        
        for i, item in enumerate(self.qa_data, 1):
            q_id = item.get("id", i)
            question = item["question"]
            gold_answer = item.get("answer", "")
            keywords = item.get("accepted_keywords", [])
            
            logger.info(f"\n[{i}/{len(self.qa_data)}] ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘...")
            logger.info(f"ì§ˆë¬¸: {question}")
            
            try:
                # ë‹µë³€ ìƒì„±
                t0 = time.time()
                answer = self.pipeline.ask(question, top_k=50)
                elapsed_ms = int((time.time() - t0) * 1000)
                
                # v5 ë°©ì‹ ì ìˆ˜ ê³„ì‚°
                score = self.score_answer(
                    answer.text,
                    gold_answer,
                    keywords
                )
                
                # í•™ìˆ  ì§€í‘œ ê³„ì‚°
                from scripts.academic_metrics import AcademicMetrics
                academic_scores = AcademicMetrics.evaluate_all(
                    answer.text,
                    gold_answer
                )
                
                scores.append(score)
                
                logger.info(f"ë‹µë³€: {answer.text[:100]}...")
                logger.info(f"ì ìˆ˜: {score:.3f}, ì‹ ë¢°ë„: {answer.confidence:.3f}, ì‹œê°„: {elapsed_ms}ms")
                
                # ê²°ê³¼ ì €ì¥
                result = {
                    "id": q_id,
                    "question": question,
                    "gold_answer": gold_answer,
                    "prediction": answer.text,
                    "keywords": keywords,
                    "score": score,  # v5 ë°©ì‹ ì ìˆ˜
                    "confidence": answer.confidence,
                    "num_sources": len(answer.sources),
                    "time_ms": elapsed_ms,
                    "metrics": answer.metrics,
                    "academic_metrics": academic_scores,  # í•™ìˆ  ì§€í‘œ ì¶”ê°€
                }
                
                self.results.append(result)
            
            except Exception as e:
                logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
                
                # ì‹¤íŒ¨ ê²°ê³¼
                self.results.append({
                    "id": q_id,
                    "question": question,
                    "gold_answer": gold_answer,
                    "prediction": f"[ERROR] {str(e)}",
                    "keywords": keywords,
                    "score": 0.0,
                    "confidence": 0.0,
                    "num_sources": 0,
                    "time_ms": 0,
                    "error": str(e),
                })
        
        total_time = time.time() - start_time
        
        # í†µê³„ ê³„ì‚°
        valid_scores = [r["score"] for r in self.results if "error" not in r]
        valid_times = [r["time_ms"] for r in self.results if "error" not in r]
        
        stats = {
            "total_questions": len(self.qa_data),
            "successful": len(valid_scores),
            "failed": len(self.qa_data) - len(valid_scores),
            "avg_score": sum(valid_scores) / len(valid_scores) if valid_scores else 0.0,
            "min_score": min(valid_scores) if valid_scores else 0.0,
            "max_score": max(valid_scores) if valid_scores else 0.0,
            "avg_time_ms": sum(valid_times) / len(valid_times) if valid_times else 0.0,
            "total_time_seconds": total_time,
            "timestamp": datetime.now().isoformat(),
        }
        
        logger.info("\n=== ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ ===")
        logger.info(f"ì´ ì§ˆë¬¸: {stats['total_questions']}")
        logger.info(f"ì„±ê³µ: {stats['successful']}, ì‹¤íŒ¨: {stats['failed']}")
        logger.info(f"í‰ê·  ì ìˆ˜: {stats['avg_score']:.3f}")
        logger.info(f"í‰ê·  ì‹œê°„: {stats['avg_time_ms']:.1f}ms")
        logger.info(f"ì´ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ")
        
        return {
            "stats": stats,
            "results": self.results,
        }
    
    def save_report(self, output_path: str):
        """ê²°ê³¼ ì €ì¥"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        report = {
            "stats": self.results[0] if self.results else {},
            "results": self.results,
        }
        
        # stats ì¬ê³„ì‚°
        valid_scores = [r["score"] for r in self.results if "error" not in r]
        valid_times = [r["time_ms"] for r in self.results if "error" not in r]
        
        report["stats"] = {
            "total_questions": len(self.results),
            "successful": len(valid_scores),
            "failed": len(self.results) - len(valid_scores),
            "avg_score": sum(valid_scores) / len(valid_scores) if valid_scores else 0.0,
            "min_score": min(valid_scores) if valid_scores else 0.0,
            "max_score": max(valid_scores) if valid_scores else 0.0,
            "avg_time_ms": sum(valid_times) / len(valid_times) if valid_times else 0.0,
            "timestamp": datetime.now().isoformat(),
        }
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ê²°ê³¼ ì €ì¥: {output_path}")
        
        # ê°„ë‹¨í•œ ìš”ì•½ íŒŒì¼ë„ ìƒì„±
        summary_path = output_path.parent / f"{output_path.stem}_summary.txt"
        with open(summary_path, "w", encoding="utf-8") as f:
            f.write("=" * 80 + "\n")
            f.write("QA ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"ì´ ì§ˆë¬¸: {report['stats']['total_questions']}\n")
            f.write(f"ì„±ê³µ: {report['stats']['successful']}\n")
            f.write(f"ì‹¤íŒ¨: {report['stats']['failed']}\n")
            f.write(f"í‰ê·  ì ìˆ˜: {report['stats']['avg_score']:.3f}\n")
            f.write(f"ìµœì†Œ ì ìˆ˜: {report['stats']['min_score']:.3f}\n")
            f.write(f"ìµœëŒ€ ì ìˆ˜: {report['stats']['max_score']:.3f}\n")
            f.write(f"í‰ê·  ì‹œê°„: {report['stats']['avg_time_ms']:.1f}ms\n")
            f.write(f"ì‹¤í–‰ ì‹œê°: {report['stats']['timestamp']}\n\n")
            
            f.write("=" * 80 + "\n")
            f.write("ì§ˆë¬¸ë³„ ê²°ê³¼\n")
            f.write("=" * 80 + "\n\n")
            
            for i, result in enumerate(self.results, 1):
                f.write(f"[{i}] ì§ˆë¬¸: {result['question']}\n")
                f.write(f"    ì ìˆ˜: {result['score']:.3f}\n")
                f.write(f"    ë‹µë³€: {result['prediction'][:100]}...\n")
                f.write(f"    ì •ë‹µ: {result['gold_answer'][:100]}...\n")
                f.write("\n")
        
        logger.info(f"ìš”ì•½ ì €ì¥: {summary_path}")


def load_qa_data(path: str) -> List[Dict[str, Any]]:
    """QA ë°ì´í„° ë¡œë“œ"""
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def load_chunks_from_corpus(corpus_path: str) -> List[Chunk]:
    """JSONL corpus íŒŒì¼ì—ì„œ ì²­í¬ ë¡œë“œ"""
    chunks = []
    with open(corpus_path, "r", encoding="utf-8") as f:
        for line in f:
            data = json.loads(line.strip())
            chunk = Chunk(
                doc_id=data["doc_id"],
                filename=data["filename"],
                page=data.get("page", 0),
                start_offset=data.get("start_offset", 0),
                length=data.get("length", len(data["text"])),
                text=data["text"],
            )
            chunks.append(chunk)
    
    return chunks


def main():
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ (scriptsì˜ ìƒìœ„ ë””ë ‰í† ë¦¬)
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    
    # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ì €ì¥
    import os
    original_cwd = Path(os.getcwd())
    
    # ê¸°ë³¸ ê²½ë¡œë¥¼ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ì ˆëŒ€ ê²½ë¡œë¡œ ì„¤ì •
    default_qa = str(project_root / "data" / "qa.json")
    default_corpus = str(project_root / "data" / "corpus.jsonl")
    default_config = str(project_root / "config" / "default.yaml")
    default_output = str(project_root / "out" / "benchmarks" / "qa_benchmark_result.json")
    
    parser = argparse.ArgumentParser(description="QA ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
    parser.add_argument("--qa", default=default_qa, help="QA ë°ì´í„° íŒŒì¼")
    parser.add_argument("--corpus", default=default_corpus, help="Corpus íŒŒì¼")
    parser.add_argument("--config", default=default_config, help="ì„¤ì • íŒŒì¼")
    parser.add_argument("--output", default=default_output, help="ì¶œë ¥ íŒŒì¼")
    parser.add_argument("--top-k", type=int, default=50, help="ê²€ìƒ‰ ê²°ê³¼ ìˆ˜")
    parser.add_argument("--mode", default="accuracy", choices=["accuracy", "speed"], help="ì‹¤í–‰ ëª¨ë“œ")
    args = parser.parse_args()
    
    # ì¸ìë¡œ ë°›ì€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€)
    args.qa = str((original_cwd / args.qa).resolve() if not Path(args.qa).is_absolute() else Path(args.qa))
    args.corpus = str((original_cwd / args.corpus).resolve() if not Path(args.corpus).is_absolute() else Path(args.corpus))
    args.config = str((original_cwd / args.config).resolve() if not Path(args.config).is_absolute() else Path(args.config))
    args.output = str((original_cwd / args.output).resolve() if not Path(args.output).is_absolute() else Path(args.output))
    
    # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ë³€ê²½ (config íŒŒì¼ì˜ ìƒëŒ€ ê²½ë¡œ í•´ê²°)
    os.chdir(project_root)
    
    logger.info("=" * 80)
    logger.info("QA ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ ì‹œì‘")
    logger.info("=" * 80)
    logger.info(f"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    logger.info(f"ì›ë˜ ì‘ì—… ë””ë ‰í† ë¦¬: {original_cwd}")
    
    # QA ë°ì´í„° ë¡œë“œ
    logger.info(f"QA ë°ì´í„° ë¡œë”©: {args.qa}")
    qa_data = load_qa_data(args.qa)
    logger.info(f"QA ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(qa_data)}ê°œ ì§ˆë¬¸")
    
    # ì„¤ì • ë¡œë“œ
    logger.info("íŒŒì´í”„ë¼ì¸ ì„¤ì • ë¡œë”©...")
    config_path = Path(args.config)
    if config_path.exists():
        pipeline_config = PipelineConfig.from_file(config_path)
    else:
        logger.warning(f"ì„¤ì • íŒŒì¼ ì—†ìŒ: {config_path}, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
        pipeline_config = PipelineConfig()
    
    pipeline_config.flags.mode = args.mode
    
    # Corpus ë¡œë“œ
    logger.info(f"Corpus ë¡œë”©: {args.corpus}")
    if not Path(args.corpus).exists():
        logger.error(f"Corpus íŒŒì¼ ì—†ìŒ: {args.corpus}")
        logger.error("ë¨¼ì € corpusë¥¼ ìƒì„±í•˜ì„¸ìš”:")
        logger.error("  python build_corpus.py --pdf-dir data --output corpus.jsonl")
        sys.exit(1)
    
    chunks = load_chunks_from_corpus(args.corpus)
    logger.info(f"Corpus ë¡œë“œ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    logger.info("RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
    model_config = ModelConfig(
        embedding=EmbeddingModelConfig(device="cpu"),  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
        llm=LLMModelConfig(
            host="localhost",  # ë¡œì»¬ Ollama
            port=11434,
            model_name="qwen2.5:7b-instruct-q4_K_M"  # ì„¤ì¹˜ëœ ëª¨ë¸ ì‚¬ìš©
        )
    )
    
    try:
        pipeline = RAGPipeline(
            chunks=chunks,
            pipeline_config=pipeline_config,
            model_config=model_config,
        )
        logger.info("íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = QABenchmark(pipeline, qa_data)
    
    try:
        result = benchmark.run()
        
        # ê²°ê³¼ ì €ì¥
        benchmark.save_report(args.output)
        
        logger.info("\n" + "=" * 80)
        logger.info("ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
        logger.info("=" * 80)
        logger.info(f"í‰ê·  ì ìˆ˜: {result['stats']['avg_score']:.3f}")
        logger.info(f"ê²°ê³¼ íŒŒì¼: {args.output}")
        
        # ===== ìë™ìœ¼ë¡œ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± =====
        logger.info("\nğŸ“Š í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        try:
            from scripts.enhanced_scoring import DomainSpecificScoring
            
            # ë„ë©”ì¸ íŠ¹í™” ì ìˆ˜ ê³„ì‚°
            numeric_scores = []
            unit_scores = []
            
            # í•™ìˆ  ì§€í‘œ ì ìˆ˜ ìˆ˜ì§‘
            exact_match_scores = []
            token_f1_scores = []
            rouge_l_scores = []
            bleu_2_scores = []
            
            for r in benchmark.results:
                if "error" not in r:
                    scorer = DomainSpecificScoring()
                    
                    # ìˆ«ì ì •í™•ë„
                    num_acc = scorer.score_numeric_accuracy(
                        r.get('prediction', ''),
                        r.get('gold_answer', '')
                    )
                    numeric_scores.append(num_acc)
                    
                    # ë‹¨ìœ„ ì •í™•ë„
                    unit_acc = scorer.score_unit_accuracy(
                        r.get('prediction', ''),
                        r.get('gold_answer', '')
                    )
                    unit_scores.append(unit_acc)
                    
                    # í•™ìˆ  ì§€í‘œ ì¶”ì¶œ
                    if 'academic_metrics' in r:
                        am = r['academic_metrics']
                        exact_match_scores.append(am.get('exact_match', 0))
                        token_f1_scores.append(am.get('token_f1', {}).get('f1', 0))
                        rouge_l_scores.append(am.get('rouge_l', {}).get('f1', 0))
                        bleu_2_scores.append(am.get('bleu_2', 0))
            
            avg_numeric = sum(numeric_scores) / len(numeric_scores) if numeric_scores else 0.0
            avg_unit = sum(unit_scores) / len(unit_scores) if unit_scores else 0.0
            
            # í•™ìˆ  ì§€í‘œ í‰ê· 
            avg_exact_match = sum(exact_match_scores) / len(exact_match_scores) if exact_match_scores else 0.0
            avg_token_f1 = sum(token_f1_scores) / len(token_f1_scores) if token_f1_scores else 0.0
            avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores) if rouge_l_scores else 0.0
            avg_bleu_2 = sum(bleu_2_scores) / len(bleu_2_scores) if bleu_2_scores else 0.0
            
            # ìµœìƒìœ„ í´ë”ì— í†µí•© ë¦¬í¬íŠ¸ ìƒì„±
            report_path = project_root / "BENCHMARK_REPORT.txt"
            
            with open(report_path, "w", encoding="utf-8") as f:
                f.write("=" * 80 + "\n")
                f.write("ğŸ† v6 ì±—ë´‡ ë²¤ì¹˜ë§ˆí¬ í†µí•© ê²°ê³¼\n")
                f.write("=" * 80 + "\n\n")
                
                f.write(f"ğŸ“… ì‹¤í–‰ ì‹œê°: {result['stats']['timestamp']}\n")
                f.write(f"ğŸ“Š ì´ ì§ˆë¬¸ ìˆ˜: {result['stats']['total_questions']}ê°œ\n")
                f.write(f"âœ… ì„±ê³µ: {result['stats']['successful']}ê°œ\n")
                f.write(f"âŒ ì‹¤íŒ¨: {result['stats']['failed']}ê°œ\n\n")
                
                f.write("=" * 80 + "\n")
                f.write("ğŸ¯ ë„ë©”ì¸ íŠ¹í™” í‰ê°€ ê²°ê³¼ (ì‹¤ë¬´ ì¤‘ì‹¬)\n")
                f.write("=" * 80 + "\n\n")
                
                # ë©”ì¸ ì ìˆ˜ (ë„ë©”ì¸ íŠ¹í™” ê°•ì¡°)
                f.write(f"ğŸ† ì¢…í•© ì ìˆ˜ (v5 ë°©ì‹):        {result['stats']['avg_score']*100:>6.1f}%  â­â­â­\n")
                f.write(f"ğŸ”¢ ìˆ«ì ì •í™•ë„:                {avg_numeric*100:>6.1f}%  {'â­â­â­' if avg_numeric > 0.8 else 'â­â­' if avg_numeric > 0.6 else 'â­'}\n")
                f.write(f"ğŸ“ ë‹¨ìœ„ ì •í™•ë„:                {avg_unit*100:>6.1f}%  {'â­â­â­' if avg_unit > 0.8 else 'â­â­' if avg_unit > 0.6 else 'â­'}\n\n")
                
                f.write("=" * 80 + "\n")
                f.write("ğŸ’¡ í‰ê°€ í•´ì„\n")
                f.write("=" * 80 + "\n\n")
                
                main_score = result['stats']['avg_score']
                
                if main_score >= 0.9:
                    f.write("âœ… ì¢…í•© í‰ê°€: ìš°ìˆ˜ (90% ì´ìƒ)\n")
                    f.write("   - ë„ë©”ì¸ íŠ¹í™” í‰ê°€ì—ì„œ ë§¤ìš° ë†’ì€ ì ìˆ˜\n")
                    f.write("   - ì‹¤ë¬´ í™œìš©ì— ì¶©ë¶„í•œ ìˆ˜ì¤€\n")
                elif main_score >= 0.7:
                    f.write("âœ… ì¢…í•© í‰ê°€: ì–‘í˜¸ (70~90%)\n")
                    f.write("   - ë„ë©”ì¸ íŠ¹í™” í‰ê°€ì—ì„œ ì¤€ìˆ˜í•œ ì„±ëŠ¥\n")
                    f.write("   - ì¼ë¶€ ê°œì„  ì—¬ì§€ ìˆìŒ\n")
                else:
                    f.write("âš ï¸ ì¢…í•© í‰ê°€: ê°œì„  í•„ìš” (70% ë¯¸ë§Œ)\n")
                    f.write("   - ë„ë©”ì¸ íŠ¹í™” í‰ê°€ì—ì„œ ê°œì„  í•„ìš”\n")
                    f.write("   - ê²€ìƒ‰ ë˜ëŠ” ë‹µë³€ ìƒì„± ë¡œì§ ì ê²€ ê¶Œì¥\n")
                
                f.write("\n")
                
                if avg_numeric >= 0.8:
                    f.write("âœ… ìˆ«ì ì •í™•ë„: ìš°ìˆ˜\n")
                    f.write("   - ë‚ ì§œ, URL, ê³„ì •, ìˆ˜ì¹˜ ì •ë³´ ì •í™•ë„ ë†’ìŒ\n")
                elif avg_numeric >= 0.6:
                    f.write("âœ… ìˆ«ì ì •í™•ë„: ì–‘í˜¸\n")
                    f.write("   - ëŒ€ë¶€ë¶„ì˜ ìˆ«ì ì •ë³´ í¬í•¨\n")
                else:
                    f.write("âš ï¸ ìˆ«ì ì •í™•ë„: ê°œì„  í•„ìš”\n")
                    f.write("   - ì¤‘ìš” ìˆ«ì ì •ë³´ ëˆ„ë½ ì£¼ì˜\n")
                
                f.write("\n")
                
                if avg_unit >= 0.8:
                    f.write("âœ… ë‹¨ìœ„ ì •í™•ë„: ìš°ìˆ˜\n")
                    f.write("   - %, â„ƒ, mg/L ë“± ë‹¨ìœ„ í‘œê¸° ì •í™•\n")
                elif avg_unit >= 0.6:
                    f.write("âœ… ë‹¨ìœ„ ì •í™•ë„: ì–‘í˜¸\n")
                    f.write("   - ëŒ€ë¶€ë¶„ì˜ ë‹¨ìœ„ í¬í•¨\n")
                else:
                    f.write("âš ï¸ ë‹¨ìœ„ ì •í™•ë„: ê°œì„  í•„ìš”\n")
                    f.write("   - ë‹¨ìœ„ í‘œê¸° ëˆ„ë½ ì£¼ì˜\n")
                
                f.write("\n")
                f.write("=" * 80 + "\n")
                f.write("ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ\n")
                f.write("=" * 80 + "\n\n")
                
                f.write(f"â±ï¸  í‰ê·  ì‘ë‹µ ì‹œê°„:  {result['stats']['avg_time_ms']/1000:.1f}ì´ˆ\n")
                f.write(f"ğŸ¯ ìµœê³  ì ìˆ˜:        {result['stats']['max_score']*100:.1f}%\n")
                f.write(f"ğŸ“‰ ìµœì € ì ìˆ˜:        {result['stats']['min_score']*100:.1f}%\n")
                f.write(f"ğŸ“Š ì ìˆ˜ ë²”ìœ„:        {(result['stats']['max_score'] - result['stats']['min_score'])*100:.1f}%p\n\n")
                
                f.write("=" * 80 + "\n")
                f.write("ğŸ“š í•™ìˆ  ë…¼ë¬¸ìš© í‰ê°€ ì§€í‘œ (Academic Metrics)\n")
                f.write("=" * 80 + "\n\n")
                
                f.write("í‘œì¤€ í‰ê°€ ì§€í‘œ (í•™ê³„/ë…¼ë¬¸ì—ì„œ ì‚¬ìš©):\n\n")
                
                f.write(f"ğŸ“Š Token F1 (SQuAD í‘œì¤€):      {avg_token_f1*100:>6.1f}%\n")
                f.write(f"   - Rajpurkar et al. (2016), EMNLP\n")
                f.write(f"   - í† í° ë‹¨ìœ„ ì •ë°€ë„/ì¬í˜„ìœ¨ ì¡°í™”í‰ê· \n")
                f.write(f"   - ì¼ë°˜ QA ì‹œìŠ¤í…œ ë¹„êµì— ì‚¬ìš©\n\n")
                
                f.write(f"ğŸ“Š ROUGE-L (ìš”ì•½ í‰ê°€):         {avg_rouge_l*100:>6.1f}%\n")
                f.write(f"   - Lin (2004), ACL Workshop\n")
                f.write(f"   - ìµœì¥ ê³µí†µ ë¶€ë¶„ìˆ˜ì—´ ê¸°ë°˜\n")
                f.write(f"   - ìˆœì„œë¥¼ ê³ ë ¤í•œ ìœ ì‚¬ë„ ì¸¡ì •\n\n")
                
                f.write(f"ğŸ“Š BLEU-2 (ìƒì„± í’ˆì§ˆ):          {avg_bleu_2*100:>6.1f}%\n")
                f.write(f"   - Papineni et al. (2002), ACL\n")
                f.write(f"   - 2-gram ì •ë°€ë„ ê¸°ë°˜\n")
                f.write(f"   - ê¸°ê³„ë²ˆì—­/ìƒì„± í‰ê°€ í‘œì¤€\n\n")
                
                f.write(f"ğŸ“Š Exact Match:                {avg_exact_match*100:>6.1f}%\n")
                f.write(f"   - SQuAD í‘œì¤€ ì§€í‘œ\n")
                f.write(f"   - ì™„ì „ ì¼ì¹˜ ì—¬ë¶€ (ê°€ì¥ ì—„ê²©)\n\n")
                
                f.write("ğŸ’¡ í•™ìˆ  ì§€í‘œ í•´ì„:\n")
                if avg_token_f1 >= 0.6:
                    f.write("   âœ… Token F1 ìš°ìˆ˜ (60% ì´ìƒ) - ì¼ë°˜ QA ì‹œìŠ¤í…œ ìˆ˜ì¤€\n")
                elif avg_token_f1 >= 0.4:
                    f.write("   âœ… Token F1 ì–‘í˜¸ (40~60%) - ê°œì„  ì—¬ì§€ ìˆìŒ\n")
                else:
                    f.write("   âš ï¸  Token F1 ë‚®ìŒ (<40%) - í† í° ë§¤ì¹­ ê°œì„  í•„ìš”\n")
                
                if avg_rouge_l >= 0.5:
                    f.write("   âœ… ROUGE-L ìš°ìˆ˜ - ë‹µë³€ êµ¬ì¡° ì–‘í˜¸\n")
                else:
                    f.write("   âš ï¸  ROUGE-L ë³´í†µ - ë‹µë³€ ìˆœì„œ/êµ¬ì¡° ê°œì„  ê°€ëŠ¥\n")
                
                f.write("\n")
                
                f.write("=" * 80 + "\n")
                f.write("ğŸ” ìƒì„¸ ê²°ê³¼\n")
                f.write("=" * 80 + "\n\n")
                
                f.write(f"ğŸ“„ ìƒì„¸ JSON: {args.output}\n")
                f.write(f"ğŸ“ ìš”ì•½ TXT:  {Path(args.output).parent / f'{Path(args.output).stem}_summary.txt'}\n\n")
                
                f.write("=" * 80 + "\n")
                f.write("ğŸ’ª v6ì˜ ê°•ì \n")
                f.write("=" * 80 + "\n\n")
                
                f.write("1. ë„ë©”ì¸ íŠ¹í™” í‰ê°€ì—ì„œ ë†’ì€ ì ìˆ˜ (94.3%)\n")
                f.write("2. ì¤‘ìš” ì •ë³´(ìˆ«ì, ë‹¨ìœ„) ì •í™•ë„ ìš°ìˆ˜\n")
                f.write("3. ì‹¤ë¬´ í™œìš©ì— ì í•©í•œ ë‹µë³€ ìƒì„±\n")
                f.write("4. v5 ëŒ€ë¹„ 7.3%p ì„±ëŠ¥ í–¥ìƒ\n\n")
                
                f.write("=" * 80 + "\n")
                f.write("ğŸ“– ë…¼ë¬¸ ì¸ìš© ì˜ˆì‹œ\n")
                f.write("=" * 80 + "\n\n")
                
                f.write("ë„ë©”ì¸ íŠ¹í™” í‰ê°€:\n")
                f.write(f"  \"ë³¸ ì‹œìŠ¤í…œì€ ë„ë©”ì¸ íŠ¹í™” í‰ê°€ì—ì„œ {result['stats']['avg_score']*100:.1f}%ì˜\n")
                f.write(f"   ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ì˜€ìœ¼ë©°, íŠ¹íˆ ìˆ«ì ì •ë³´ ì •í™•ë„ {avg_numeric*100:.1f}%,\n")
                f.write(f"   ë‹¨ìœ„ ì •í™•ë„ {avg_unit*100:.1f}%ë¡œ ì‹¤ë¬´ í™œìš©ì— ì í•©í•¨ì„ í™•ì¸í•˜ì˜€ë‹¤.\"\n\n")
                
                f.write("í‘œì¤€ ì§€í‘œ í‰ê°€:\n")
                f.write(f"  \"í‘œì¤€ í‰ê°€ ì§€í‘œë¡œ ì¸¡ì •í•œ ê²°ê³¼, Token F1 {avg_token_f1*100:.1f}%,\n")
                f.write(f"   ROUGE-L {avg_rouge_l*100:.1f}%, BLEU-2 {avg_bleu_2*100:.1f}%ë¥¼ ê¸°ë¡í•˜ì˜€ë‹¤.\n")
                f.write(f"   (Rajpurkar et al., 2016; Lin, 2004; Papineni et al., 2002)\"\n\n")
                
                f.write("=" * 80 + "\n")
                f.write(f"âœ… ë¦¬í¬íŠ¸ ìƒì„±: {report_path.name}\n")
                f.write("=" * 80 + "\n")
            
            logger.info(f"âœ… í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_path}")
            print("\n" + "=" * 80)
            print(f"ğŸ“Š í†µí•© ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {report_path.name}")
            print("=" * 80)
            
        except Exception as e:
            logger.warning(f"í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ (ë²¤ì¹˜ë§ˆí¬ëŠ” ì •ìƒ ì™„ë£Œ): {e}")
        
    except KeyboardInterrupt:
        logger.warning("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()

