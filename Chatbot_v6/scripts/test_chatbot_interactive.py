#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ëŒ€í™”í˜• ì±—ë´‡ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

evaluate_qa_unifiedì²˜ëŸ¼ ë¡œì»¬ì—ì„œ ì§ì ‘ RAG íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆëŠ” ëŒ€í™”í˜• ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
"""

from __future__ import annotations

import sys
import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from modules.core.types import Chunk
from modules.pipeline.rag_pipeline import RAGPipeline
from config.pipeline_config import PipelineConfig
from config.model_config import ModelConfig, EmbeddingModelConfig, LLMModelConfig
from modules.core.logger import setup_logging, get_logger
from scripts.unified_evaluation import UnifiedEvaluator

setup_logging(log_dir="logs", log_level="INFO", log_format="simple")
logger = get_logger(__name__)


def load_chunks_from_corpus(corpus_path: str) -> List[Chunk]:
    """JSONL corpus íŒŒì¼ì—ì„œ ì²­í¬ ë¡œë“œ"""
    chunks = []
    with open(corpus_path, "r", encoding="utf-8") as f:
        for line in f:
            data = json.loads(line.strip())
            
            neighbor_hint = data.get("neighbor_hint")
            if neighbor_hint and isinstance(neighbor_hint, list):
                neighbor_hint = tuple(neighbor_hint)
            
            chunk = Chunk(
                doc_id=data["doc_id"],
                filename=data["filename"],
                page=data.get("page"),
                start_offset=data.get("start_offset", 0),
                length=data.get("length", len(data["text"])),
                text=data["text"],
                neighbor_hint=neighbor_hint,
                extra=data.get("extra", {}),
            )
            chunks.append(chunk)
    
    return chunks


def print_answer(answer, processing_time: float):
    """ë‹µë³€ì„ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
    print("\n" + "=" * 80)
    print("ğŸ¤– ì±—ë´‡ ë‹µë³€:")
    print("=" * 80)
    print(f"\n{answer.text}\n")
    
    # ë©”íƒ€ë°ì´í„° ì¶œë ¥
    print("-" * 80)
    print(f"ì‹ ë¢°ë„: {answer.confidence:.3f}")
    print(f"ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
    print(f"ì¶œì²˜ ìˆ˜: {len(answer.sources)}")
    
    # ìƒìœ„ 3ê°œ ì¶œì²˜ ì¶œë ¥
    if answer.sources:
        print("\nğŸ“š ì£¼ìš” ì¶œì²˜:")
        for i, source in enumerate(answer.sources[:3], 1):
            print(f"\n[{i}] {source.chunk.filename} (í˜ì´ì§€: {source.chunk.page})")
            print(f"    ì ìˆ˜: {source.score:.4f}")
            print(f"    ë‚´ìš©: {source.chunk.text[:150]}...")
    
    print("=" * 80 + "\n")


def evaluate_answer(evaluator: UnifiedEvaluator, question: str, answer_text: str, 
                   gold_answer: str = None, keywords: List[str] = None):
    """ë‹µë³€ í‰ê°€ (ì„ íƒì )"""
    if not gold_answer:
        print("\nğŸ’¡ í‰ê°€ë¥¼ ìœ„í•´ ì •ë‹µ(gold_answer)ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    try:
        contexts = [src.chunk.text for src in getattr(answer, 'sources', [])]
        results = evaluator.evaluate_all(
            question=question,
            prediction=answer_text,
            ground_truth=gold_answer,
            contexts=contexts,
            keywords=keywords or []
        )
        
        print("\n" + "=" * 80)
        print("ğŸ“Š í‰ê°€ ê²°ê³¼:")
        print("=" * 80)
        
        summary = results['summary']
        print(f"\nê¸°ë³¸ Score (v5): {summary.get('basic_v5_score', 0)*100:.1f}%")
        
        if 'faithfulness' in summary:
            print(f"Faithfulness: {summary['faithfulness']*100:.1f}%")
            print(f"Answer Correctness: {summary['answer_correctness']*100:.1f}%")
        
        if 'token_f1' in summary:
            print(f"Token F1: {summary['token_f1']*100:.1f}%")
            print(f"ROUGE-L: {summary['rouge_l']*100:.1f}%")
        
        print("=" * 80 + "\n")
        
    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\n")


def interactive_chat(pipeline: RAGPipeline, evaluator: Optional[UnifiedEvaluator] = None):
    """ëŒ€í™”í˜• ì±—ë´‡"""
    print("\n" + "=" * 80)
    print("ğŸ¤– ëŒ€í™”í˜• ì±—ë´‡ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    print("=" * 80)
    print("\nëª…ë ¹ì–´:")
    print("  - 'exit' ë˜ëŠ” 'quit': ì¢…ë£Œ")
    print("  - 'clear': í™”ë©´ ì§€ìš°ê¸°")
    print("  - 'eval <ì§ˆë¬¸> | <ì •ë‹µ> | <í‚¤ì›Œë“œ1,í‚¤ì›Œë“œ2>': ë‹µë³€ í‰ê°€ (ì„ íƒì )")
    print("  - ê·¸ ì™¸: ì¼ë°˜ ì§ˆë¬¸\n")
    print("=" * 80 + "\n")
    
    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥
            question = input("ì§ˆë¬¸: ").strip()
            
            if not question:
                continue
            
            # ëª…ë ¹ì–´ ì²˜ë¦¬
            if question.lower() in ['exit', 'quit', 'q']:
                print("\nğŸ‘‹ í…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")
                break
            
            if question.lower() == 'clear':
                import os
                os.system('cls' if os.name == 'nt' else 'clear')
                continue
            
            # í‰ê°€ ëª¨ë“œ (ì˜ˆ: eval ì§ˆë¬¸ | ì •ë‹µ | í‚¤ì›Œë“œ1,í‚¤ì›Œë“œ2)
            if question.startswith('eval '):
                parts = question[5:].split('|')
                if len(parts) >= 2:
                    question = parts[0].strip()
                    gold_answer = parts[1].strip()
                    keywords = parts[2].strip().split(',') if len(parts) > 2 else []
                else:
                    print("âŒ ì˜ëª»ëœ í‰ê°€ ëª…ë ¹ì–´ í˜•ì‹ì…ë‹ˆë‹¤.")
                    print("   í˜•ì‹: eval ì§ˆë¬¸ | ì •ë‹µ | í‚¤ì›Œë“œ1,í‚¤ì›Œë“œ2")
                    continue
            
            # ì§ˆë¬¸ ì²˜ë¦¬
            start_time = time.time()
            answer = pipeline.ask(question, top_k=50)
            processing_time = time.time() - start_time
            
            # ë‹µë³€ ì¶œë ¥
            print_answer(answer, processing_time)
            
            # í‰ê°€ ì‹¤í–‰ (í‰ê°€ ëª¨ë“œì¸ ê²½ìš°)
            if question.startswith('eval ') or 'gold_answer' in locals():
                if evaluator and 'gold_answer' in locals():
                    evaluate_answer(evaluator, question, answer.text, gold_answer, keywords)
                    del gold_answer
                elif not evaluator:
                    print("âš ï¸ í‰ê°€ ëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ í…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}\n")
            logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)


def batch_test(pipeline: RAGPipeline, questions: List[str]):
    """ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ëª¨ë“œ"""
    print("\n" + "=" * 80)
    print("ğŸ“‹ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    print("=" * 80)
    print(f"ì´ {len(questions)}ê°œ ì§ˆë¬¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n")
    
    results = []
    
    for i, question in enumerate(questions, 1):
        print(f"\n[{i}/{len(questions)}] ì§ˆë¬¸: {question}")
        
        try:
            start_time = time.time()
            answer = pipeline.ask(question, top_k=50)
            processing_time = time.time() - start_time
            
            print(f"ë‹µë³€: {answer.text[:100]}...")
            print(f"ì‹ ë¢°ë„: {answer.confidence:.3f}, ì‹œê°„: {processing_time:.2f}ì´ˆ")
            
            results.append({
                'question': question,
                'answer': answer.text,
                'confidence': answer.confidence,
                'time': processing_time,
                'num_sources': len(answer.sources)
            })
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            results.append({
                'question': question,
                'error': str(e)
            })
    
    print("\n" + "=" * 80)
    print("âœ… ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 80)
    
    # í†µê³„ ì¶œë ¥
    successful = [r for r in results if 'error' not in r]
    if successful:
        avg_confidence = sum(r['confidence'] for r in successful) / len(successful)
        avg_time = sum(r['time'] for r in successful) / len(successful)
        print(f"\nì„±ê³µ: {len(successful)}/{len(questions)}")
        print(f"í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
        print(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ì´ˆ\n")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="ëŒ€í™”í˜• ì±—ë´‡ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ëŒ€í™”í˜• ëª¨ë“œ
  python scripts/test_chatbot_interactive.py
  
  # ë°°ì¹˜ ëª¨ë“œ (qa.json íŒŒì¼ ì‚¬ìš©)
  python scripts/test_chatbot_interactive.py --qa data/qa.json --batch
  
  # ë‹¤ë¥¸ corpus íŒŒì¼ ì‚¬ìš©
  python scripts/test_chatbot_interactive.py --corpus data/my_corpus.jsonl
  
  # í‰ê°€ ëª¨ë“ˆ ì‚¬ìš© ì•ˆ í•¨
  python scripts/test_chatbot_interactive.py --no-eval
        """
    )
    
    parser.add_argument("--corpus", default="data/corpus.jsonl", help="Corpus íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--config", default="config/default.yaml", help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--qa", default=None, help="QA íŒŒì¼ ê²½ë¡œ (ë°°ì¹˜ ëª¨ë“œìš©)")
    parser.add_argument("--batch", action="store_true", help="ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    parser.add_argument("--top-k", type=int, default=50, help="ê²€ìƒ‰ ê²°ê³¼ ìˆ˜")
    parser.add_argument("--model", default="qwen2.5:3b-instruct-q4_K_M", help="LLM ëª¨ë¸ëª…")
    parser.add_argument("--no-eval", action="store_true", help="í‰ê°€ ëª¨ë“ˆ ì‚¬ìš© ì•ˆ í•¨")
    parser.add_argument("--mode", default="accuracy", choices=["accuracy", "speed"], help="ì‹¤í–‰ ëª¨ë“œ")
    
    args = parser.parse_args()
    
    # ê²½ë¡œ ë³€í™˜
    project_root = Path(__file__).parent.parent
    corpus_path = project_root / args.corpus
    config_path = project_root / args.config
    
    logger.info("=" * 80)
    logger.info("ëŒ€í™”í˜• ì±—ë´‡ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 80)
    
    # Corpus ë¡œë“œ
    logger.info(f"Corpus ë¡œë”©: {corpus_path}")
    if not corpus_path.exists():
        logger.error(f"Corpus íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {corpus_path}")
        sys.exit(1)
    
    chunks = load_chunks_from_corpus(str(corpus_path))
    logger.info(f"Corpus ë¡œë“œ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
    
    # ì„¤ì • ë¡œë“œ
    pipeline_config = PipelineConfig()
    if config_path.exists():
        logger.info(f"ì„¤ì • ë¡œë“œ: {config_path}")
        pipeline_config = PipelineConfig.from_file(config_path)
    
    pipeline_config.flags.mode = args.mode
    
    # LLM ëª¨ë¸ í™•ì¸
    logger.info(f"LLM ëª¨ë¸ í™•ì¸: {args.model}")
    try:
        from modules.generation.ollama_manager import ollama_manager
        if not ollama_manager.ensure_model_available(args.model):
            logger.error(f"LLM ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.model}")
            logger.info(f"ìˆ˜ë™ ì„¤ì¹˜: ollama pull {args.model}")
            sys.exit(1)
    except Exception as e:
        logger.warning(f"ëª¨ë¸ ìë™ ì„¤ì¹˜ í™•ì¸ ì‹¤íŒ¨: {e}")
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    logger.info("RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
    model_config = ModelConfig(
        embedding=EmbeddingModelConfig(device="cuda"),
        llm=LLMModelConfig(
            host="localhost",
            port=11434,
            model_name=args.model
        )
    )
    
    try:
        pipeline = RAGPipeline(
            chunks=chunks,
            pipeline_config=pipeline_config,
            model_config=model_config,
            evaluation_mode=False,
        )
        logger.info("íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)
    
    # í‰ê°€ ëª¨ë“ˆ ì´ˆê¸°í™” (ì„ íƒì )
    evaluator = None
    if not args.no_eval:
        try:
            evaluator = UnifiedEvaluator()
            logger.info("í‰ê°€ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"í‰ê°€ ëª¨ë“ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.info("í‰ê°€ ê¸°ëŠ¥ ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
    
    # ë°°ì¹˜ ëª¨ë“œ ë˜ëŠ” ëŒ€í™”í˜• ëª¨ë“œ
    if args.batch:
        if args.qa:
            qa_path = project_root / args.qa
            if not qa_path.exists():
                logger.error(f"QA íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {qa_path}")
                sys.exit(1)
            
            with open(qa_path, 'r', encoding='utf-8') as f:
                qa_data = json.load(f)
            
            questions = [item['question'] for item in qa_data]
            batch_test(pipeline, questions)
        else:
            logger.error("ë°°ì¹˜ ëª¨ë“œì—ì„œëŠ” --qa ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            sys.exit(1)
    else:
        interactive_chat(pipeline, evaluator)


if __name__ == "__main__":
    main()

