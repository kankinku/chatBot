# í‰ê°€ ì‹œìŠ¤í…œ ì™„ë²½ ê°€ì´ë“œ

RAG ì±—ë´‡ì˜ ëª¨ë“  í‰ê°€ ì§€í‘œë¥¼ ì´í•´í•˜ê³  í™œìš©í•˜ëŠ” ë°©ë²•

---

## ðŸ“‹ ëª©ì°¨

1. [í‰ê°€ ì‹œìŠ¤í…œ ê°œìš”](#í‰ê°€-ì‹œìŠ¤í…œ-ê°œìš”)
2. [4ê°€ì§€ í‰ê°€ ì²´ê³„](#4ê°€ì§€-í‰ê°€-ì²´ê³„)
3. [í†µí•© í‰ê°€ ëª¨ë“ˆ ì‚¬ìš©ë²•](#í†µí•©-í‰ê°€-ëª¨ë“ˆ-ì‚¬ìš©ë²•)
4. [ê° ì§€í‘œì˜ ì˜ë¯¸ì™€ í™œìš©](#ê°-ì§€í‘œì˜-ì˜ë¯¸ì™€-í™œìš©)
5. [ì‹¤ì „ ì˜ˆì‹œ](#ì‹¤ì „-ì˜ˆì‹œ)

---

## í‰ê°€ ì‹œìŠ¤í…œ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” **4ê°€ì§€ ë…ë¦½ì ì¸ í‰ê°€ ì²´ê³„**ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

| í‰ê°€ ì²´ê³„ | ëª©ì  | ì£¼ìš” ì§€í‘œ | í™œìš© ë¶„ì•¼ |
|---------|------|----------|----------|
| **1. ê¸°ë³¸ Score (v5)** | ë„ë©”ì¸ ì‹¤ë¬´ í‰ê°€ | ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì¢…í•© ì ìˆ˜ | ì‹¤ë¬´ ì„±ëŠ¥ ì¸¡ì • |
| **2. ë„ë©”ì¸ íŠ¹í™”** | ì •ìˆ˜ìž¥ íŠ¹í™” í‰ê°€ | ìˆ«ìž/ë‹¨ìœ„ ì •í™•ë„ | ê¸°ìˆ  ì •ë³´ ì •í™•ë„ |
| **3. RAG í•µì‹¬ 3ëŒ€** | RAG ì‹œìŠ¤í…œ í‰ê°€ | Faithfulness, Correctness, Precision | ë…¼ë¬¸/í•™ìˆ  ì—°êµ¬ |
| **4. í•™ìˆ  í‘œì¤€** | ë²”ìš© NLP í‰ê°€ | F1, ROUGE, BLEU, EM | íƒ€ ì‹œìŠ¤í…œ ë¹„êµ |

---

## 4ê°€ì§€ í‰ê°€ ì²´ê³„

### 1ï¸âƒ£ ê¸°ë³¸ Score (v5 ë°©ì‹) - ë„ë©”ì¸ ê°€ì¤‘ì¹˜ í‰ê°€

**íŒŒì¼**: `scripts/run_qa_benchmark.py` (46-127ì¤„)

**íŠ¹ì§•**: 
- ìˆ«ìž, ë‹¨ìœ„, í‚¤ì›Œë“œì— ì°¨ë“± ê°€ì¤‘ì¹˜ ì ìš©
- ì‹¤ë¬´ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ê°•ì¡°
- v5ì—ì„œ ê²€ì¦ëœ í‰ê°€ ë°©ì‹

**ê°€ì¤‘ì¹˜**:
- ìˆ«ìž: **1.5** (ê°€ìž¥ ì¤‘ìš”)
- ë‹¨ìœ„: **1.3**
- í‚¤ì›Œë“œ: **1.0**

**ì˜ˆì‹œ**:
```python
score = benchmark.score_answer(
    prediction="ìˆ˜ì§ˆ ê¸°ì¤€ì€ 25â„ƒ ìž…ë‹ˆë‹¤.",
    gold_answer="ìˆ˜ì§ˆ ê¸°ì¤€ì€ 25â„ƒìž…ë‹ˆë‹¤.",
    keywords=["ìˆ˜ì§ˆ", "ê¸°ì¤€", "25", "â„ƒ"]
)
# score: 1.0 (ì™„ë²½ ì¼ì¹˜)
```

**ì–¸ì œ ì‚¬ìš©**:
- âœ… ì‹¤ë¬´ ì„±ëŠ¥ ì¸¡ì •í•  ë•Œ
- âœ… v5ì™€ ì„±ëŠ¥ ë¹„êµí•  ë•Œ
- âœ… ë„ë©”ì¸ íŠ¹í™” ì„±ëŠ¥ì„ ê°•ì¡°í•  ë•Œ

---

### 2ï¸âƒ£ ë„ë©”ì¸ íŠ¹í™” í‰ê°€ - ì •ìˆ˜ìž¥ íŠ¹í™”

**íŒŒì¼**: `scripts/enhanced_scoring.py`

**íŠ¹ì§•**:
- ìˆ«ìž/ë‹¨ìœ„ ì •í™•ë„ ë³„ë„ ì¸¡ì •
- ì„¸ë¶€ ë¶„ì„ ì •ë³´ ì œê³µ
- ë„ë©”ì¸ ë™ì˜ì–´ ì²˜ë¦¬ (mg/L â†” ppm)

**ì£¼ìš” ë©”ì„œë“œ**:
```python
from scripts.enhanced_scoring import DomainSpecificScoring

scorer = DomainSpecificScoring()

# 1. ì¢…í•© í‰ê°€ (v5 ìŠ¤íƒ€ì¼)
result = scorer.score_answer_v5_style(pred, gold, keywords)
# {
#     'total_score': 0.95,
#     'numeric_score': 1.0,
#     'unit_score': 1.0,
#     'keyword_score': 0.85,
#     'details': {...}
# }

# 2. ìˆ«ìžë§Œ í‰ê°€
numeric_acc = scorer.score_numeric_accuracy(pred, gold)
# 0.0 ~ 1.0

# 3. ë‹¨ìœ„ë§Œ í‰ê°€
unit_acc = scorer.score_unit_accuracy(pred, gold)
# 0.0 ~ 1.0
```

**ì–¸ì œ ì‚¬ìš©**:
- âœ… ìˆ«ìž ì •í™•ë„ë¥¼ ìƒì„¸ížˆ ë¶„ì„í•  ë•Œ
- âœ… ë‹¨ìœ„ ë³€í™˜ ì •í™•ë„ë¥¼ í™•ì¸í•  ë•Œ
- âœ… ë„ë©”ì¸ íŠ¹í™” ì„±ëŠ¥ì„ ê°•ì¡°í•  ë•Œ

---

### 3ï¸âƒ£ RAG í•µì‹¬ 3ëŒ€ ì§€í‘œ - í•™ìˆ  ì—°êµ¬ìš©

**íŒŒì¼**: `scripts/rag_core_metrics.py`

**íŠ¹ì§•**:
- RAGAs Framework ê¸°ë°˜ (Es et al., 2023)
- ë…¼ë¬¸ ì¸ìš© ê°€ëŠ¥í•œ í‘œì¤€ ì§€í‘œ
- RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ ì„±ëŠ¥ ì¸¡ì •

**3ëŒ€ ì§€í‘œ**:

#### ðŸ“Œ Faithfulness (ì¶©ì‹¤ì„±) - í™˜ê° ë°©ì§€
```python
from scripts.rag_core_metrics import RAGCoreMetrics

faith = RAGCoreMetrics.faithfulness(answer, contexts)
# {
#     'score': 0.85,
#     'supported_claims': 17,
#     'total_claims': 20,
#     'support_ratio': 0.85
# }
```

**í‰ê°€ ì§ˆë¬¸**: "ë‹µë³€ì´ ìžë£Œ ë°–ì˜ ê±°ì§“ë§ì„ í–ˆë‚˜?"
- ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (í™˜ê° ì—†ìŒ)
- ë…¼ë¬¸ í•µì‹¬ ì§€í‘œ 1ìˆœìœ„

#### ðŸ“Œ Answer Correctness (ë‹µë³€ ì •í™•ë„) - ì‚¬ì‹¤ì  ì¼ì¹˜
```python
correctness = RAGCoreMetrics.answer_correctness(answer, ground_truth)
# {
#     'score': 0.92,
#     'semantic_similarity': 0.88,
#     'factual_correctness': 0.95
# }
```

**í‰ê°€ ì§ˆë¬¸**: "ë‹µë³€ì´ ì •ë‹µê³¼ ì‚¬ì‹¤ìƒ ë™ì¼í•œê°€?"
- ì˜ë¯¸ ìœ ì‚¬ë„(40%) + ì‚¬ì‹¤ ì •í™•ë„(60%)
- ìˆ«ìž/ë‹¨ìœ„ ì¼ì¹˜ë¥¼ ê°•ì¡°

#### ðŸ“Œ Context Precision (ë¬¸ë§¥ ì •ë°€ë„) - ê²€ìƒ‰ íš¨ìœ¨ì„±
```python
precision = RAGCoreMetrics.context_precision(
    question, contexts, answer, ground_truth
)
# {
#     'score': 0.75,
#     'relevant_contexts': 3,
#     'total_contexts': 4,
#     'precision': 0.75
# }
```

**í‰ê°€ ì§ˆë¬¸**: "ì—‰ëš±í•œ ìžë£Œë¥¼ ê°€ì ¸ì™€ì„œ í—·ê°ˆë¦¬ì§€ ì•Šì•˜ë‚˜?"
- ê²€ìƒ‰ëœ ìžë£Œì˜ íš¨ìœ¨ì„± ì¸¡ì •
- ë¶ˆí•„ìš”í•œ ìžë£Œ ë¹„ìœ¨ í™•ì¸

**ì¢…í•© í‰ê°€**:
```python
results = RAGCoreMetrics.evaluate_all(
    question, answer, ground_truth, contexts
)
# {
#     'faithfulness': {...},
#     'answer_correctness': {...},
#     'context_precision': {...},
#     'overall_score': 0.84  # ê°€ì¤‘ í‰ê· 
# }
```

**ê°€ì¤‘ì¹˜**: Faithfulness(40%) + Correctness(40%) + Precision(20%)

**ì–¸ì œ ì‚¬ìš©**:
- âœ… ë…¼ë¬¸ ìž‘ì„±í•  ë•Œ
- âœ… RAG ì‹œìŠ¤í…œì„ í‰ê°€í•  ë•Œ
- âœ… í™˜ê°(hallucination) ë¬¸ì œë¥¼ ë¶„ì„í•  ë•Œ

**ë…¼ë¬¸ ì¸ìš© ì˜ˆì‹œ**:
```
ë³¸ ì—°êµ¬ì˜ RAG ì±—ë´‡ ì‹œìŠ¤í…œì„ 20ê°œ ì§ˆë¬¸ìœ¼ë¡œ í‰ê°€í•œ ê²°ê³¼,
Faithfulness 85.3%, Answer Correctness 92.1%, Context Precision 75.4%ë¥¼ 
ë‹¬ì„±í•˜ì˜€ë‹¤ (Es et al., 2023).

ì°¸ê³ ë¬¸í—Œ:
Es, S., James, J., Espinosa-Anke, L., & Schockaert, S. (2023).
RAGAS: Automated Evaluation of Retrieval Augmented Generation.
arXiv preprint arXiv:2309.15217.
```

---

### 4ï¸âƒ£ í•™ìˆ  í‘œì¤€ ì§€í‘œ - ë²”ìš© NLP í‰ê°€

**íŒŒì¼**: `scripts/academic_metrics.py`

**íŠ¹ì§•**:
- SQuAD, ROUGE, BLEU ë“± í‘œì¤€ ì§€í‘œ
- ë‹¤ë¥¸ ì‹œìŠ¤í…œê³¼ ë¹„êµ ê°€ëŠ¥
- ë…¼ë¬¸ ì¸ìš© ê°€ëŠ¥

**4ê°€ì§€ ì§€í‘œ**:

#### ðŸ“Œ Exact Match (ì™„ì „ ì¼ì¹˜)
```python
from scripts.academic_metrics import AcademicMetrics

em = AcademicMetrics.exact_match(pred, gold)
# 1.0 (ì¼ì¹˜) or 0.0 (ë¶ˆì¼ì¹˜)
```

**ì¶œì²˜**: Rajpurkar et al. (2016), SQuAD, EMNLP 2016

#### ðŸ“Œ Token F1 Score
```python
f1_result = AcademicMetrics.token_f1_score(pred, gold)
# {
#     'precision': 0.92,
#     'recall': 0.88,
#     'f1': 0.90
# }
```

**ì¶œì²˜**: Rajpurkar et al. (2016), SQuAD, EMNLP 2016
- SQuADì˜ ì£¼ìš” í‰ê°€ ì§€í‘œ

#### ðŸ“Œ ROUGE-L (ìš”ì•½ í‰ê°€)
```python
rouge = AcademicMetrics.rouge_l(pred, gold)
# {
#     'precision': 0.85,
#     'recall': 0.82,
#     'f1': 0.83
# }
```

**ì¶œì²˜**: Lin (2004), ROUGE, ACL Workshop 2004
- ìµœìž¥ ê³µí†µ ë¶€ë¶„ìˆ˜ì—´(LCS) ê¸°ë°˜
- ìˆœì„œë¥¼ ê³ ë ¤í•œ ìœ ì‚¬ë„

#### ðŸ“Œ BLEU (ê¸°ê³„ë²ˆì—­ í‘œì¤€)
```python
bleu1 = AcademicMetrics.bleu_n(pred, gold, n=1)  # BLEU-1
bleu2 = AcademicMetrics.bleu_n(pred, gold, n=2)  # BLEU-2
```

**ì¶œì²˜**: Papineni et al. (2002), BLEU, ACL 2002
- n-gram ì •ë°€ë„ ê¸°ë°˜

**ì¢…í•© í‰ê°€**:
```python
all_metrics = AcademicMetrics.evaluate_all(pred, gold)
# {
#     'exact_match': 0.0,
#     'token_f1': {'f1': 0.90, ...},
#     'rouge_l': {'f1': 0.83, ...},
#     'bleu_1': 0.88,
#     'bleu_2': 0.75
# }
```

**ì–¸ì œ ì‚¬ìš©**:
- âœ… íƒ€ ì‹œìŠ¤í…œê³¼ ì„±ëŠ¥ ë¹„êµí•  ë•Œ
- âœ… êµ­ì œ í•™íšŒ ë…¼ë¬¸ ìž‘ì„±í•  ë•Œ
- âœ… í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ì™€ ë¹„êµí•  ë•Œ

---

## í†µí•© í‰ê°€ ëª¨ë“ˆ ì‚¬ìš©ë²•

**íŒŒì¼**: `scripts/unified_evaluation.py` (ìƒˆë¡œ ìƒì„±)

**íŠ¹ì§•**:
- 4ê°€ì§€ í‰ê°€ ì²´ê³„ë¥¼ í•œ ë²ˆì— ì‹¤í–‰
- ìž¬ì‚¬ìš© ê°€ëŠ¥í•œ í†µí•© ì¸í„°íŽ˜ì´ìŠ¤
- ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥

### ê¸°ë³¸ ì‚¬ìš©ë²•

#### 1. ë‹¨ì¼ í‰ê°€

```python
from scripts.unified_evaluation import UnifiedEvaluator

evaluator = UnifiedEvaluator()

results = evaluator.evaluate_all(
    question="AI í”Œëž«í¼ì˜ ê¸°ë³¸ ê´€ë¦¬ìž ì•„ì´ë””ëŠ”?",
    prediction="ê¸°ë³¸ ê´€ë¦¬ìž ì•„ì´ë””ëŠ” KWATERìž…ë‹ˆë‹¤.",
    ground_truth="ê¸°ë³¸ ê´€ë¦¬ìž ì•„ì´ë””ëŠ” KWATERìž…ë‹ˆë‹¤.",
    contexts=["ê´€ë¦¬ìž ê³„ì •: KWATER", "ì‹œìŠ¤í…œ ì ‘ì† ì •ë³´..."],
    keywords=["KWATER", "ê´€ë¦¬ìž", "ì•„ì´ë””"]
)

# ê²°ê³¼ ì¶œë ¥
evaluator.print_results(results)
```

#### 2. ë°°ì¹˜ í‰ê°€ (ì—¬ëŸ¬ ì§ˆë¬¸ í•œ ë²ˆì—)

```python
qa_pairs = [
    {
        'question': 'ì§ˆë¬¸ 1',
        'prediction': 'ë‹µë³€ 1',
        'ground_truth': 'ì •ë‹µ 1',
        'contexts': [...],
        'keywords': [...]
    },
    {
        'question': 'ì§ˆë¬¸ 2',
        'prediction': 'ë‹µë³€ 2',
        'ground_truth': 'ì •ë‹µ 2',
        'contexts': [...],
        'keywords': [...]
    }
]

batch_results = evaluator.evaluate_batch(qa_pairs)
evaluator.print_results(batch_results)
```

#### 3. ê²°ê³¼ êµ¬ì¡°

```python
{
    'basic_score': {           # ê¸°ë³¸ Score (v5 ë°©ì‹)
        'score': 0.94,
        'numeric_hit': 1.0,
        'unit_hit': 1.0,
        'keyword_hit': 0.85
    },
    'domain_specific': {       # ë„ë©”ì¸ íŠ¹í™”
        'total_score': 0.95,
        'numeric_score': 1.0,
        'unit_score': 1.0,
        'keyword_score': 0.85
    },
    'rag_metrics': {          # RAG í•µì‹¬ 3ëŒ€
        'faithfulness': {'score': 0.85, ...},
        'answer_correctness': {'score': 0.92, ...},
        'context_precision': {'score': 0.75, ...},
        'overall_score': 0.84
    },
    'academic_metrics': {      # í•™ìˆ  í‘œì¤€
        'exact_match': 0.0,
        'token_f1': {'f1': 0.90, ...},
        'rouge_l': {'f1': 0.83, ...},
        'bleu_1': 0.88,
        'bleu_2': 0.75
    },
    'summary': {              # ì£¼ìš” ì ìˆ˜ ìš”ì•½
        'basic_v5_score': 0.94,
        'domain_total_score': 0.95,
        'numeric_accuracy': 1.0,
        'unit_accuracy': 1.0,
        'faithfulness': 0.85,
        'answer_correctness': 0.92,
        'context_precision': 0.75,
        'token_f1': 0.90,
        'rouge_l': 0.83,
        'bleu_2': 0.75
    }
}
```

---

## ê° ì§€í‘œì˜ ì˜ë¯¸ì™€ í™œìš©

### ì ìˆ˜ í•´ì„ ê°€ì´ë“œ

| ì ìˆ˜ ë²”ìœ„ | í‰ê°€ | ì˜ë¯¸ | ì¡°ì¹˜ ì‚¬í•­ |
|----------|------|------|----------|
| **90~100%** | ìš°ìˆ˜ â­â­â­ | ì‹¤ë¬´ í™œìš© ê°€ëŠ¥ | ìœ ì§€ |
| **70~90%** | ì–‘í˜¸ â­â­ | ì¤€ìˆ˜í•œ ì„±ëŠ¥ | ì„¸ë¶€ ê°œì„  |
| **50~70%** | ë³´í†µ â­ | ê°œì„  í•„ìš” | ê²€ìƒ‰/ìƒì„± ì ê²€ |
| **50% ë¯¸ë§Œ** | ë¶€ì¡± âš ï¸ | ì‹œìŠ¤í…œ ì ê²€ í•„ìš” | ì „ë©´ ìž¬ê²€í†  |

### ì§€í‘œë³„ ê°œì„  ë°©ë²•

#### ê¸°ë³¸ Score (v5) / ë„ë©”ì¸ íŠ¹í™” ì ìˆ˜ê°€ ë‚®ì„ ë•Œ
- **ìˆ«ìž ì •í™•ë„ â†“**: 
  - BM25 ê°€ì¤‘ì¹˜ ë†’ì´ê¸°
  - ìˆ«ìž ì£¼ë³€ ë¬¸ë§¥ í™•ìž¥
- **ë‹¨ìœ„ ì •í™•ë„ â†“**: 
  - ë‹¨ìœ„ ë™ì˜ì–´ ì‚¬ì „ í™•ìž¥
  - ë‹¨ìœ„ ì •ê·œí™” ê°•í™”
- **í‚¤ì›Œë“œ ì •í™•ë„ â†“**: 
  - ë„ë©”ì¸ ì‚¬ì „ í™•ìž¥
  - í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§ ê°œì„ 

#### Faithfulness (ì¶©ì‹¤ì„±)ê°€ ë‚®ì„ ë•Œ (í™˜ê° ë°œìƒ)
- Context í’ˆì§ˆ ê°œì„ 
- Temperature ë‚®ì¶”ê¸° (0.0 ê¶Œìž¥)
- Promptì— "ìžë£Œ ê¸°ë°˜ ë‹µë³€" ê°•ì¡°

#### Answer Correctnessê°€ ë‚®ì„ ë•Œ
- ê²€ìƒ‰ í’ˆì§ˆ ê°œì„  (Retrieval)
- ë‹µë³€ ìƒì„± Prompt ê°œì„ 
- Context ìˆ˜(k) ì¡°ì •

#### Context Precisionì´ ë‚®ì„ ë•Œ (ë¶ˆí•„ìš”í•œ ìžë£Œ ë§ŽìŒ)
- Confidence Threshold ë†’ì´ê¸°
- Reranking ê°•í™”
- BM25 vs Vector ê°€ì¤‘ì¹˜ ì¡°ì •

#### í•™ìˆ  ì§€í‘œ(F1, ROUGE, BLEU)ê°€ ë‚®ì„ ë•Œ
- í‘œí˜„ ìŠ¤íƒ€ì¼ ë§žì¶”ê¸°
- ë¬¸ìž¥ êµ¬ì¡° ê°œì„ 
- í•µì‹¬ í‚¤ì›Œë“œ ë³´ì¡´

---

## ì‹¤ì „ ì˜ˆì‹œ

### ì˜ˆì‹œ 1: ë‹¨ì¼ ì§ˆë¬¸ í‰ê°€

```python
from scripts.unified_evaluation import UnifiedEvaluator

evaluator = UnifiedEvaluator()

# í‰ê°€ ì‹¤í–‰
results = evaluator.evaluate_all(
    question="AI í”Œëž«í¼ì˜ ë°œì£¼ê¸°ê´€ì€?",
    prediction="ë°œì£¼ê¸°ê´€ì€ í•œêµ­ìˆ˜ìžì›ê³µì‚¬ìž…ë‹ˆë‹¤.",
    ground_truth="ë°œì£¼ê¸°ê´€ì€ í•œêµ­ìˆ˜ìžì›ê³µì‚¬ìž…ë‹ˆë‹¤.",
    contexts=[
        "ë°œì£¼ê¸°ê´€: í•œêµ­ìˆ˜ìžì›ê³µì‚¬",
        "ì‚¬ì—…ëª…: ê¸ˆê°• ìœ ì—­ ìŠ¤ë§ˆíŠ¸ ì •ìˆ˜ìž¥"
    ],
    keywords=["ë°œì£¼ê¸°ê´€", "í•œêµ­ìˆ˜ìžì›ê³µì‚¬"]
)

# ê²°ê³¼ ì¶œë ¥
evaluator.print_results(results)
```

**ì¶œë ¥**:
```
================================================================================
ðŸ“Š í†µí•© í‰ê°€ ê²°ê³¼
================================================================================

1ï¸âƒ£  ê¸°ë³¸ Score (v5 ë°©ì‹)
   ì¢…í•© ì ìˆ˜: 100.0%

2ï¸âƒ£  ë„ë©”ì¸ íŠ¹í™” í‰ê°€
   ì¢…í•© ì ìˆ˜: 100.0%
   ìˆ«ìž ì •í™•ë„: 100.0%
   ë‹¨ìœ„ ì •í™•ë„: 100.0%
   í‚¤ì›Œë“œ ì •í™•ë„: 100.0%

3ï¸âƒ£  RAG í•µì‹¬ 3ëŒ€ ì§€í‘œ
   Faithfulness (ì¶©ì‹¤ì„±): 100.0%
   Answer Correctness (ì •í™•ë„): 100.0%
   Context Precision (ì •ë°€ë„): 100.0%
   RAG ì¢…í•© ì ìˆ˜: 100.0%

4ï¸âƒ£  í•™ìˆ  í‘œì¤€ ì§€í‘œ
   Token F1: 100.0%
   ROUGE-L: 100.0%
   BLEU-2: 100.0%
   Exact Match: 100.0%
```

---

### ì˜ˆì‹œ 2: ë°°ì¹˜ í‰ê°€ (QA ë²¤ì¹˜ë§ˆí¬)

```python
import json
from scripts.unified_evaluation import UnifiedEvaluator

# QA ë°ì´í„° ë¡œë“œ
with open('data/qa.json', 'r', encoding='utf-8') as f:
    qa_data = json.load(f)

# í‰ê°€í•  ë°ì´í„° ì¤€ë¹„ (RAG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë‹µë³€ ìƒì„± í›„)
qa_pairs = []
for item in qa_data:
    # ë‹µë³€ ìƒì„± (ì˜ˆì‹œ)
    answer_result = pipeline.ask(item['question'])
    
    qa_pairs.append({
        'question': item['question'],
        'prediction': answer_result.text,
        'ground_truth': item['answer'],
        'contexts': [src.chunk.text for src in answer_result.sources],
        'keywords': item.get('accepted_keywords', [])
    })

# ë°°ì¹˜ í‰ê°€ ì‹¤í–‰
evaluator = UnifiedEvaluator()
batch_results = evaluator.evaluate_batch(qa_pairs)

# ê²°ê³¼ ì¶œë ¥
evaluator.print_results(batch_results)

# JSONìœ¼ë¡œ ì €ìž¥
with open('evaluation_results.json', 'w', encoding='utf-8') as f:
    json.dump(batch_results, f, indent=2, ensure_ascii=False)
```

---

### ì˜ˆì‹œ 3: íŠ¹ì • ì§€í‘œë§Œ ì‚¬ìš©

ê° í‰ê°€ ëª¨ë“ˆì„ ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤:

```python
# 1. RAG í•µì‹¬ ì§€í‘œë§Œ ì‚¬ìš©
from scripts.rag_core_metrics import RAGCoreMetrics

rag_scores = RAGCoreMetrics.evaluate_all(
    question, answer, ground_truth, contexts
)
print(f"Faithfulness: {rag_scores['faithfulness']['score']}")

# 2. ë„ë©”ì¸ íŠ¹í™”ë§Œ ì‚¬ìš©
from scripts.enhanced_scoring import DomainSpecificScoring

scorer = DomainSpecificScoring()
numeric_acc = scorer.score_numeric_accuracy(answer, ground_truth)
print(f"ìˆ«ìž ì •í™•ë„: {numeric_acc}")

# 3. í•™ìˆ  ì§€í‘œë§Œ ì‚¬ìš©
from scripts.academic_metrics import AcademicMetrics

academic = AcademicMetrics.evaluate_all(answer, ground_truth)
print(f"Token F1: {academic['token_f1']['f1']}")
```

---

## í‰ê°€ ê²°ê³¼ í™œìš© ì‹œë‚˜ë¦¬ì˜¤

### ðŸ“Š ì‹œë‚˜ë¦¬ì˜¤ 1: ë…¼ë¬¸ ìž‘ì„±

```python
# RAG í•µì‹¬ 3ëŒ€ ì§€í‘œ + í•™ìˆ  ì§€í‘œ ì‚¬ìš©
evaluator = UnifiedEvaluator()
results = evaluator.evaluate_all(...)

summary = results['summary']

print(f"""
ë…¼ë¬¸ ì´ˆë¡ ì˜ˆì‹œ:
ë³¸ ì—°êµ¬ì˜ RAG ì±—ë´‡ ì‹œìŠ¤í…œì„ 20ê°œ ì§ˆë¬¸ìœ¼ë¡œ í‰ê°€í•œ ê²°ê³¼,
- Faithfulness: {summary['faithfulness']*100:.1f}%
- Answer Correctness: {summary['answer_correctness']*100:.1f}%
- Context Precision: {summary['context_precision']*100:.1f}%
- Token F1: {summary['token_f1']*100:.1f}%
ë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤.
""")
```

---

### ðŸ”§ ì‹œë‚˜ë¦¬ì˜¤ 2: ì‹œìŠ¤í…œ ê°œì„ 

```python
# ê°œì„  ì „í›„ ë¹„êµ
before_results = evaluator.evaluate_all(...)
# ... ì‹œìŠ¤í…œ ê°œì„  ...
after_results = evaluator.evaluate_all(...)

# ë¹„êµ ë¶„ì„
improvements = {
    key: after_results['summary'][key] - before_results['summary'][key]
    for key in after_results['summary'].keys()
}

print("ê°œì„  íš¨ê³¼:")
for metric, improvement in improvements.items():
    if improvement > 0:
        print(f"  âœ… {metric}: +{improvement*100:.1f}%p")
    elif improvement < 0:
        print(f"  âš ï¸ {metric}: {improvement*100:.1f}%p")
```

---

### ðŸ“ˆ ì‹œë‚˜ë¦¬ì˜¤ 3: ë²„ì „ ë¹„êµ (v5 vs v6)

```python
# v5 ê²°ê³¼ (ê¸°ë³¸ Scoreë§Œ ìžˆìŒ)
v5_score = 0.870  # 87.0%

# v6 ê²°ê³¼ (ëª¨ë“  ì§€í‘œ)
v6_results = evaluator.evaluate_all(...)

print(f"""
ë²„ì „ ë¹„êµ:
- v5 ê¸°ë³¸ Score: {v5_score*100:.1f}%
- v6 ê¸°ë³¸ Score: {v6_results['summary']['basic_v5_score']*100:.1f}%
- ê°œì„ í­: +{(v6_results['summary']['basic_v5_score'] - v5_score)*100:.1f}%p

v6 ì¶”ê°€ ì§€í‘œ:
- Faithfulness: {v6_results['summary']['faithfulness']*100:.1f}%
- Answer Correctness: {v6_results['summary']['answer_correctness']*100:.1f}%
- Token F1: {v6_results['summary']['token_f1']*100:.1f}%
""")
```

---

## ìš”ì•½

### ì–´ë–¤ í‰ê°€ë¥¼ ì‚¬ìš©í•´ì•¼ í• ê¹Œ?

| ëª©ì  | ì¶”ì²œ í‰ê°€ | ì´ìœ  |
|-----|----------|------|
| **ì‹¤ë¬´ ì„±ëŠ¥ í™•ì¸** | ê¸°ë³¸ Score (v5) + ë„ë©”ì¸ íŠ¹í™” | ìˆ«ìž/ë‹¨ìœ„ ê°•ì¡°, ì‹¤ìš©ì  |
| **ë…¼ë¬¸ ìž‘ì„±** | RAG í•µì‹¬ 3ëŒ€ + í•™ìˆ  í‘œì¤€ | ì¸ìš© ê°€ëŠ¥, í‘œì¤€ ì§€í‘œ |
| **íƒ€ ì‹œìŠ¤í…œ ë¹„êµ** | í•™ìˆ  í‘œì¤€ (F1, ROUGE, BLEU) | êµ­ì œ í‘œì¤€ |
| **í™˜ê° ë¬¸ì œ ë¶„ì„** | Faithfulness (RAG) | ìžë£Œ ê¸°ë°˜ ë‹µë³€ ê²€ì¦ |
| **ê²€ìƒ‰ ì„±ëŠ¥ ë¶„ì„** | Context Precision (RAG) | ê²€ìƒ‰ íš¨ìœ¨ì„± ì¸¡ì • |
| **ì¢…í•© í‰ê°€** | í†µí•© ëª¨ë“ˆ (ëª¨ë‘) | ëª¨ë“  ê´€ì ì—ì„œ ë¶„ì„ |

### í•µì‹¬ ìš”ì•½

```python
# ðŸ’¡ ê°€ìž¥ ì‰¬ìš´ ë°©ë²•: í†µí•© í‰ê°€ ëª¨ë“ˆ ì‚¬ìš©
from scripts.unified_evaluation import UnifiedEvaluator

evaluator = UnifiedEvaluator()
results = evaluator.evaluate_all(
    question, prediction, ground_truth, contexts, keywords
)
evaluator.print_results(results)

# ëª¨ë“  ì§€í‘œê°€ í•œ ë²ˆì—! ðŸŽ‰
```

---

## ì°¸ê³  ë¬¸í—Œ

1. **RAG í‰ê°€**:
   - Es, S., James, J., Espinosa-Anke, L., & Schockaert, S. (2023). RAGAS: Automated Evaluation of Retrieval Augmented Generation. arXiv:2309.15217.

2. **QA í‰ê°€**:
   - Rajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P. (2016). SQuAD: 100,000+ Questions for Machine Comprehension of Text. EMNLP 2016.

3. **ìš”ì•½ í‰ê°€**:
   - Lin, C. Y. (2004). ROUGE: A Package for Automatic Evaluation of Summaries. ACL Workshop 2004.

4. **ë²ˆì—­ í‰ê°€**:
   - Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002). BLEU: a Method for Automatic Evaluation of Machine Translation. ACL 2002.

---

**ìž‘ì„±ì¼**: 2025-10-12  
**ë²„ì „**: v6  
**ë¬¸ì˜**: í”„ë¡œì íŠ¸ ì´ìŠˆ íŠ¸ëž˜ì»¤

