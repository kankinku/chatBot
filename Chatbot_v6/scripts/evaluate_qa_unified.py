#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© í‰ê°€ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•œ QA ë²¤ì¹˜ë§ˆí¬

qa.jsonì˜ ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ë‹µë³€ì„ ìƒì„±í•˜ê³  í†µí•© í‰ê°€ ëª¨ë“ˆë¡œ í‰ê°€í•©ë‹ˆë‹¤.
ëª¨ë“  í‰ê°€ ì§€í‘œ(ê¸°ë³¸ Score, ë„ë©”ì¸ íŠ¹í™”, RAG 3ëŒ€, í•™ìˆ  í‘œì¤€)ë¥¼ í•œ ë²ˆì— ê³„ì‚°í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import sys
import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from modules.core.types import Chunk
from modules.pipeline.rag_pipeline import RAGPipeline
from config.pipeline_config import PipelineConfig
from config.model_config import ModelConfig, EmbeddingModelConfig, LLMModelConfig
from modules.core.logger import setup_logging, get_logger
from scripts.unified_evaluation import UnifiedEvaluator

setup_logging(log_dir="logs", log_level="INFO", log_format="json")
logger = get_logger(__name__)


class UnifiedQABenchmark:
    """í†µí•© í‰ê°€ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•œ QA ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(
        self,
        pipeline: RAGPipeline,
        qa_data: List[Dict[str, Any]],
    ):
        self.pipeline = pipeline
        self.qa_data = qa_data
        self.evaluator = UnifiedEvaluator()
        self.results = []
    
    def run(self) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        logger.info(f"=== í†µí•© í‰ê°€ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ===")
        logger.info(f"ì´ ì§ˆë¬¸ ìˆ˜: {len(self.qa_data)}")
        
        start_time = time.time()
        qa_pairs = []
        
        for i, item in enumerate(self.qa_data, 1):
            q_id = item.get("id", i)
            question = item["question"]
            gold_answer = item.get("answer", "")
            keywords = item.get("accepted_keywords", [])
            
            logger.info(f"\n[{i}/{len(self.qa_data)}] ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘...")
            logger.info(f"ì§ˆë¬¸: {question}")
            
            try:
                # ë‹µë³€ ìƒì„±
                t0 = time.time()
                answer = self.pipeline.ask(question, top_k=50)
                elapsed_ms = int((time.time() - t0) * 1000)
                
                # Context í…ìŠ¤íŠ¸ ì¶”ì¶œ
                context_texts = [src.chunk.text for src in answer.sources]
                
                # í†µí•© í‰ê°€ ì‹¤í–‰
                eval_results = self.evaluator.evaluate_all(
                    question=question,
                    prediction=answer.text,
                    ground_truth=gold_answer,
                    contexts=context_texts,
                    keywords=keywords
                )
                
                logger.info(f"ë‹µë³€: {answer.text[:100]}...")
                logger.info(f"ê¸°ë³¸ Score: {eval_results['summary']['basic_v5_score']:.3f}")
                logger.info(f"ì‹ ë¢°ë„: {answer.confidence:.3f}, ì‹œê°„: {elapsed_ms}ms")
                
                # ê²°ê³¼ ì €ì¥
                result = {
                    "id": q_id,
                    "question": question,
                    "gold_answer": gold_answer,
                    "prediction": answer.text,
                    "keywords": keywords,
                    
                    # ê¸°ë³¸ ì •ë³´
                    "confidence": answer.confidence,
                    "num_sources": len(answer.sources),
                    "time_ms": elapsed_ms,
                    
                    # í†µí•© í‰ê°€ ê²°ê³¼
                    "evaluation": {
                        "basic_score": eval_results['basic_score'],
                        "domain_specific": eval_results['domain_specific'],
                        "rag_metrics": eval_results['rag_metrics'],
                        "academic_metrics": eval_results['academic_metrics'],
                        "summary": eval_results['summary']
                    },
                    
                    # íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­
                    "pipeline_metrics": answer.metrics,
                }
                
                self.results.append(result)
                
                # ë°°ì¹˜ í‰ê°€ìš© ë°ì´í„° ìˆ˜ì§‘
                qa_pairs.append({
                    'question': question,
                    'prediction': answer.text,
                    'ground_truth': gold_answer,
                    'contexts': context_texts,
                    'keywords': keywords
                })
            
            except Exception as e:
                logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
                
                # ì‹¤íŒ¨ ê²°ê³¼
                self.results.append({
                    "id": q_id,
                    "question": question,
                    "gold_answer": gold_answer,
                    "prediction": f"[ERROR] {str(e)}",
                    "keywords": keywords,
                    "confidence": 0.0,
                    "num_sources": 0,
                    "time_ms": 0,
                    "error": str(e),
                })
        
        total_time = time.time() - start_time
        
        # í†µê³„ ê³„ì‚°
        stats = self._calculate_stats(total_time)
        
        logger.info("\n=== ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ ===")
        logger.info(f"ì´ ì§ˆë¬¸: {stats['total_questions']}")
        logger.info(f"ì„±ê³µ: {stats['successful']}, ì‹¤íŒ¨: {stats['failed']}")
        logger.info(f"í‰ê·  ê¸°ë³¸ Score: {stats.get('avg_basic_v5_score', 0):.3f}")
        logger.info(f"í‰ê·  ì‹œê°„: {stats['avg_time_ms']:.1f}ms")
        logger.info(f"ì´ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ")
        
        return {
            "stats": stats,
            "results": self.results,
        }
    
    def _calculate_stats(self, total_time: float) -> Dict[str, Any]:
        """í†µê³„ ê³„ì‚°"""
        valid_results = [r for r in self.results if "error" not in r]
        
        if not valid_results:
            return {
                "total_questions": len(self.qa_data),
                "successful": 0,
                "failed": len(self.qa_data),
                "timestamp": datetime.now().isoformat(),
            }
        
        # ê¸°ë³¸ í†µê³„
        stats = {
            "total_questions": len(self.qa_data),
            "successful": len(valid_results),
            "failed": len(self.qa_data) - len(valid_results),
            "total_time_seconds": total_time,
            "timestamp": datetime.now().isoformat(),
        }
        
        # ì‹œê°„ í†µê³„
        times = [r["time_ms"] for r in valid_results]
        stats["avg_time_ms"] = sum(times) / len(times) if times else 0.0
        stats["min_time_ms"] = min(times) if times else 0.0
        stats["max_time_ms"] = max(times) if times else 0.0
        
        # í‰ê°€ ì ìˆ˜ í†µê³„ (ê° ì§€í‘œë³„)
        metrics_to_aggregate = [
            'basic_v5_score',
            'domain_total_score',
            'numeric_accuracy',
            'unit_accuracy',
            'keyword_accuracy',
            'faithfulness',
            'answer_correctness',
            'context_precision',
            'rag_overall',
            'token_f1',
            'rouge_l',
            'bleu_2',
            'exact_match'
        ]
        
        for metric in metrics_to_aggregate:
            values = []
            for r in valid_results:
                summary = r.get('evaluation', {}).get('summary', {})
                if metric in summary:
                    values.append(summary[metric])
            
            if values:
                stats[f'avg_{metric}'] = sum(values) / len(values)
                stats[f'min_{metric}'] = min(values)
                stats[f'max_{metric}'] = max(values)
        
        return stats
    
    def save_report(self, output_path: str):
        """ê²°ê³¼ ì €ì¥ (JSON + ìš”ì•½ í…ìŠ¤íŠ¸)"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # í†µê³„ ì¬ê³„ì‚°
        stats = self._calculate_stats(0)
        
        # JSON ì €ì¥
        report = {
            "stats": stats,
            "results": self.results,
        }
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ê²°ê³¼ ì €ì¥: {output_path}")
        
        # í…ìŠ¤íŠ¸ ìš”ì•½ íŒŒì¼ ìƒì„±
        summary_path = output_path.parent / f"{output_path.stem}_summary.txt"
        self._save_text_summary(summary_path, stats)
        
        logger.info(f"ìš”ì•½ ì €ì¥: {summary_path}")
    
    def _save_text_summary(self, summary_path: Path, stats: Dict):
        """í…ìŠ¤íŠ¸ ìš”ì•½ íŒŒì¼ ìƒì„±"""
        with open(summary_path, "w", encoding="utf-8") as f:
            f.write("=" * 80 + "\n")
            f.write("í†µí•© í‰ê°€ ì‹œìŠ¤í…œ - QA ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"í‰ê°€ ì¼ì‹œ: {stats['timestamp']}\n")
            f.write(f"ì´ ì§ˆë¬¸: {stats['total_questions']}ê°œ\n")
            f.write(f"ì„±ê³µ: {stats['successful']}ê°œ / ì‹¤íŒ¨: {stats['failed']}ê°œ\n\n")
            
            # 1. ê¸°ë³¸ Score (v5 ë°©ì‹)
            f.write("=" * 80 + "\n")
            f.write("1ï¸âƒ£  ê¸°ë³¸ Score (v5 ë°©ì‹) - ì‹¤ë¬´ ì„±ëŠ¥\n")
            f.write("=" * 80 + "\n\n")
            if 'avg_basic_v5_score' in stats:
                f.write(f"í‰ê·  ì ìˆ˜:  {stats['avg_basic_v5_score']*100:6.1f}%\n")
                f.write(f"ìµœê³  ì ìˆ˜:  {stats['max_basic_v5_score']*100:6.1f}%\n")
                f.write(f"ìµœì € ì ìˆ˜:  {stats['min_basic_v5_score']*100:6.1f}%\n\n")
            
            # 2. ë„ë©”ì¸ íŠ¹í™”
            f.write("=" * 80 + "\n")
            f.write("2ï¸âƒ£  ë„ë©”ì¸ íŠ¹í™” í‰ê°€ - ì •ìˆ˜ì¥ íŠ¹í™”\n")
            f.write("=" * 80 + "\n\n")
            if 'avg_domain_total_score' in stats:
                f.write(f"ì¢…í•© ì ìˆ˜:  {stats['avg_domain_total_score']*100:6.1f}%\n")
                f.write(f"ìˆ«ì ì •í™•ë„: {stats['avg_numeric_accuracy']*100:6.1f}%\n")
                f.write(f"ë‹¨ìœ„ ì •í™•ë„: {stats['avg_unit_accuracy']*100:6.1f}%\n")
                f.write(f"í‚¤ì›Œë“œ ì •í™•ë„: {stats['avg_keyword_accuracy']*100:6.1f}%\n\n")
            
            # 3. RAG í•µì‹¬ 3ëŒ€ ì§€í‘œ
            f.write("=" * 80 + "\n")
            f.write("3ï¸âƒ£  RAG í•µì‹¬ 3ëŒ€ ì§€í‘œ - í•™ìˆ  ì—°êµ¬ìš©\n")
            f.write("=" * 80 + "\n\n")
            if 'avg_faithfulness' in stats:
                f.write(f"Faithfulness (ì¶©ì‹¤ì„±):      {stats['avg_faithfulness']*100:6.1f}%\n")
                f.write(f"Answer Correctness (ì •í™•ë„): {stats['avg_answer_correctness']*100:6.1f}%\n")
                f.write(f"Context Precision (ì •ë°€ë„):  {stats['avg_context_precision']*100:6.1f}%\n")
                f.write(f"RAG ì¢…í•© ì ìˆ˜:             {stats['avg_rag_overall']*100:6.1f}%\n\n")
            
            # 4. í•™ìˆ  í‘œì¤€ ì§€í‘œ
            f.write("=" * 80 + "\n")
            f.write("4ï¸âƒ£  í•™ìˆ  í‘œì¤€ ì§€í‘œ - ë²”ìš© NLP í‰ê°€\n")
            f.write("=" * 80 + "\n\n")
            if 'avg_token_f1' in stats:
                f.write(f"Token F1:    {stats['avg_token_f1']*100:6.1f}%\n")
                f.write(f"ROUGE-L:     {stats['avg_rouge_l']*100:6.1f}%\n")
                f.write(f"BLEU-2:      {stats['avg_bleu_2']*100:6.1f}%\n")
                f.write(f"Exact Match: {stats['avg_exact_match']*100:6.1f}%\n\n")
            
            # ì„±ëŠ¥ ì§€í‘œ
            f.write("=" * 80 + "\n")
            f.write("â±ï¸  ì„±ëŠ¥ ì§€í‘œ\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {stats['avg_time_ms']/1000:.2f}ì´ˆ\n")
            f.write(f"ìµœì†Œ ì‘ë‹µ ì‹œê°„: {stats['min_time_ms']/1000:.2f}ì´ˆ\n")
            f.write(f"ìµœëŒ€ ì‘ë‹µ ì‹œê°„: {stats['max_time_ms']/1000:.2f}ì´ˆ\n")
            f.write(f"ì´ ì†Œìš” ì‹œê°„:  {stats['total_time_seconds']:.1f}ì´ˆ\n\n")
            
            # ì§ˆë¬¸ë³„ ìƒì„¸ ê²°ê³¼
            f.write("=" * 80 + "\n")
            f.write("ğŸ“‹ ì§ˆë¬¸ë³„ ìƒì„¸ ê²°ê³¼\n")
            f.write("=" * 80 + "\n\n")
            
            for i, result in enumerate(self.results, 1):
                f.write(f"[{i}] {result['question']}\n")
                
                if 'error' in result:
                    f.write(f"    âŒ ì˜¤ë¥˜: {result['error']}\n")
                else:
                    summary = result['evaluation']['summary']
                    f.write(f"    ê¸°ë³¸ Score: {summary['basic_v5_score']*100:5.1f}%")
                    f.write(f" | Faithfulness: {summary.get('faithfulness', 0)*100:5.1f}%")
                    f.write(f" | Token F1: {summary['token_f1']*100:5.1f}%\n")
                    f.write(f"    ë‹µë³€: {result['prediction'][:80]}...\n")
                
                f.write("\n")
            
            f.write("=" * 80 + "\n")


def load_qa_data(path: str) -> List[Dict[str, Any]]:
    """QA ë°ì´í„° ë¡œë“œ"""
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def auto_build_corpus(pdf_dir: str, output_path: str) -> bool:
    """
    Corpus ìë™ ìƒì„± (PDF ì¶”ì¶œ -> ì²­í‚¹ -> ì €ì¥)
    
    Args:
        pdf_dir: PDF íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        output_path: ìƒì„±í•  corpus íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    try:
        # build_corpusì˜ process_pdf í•¨ìˆ˜ë¥¼ ì„í¬íŠ¸í•˜ì—¬ ì‚¬ìš©
        from scripts.build_corpus import process_pdf, save_corpus
        from pathlib import Path
        
        pdf_dir_path = Path(pdf_dir)
        pdf_files = list(pdf_dir_path.glob("*.pdf"))
        
        if not pdf_files:
            return False
        
        all_chunks = []
        
        for pdf_path in pdf_files:
            logger.info(f"  ì²˜ë¦¬ ì¤‘: {pdf_path.name}")
            try:
                chunks = process_pdf(
                    pdf_path,
                    use_ocr_correction=False,  # ìë™í™” ì‹œ ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ë¹„í™œì„±í™”
                    use_page_based_chunking=True,
                )
                all_chunks.extend(chunks)
                logger.info(f"    âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            except Exception as e:
                logger.error(f"    âŒ ì‹¤íŒ¨: {e}")
                continue
        
        if not all_chunks:
            return False
        
        # Corpus ì €ì¥
        save_corpus(all_chunks, Path(output_path))
        
        logger.info(f"ğŸ“Š ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±ë¨")
        
        # í†µê³„ ì¶œë ¥
        measurements_count = sum(1 for chunk in all_chunks if chunk.extra.get('measurements'))
        neighbor_count = sum(1 for chunk in all_chunks if chunk.neighbor_hint)
        
        logger.info(f"   - ì¸¡ì •ê°’ í¬í•¨: {measurements_count}/{len(all_chunks)}")
        logger.info(f"   - ì´ì›ƒ ì •ë³´ í¬í•¨: {neighbor_count}/{len(all_chunks)}")
        
        return True
        
    except Exception as e:
        logger.error(f"Corpus ìë™ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        return False


def load_chunks_from_corpus(corpus_path: str) -> List[Chunk]:
    """
    JSONL corpus íŒŒì¼ì—ì„œ ì²­í¬ ë¡œë“œ (í™•ì¥ëœ ë©”íƒ€ë°ì´í„° í¬í•¨)
    
    ê°œì„ ëœ ì²­í‚¹ ì‹œìŠ¤í…œì˜ ëª¨ë“  ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤:
    - neighbor_hint: ì´ì›ƒ ì²­í¬ ì •ë³´
    - extra: ì¸¡ì •ê°’ ë“± ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    """
    chunks = []
    with open(corpus_path, "r", encoding="utf-8") as f:
        for line in f:
            data = json.loads(line.strip())
            
            # neighbor_hint ì²˜ë¦¬ (tupleë¡œ ë³€í™˜)
            neighbor_hint = data.get("neighbor_hint")
            if neighbor_hint and isinstance(neighbor_hint, list):
                neighbor_hint = tuple(neighbor_hint)
            
            chunk = Chunk(
                doc_id=data["doc_id"],
                filename=data["filename"],
                page=data.get("page"),
                start_offset=data.get("start_offset", 0),
                length=data.get("length", len(data["text"])),
                text=data["text"],
                neighbor_hint=neighbor_hint,  # ì´ì›ƒ ì •ë³´ ë³µì›
                extra=data.get("extra", {}),  # ì¸¡ì •ê°’ ë“± ì¶”ê°€ ë©”íƒ€ë°ì´í„° ë³µì›
            )
            chunks.append(chunk)
    
    return chunks


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    
    # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ì €ì¥
    import os
    original_cwd = Path(os.getcwd())
    
    # ê¸°ë³¸ ê²½ë¡œ
    default_qa = str(project_root / "data" / "qa.json")
    default_corpus = str(project_root / "data" / "corpus.jsonl")
    default_config = str(project_root / "config" / "default.yaml")
    default_output = str(project_root / "out" / "benchmarks" / "qa_unified_result.json")
    
    parser = argparse.ArgumentParser(
        description="í†µí•© í‰ê°€ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•œ QA ë²¤ì¹˜ë§ˆí¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê¸°ë³¸ ì‹¤í–‰ (qa.json ì‚¬ìš©)
  python scripts/evaluate_qa_unified.py
  
  # ë‹¤ë¥¸ QA íŒŒì¼ ì‚¬ìš©
  python scripts/evaluate_qa_unified.py --qa data/qa_test5.json
  
  # ì¶œë ¥ íŒŒì¼ ì§€ì •
  python scripts/evaluate_qa_unified.py --output out/my_result.json
        """
    )
    parser.add_argument("--qa", default=default_qa, help="QA ë°ì´í„° íŒŒì¼ (ê¸°ë³¸: data/qa.json)")
    parser.add_argument("--corpus", default=default_corpus, help="Corpus íŒŒì¼ (ê¸°ë³¸: data/corpus.jsonl)")
    parser.add_argument("--config", default=default_config, help="ì„¤ì • íŒŒì¼ (ê¸°ë³¸: config/default.yaml)")
    parser.add_argument("--output", default=default_output, help="ì¶œë ¥ íŒŒì¼ (ê¸°ë³¸: out/benchmarks/qa_unified_result.json)")
    parser.add_argument("--top-k", type=int, default=50, help="ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸: 50)")
    parser.add_argument("--mode", default="accuracy", choices=["accuracy", "speed"], help="ì‹¤í–‰ ëª¨ë“œ (ê¸°ë³¸: accuracy)")
    parser.add_argument("--model", default="qwen2.5:3b-instruct-q4_K_M", help="LLM ëª¨ë¸ëª…")
    args = parser.parse_args()
    
    # ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    args.qa = str((original_cwd / args.qa).resolve() if not Path(args.qa).is_absolute() else Path(args.qa))
    args.corpus = str((original_cwd / args.corpus).resolve() if not Path(args.corpus).is_absolute() else Path(args.corpus))
    args.config = str((original_cwd / args.config).resolve() if not Path(args.config).is_absolute() else Path(args.config))
    args.output = str((original_cwd / args.output).resolve() if not Path(args.output).is_absolute() else Path(args.output))
    
    # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ë³€ê²½
    os.chdir(project_root)
    
    logger.info("=" * 80)
    logger.info("í†µí•© í‰ê°€ ì‹œìŠ¤í…œ - QA ë²¤ì¹˜ë§ˆí¬")
    logger.info("=" * 80)
    logger.info(f"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    
    # QA ë°ì´í„° ë¡œë“œ
    logger.info(f"\nğŸ“– QA ë°ì´í„° ë¡œë”©: {args.qa}")
    if not Path(args.qa).exists():
        logger.error(f"QA íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {args.qa}")
        sys.exit(1)
    
    qa_data = load_qa_data(args.qa)
    logger.info(f"âœ… QA ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(qa_data)}ê°œ ì§ˆë¬¸")
    
    # ì„¤ì • ë¡œë“œ
    logger.info(f"\nâš™ï¸  íŒŒì´í”„ë¼ì¸ ì„¤ì • ë¡œë”©: {args.config}")
    config_path = Path(args.config)
    if config_path.exists():
        pipeline_config = PipelineConfig.from_file(config_path)
    else:
        logger.warning(f"ì„¤ì • íŒŒì¼ ì—†ìŒ, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
        pipeline_config = PipelineConfig()
    
    pipeline_config.flags.mode = args.mode
    
    # Corpus ë¡œë“œ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
    logger.info(f"\nğŸ“š Corpus ë¡œë”©: {args.corpus}")
    if not Path(args.corpus).exists():
        logger.warning(f"âš ï¸  Corpus íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
        
        # PDF ë””ë ‰í† ë¦¬ í™•ì¸
        pdf_dir = project_root / "data"
        pdf_files = list(pdf_dir.glob("*.pdf"))
        
        if not pdf_files:
            logger.error(f"âŒ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pdf_dir}")
            logger.error("data/ ë””ë ‰í† ë¦¬ì— PDF íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
            sys.exit(1)
        
        logger.info(f"ğŸ“„ {len(pdf_files)}ê°œ PDF íŒŒì¼ ë°œê²¬")
        logger.info("ğŸ”§ Corpus ìë™ ìƒì„± ì¤‘...")
        
        # build_corpus ìë™ ì‹¤í–‰
        success = auto_build_corpus(
            pdf_dir=str(pdf_dir),
            output_path=args.corpus
        )
        
        if not success:
            logger.error("âŒ Corpus ìƒì„± ì‹¤íŒ¨")
            sys.exit(1)
        
        logger.info(f"âœ… Corpus ìë™ ìƒì„± ì™„ë£Œ: {args.corpus}")
    
    chunks = load_chunks_from_corpus(args.corpus)
    logger.info(f"âœ… Corpus ë¡œë“œ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    logger.info("\nğŸš€ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
    model_config = ModelConfig(
        embedding=EmbeddingModelConfig(device="cpu"),
        llm=LLMModelConfig(
            host="localhost",
            port=11434,
            model_name=args.model
        )
    )
    
    try:
        pipeline = RAGPipeline(
            chunks=chunks,
            pipeline_config=pipeline_config,
            model_config=model_config,
            evaluation_mode=True,  # í‰ê°€ ëª¨ë“œ í™œì„±í™”
        )
        logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ (í‰ê°€ ëª¨ë“œ)")
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹œì‘")
    logger.info("=" * 80)
    
    benchmark = UnifiedQABenchmark(pipeline, qa_data)
    
    try:
        result = benchmark.run()
        
        # ê²°ê³¼ ì €ì¥
        benchmark.save_report(args.output)
        
        # ìµœì¢… ìš”ì•½ ì¶œë ¥
        stats = result['stats']
        
        print("\n" + "=" * 80)
        print("âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
        print("=" * 80)
        
        print("\nğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½:")
        print(f"\n1ï¸âƒ£  ê¸°ë³¸ Score (v5):        {stats.get('avg_basic_v5_score', 0)*100:6.1f}%")
        print(f"2ï¸âƒ£  ë„ë©”ì¸ íŠ¹í™” ì¢…í•©:        {stats.get('avg_domain_total_score', 0)*100:6.1f}%")
        print(f"    - ìˆ«ì ì •í™•ë„:          {stats.get('avg_numeric_accuracy', 0)*100:6.1f}%")
        print(f"    - ë‹¨ìœ„ ì •í™•ë„:          {stats.get('avg_unit_accuracy', 0)*100:6.1f}%")
        
        if 'avg_faithfulness' in stats:
            print(f"3ï¸âƒ£  RAG í•µì‹¬ ì§€í‘œ:")
            print(f"    - Faithfulness:         {stats['avg_faithfulness']*100:6.1f}%")
            print(f"    - Answer Correctness:   {stats['avg_answer_correctness']*100:6.1f}%")
            print(f"    - Context Precision:    {stats['avg_context_precision']*100:6.1f}%")
        
        if 'avg_token_f1' in stats:
            print(f"4ï¸âƒ£  í•™ìˆ  í‘œì¤€:")
            print(f"    - Token F1:             {stats['avg_token_f1']*100:6.1f}%")
            print(f"    - ROUGE-L:              {stats['avg_rouge_l']*100:6.1f}%")
        
        print(f"\nâ±ï¸  í‰ê·  ì‘ë‹µ ì‹œê°„:          {stats['avg_time_ms']/1000:.2f}ì´ˆ")
        
        print("\nğŸ“ ê²°ê³¼ íŒŒì¼:")
        print(f"  - JSON: {args.output}")
        print(f"  - TXT:  {Path(args.output).parent / f'{Path(args.output).stem}_summary.txt'}")
        
        print("\n" + "=" * 80)
        
    except KeyboardInterrupt:
        logger.warning("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()

