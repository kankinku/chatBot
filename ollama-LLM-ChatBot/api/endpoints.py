"""
FastAPI ì—”ë“œí¬ì¸íŠ¸ (ìµœì í™” ë²„ì „)

ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•œ ê°„ì†Œí™”ëœ API
"""

import os
import sys
import uuid
import tempfile
import locale
import time
from typing import List, Dict, Optional, Any
from datetime import datetime
from pathlib import Path
import logging

from fastapi import FastAPI, File, UploadFile, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
import uvicorn

# í•µì‹¬ ëª¨ë“ˆë“¤ ì„í¬íŠ¸
from core.document.pdf_processor import PDFProcessor, TextChunk
from core.document.vector_store import HybridVectorStore, VectorStoreInterface
from core.query.question_analyzer import QuestionAnalyzer, AnalyzedQuestion, ConversationItem
from core.llm.answer_generator import AnswerGenerator, Answer, ModelType, GenerationConfig
from core.database.sql_generator import SQLGenerator, DatabaseSchema, SQLQuery

from core.query.query_router import QueryRouter, QueryRoute
from core.query.llm_greeting_handler import GreetingHandler
from utils.chatbot_logger import chatbot_logger, QuestionType, ProcessingStep

logger = logging.getLogger(__name__)

# Pydantic ëª¨ë¸ë“¤ (ë‹¨ìˆœí™”)
class QuestionRequest(BaseModel):
    """ì§ˆë¬¸ ìš”ì²­ ëª¨ë¸"""
    question: str = Field(..., description="ì‚¬ìš©ì ì§ˆë¬¸")
    pdf_id: str = Field("", description="PDF ë¬¸ì„œ ì‹ë³„ì")
    user_id: str = Field("", description="ì‚¬ìš©ì ì‹ë³„ì")
    use_conversation_context: bool = Field(True, description="ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ì—¬ë¶€")
    max_chunks: int = Field(5, description="ê²€ìƒ‰í•  ìµœëŒ€ ì²­í¬ ìˆ˜ (ê¶Œì¥: 3~5)")

class QuestionResponse(BaseModel):
    """ì§ˆë¬¸ ì‘ë‹µ ëª¨ë¸"""
    answer: str = Field(..., description="ìƒì„±ëœ ë‹µë³€")
    confidence_score: float = Field(..., description="ë‹µë³€ ì‹ ë¢°ë„")
    used_chunks: List[str] = Field(..., description="ì‚¬ìš©ëœ ë¬¸ì„œ ì²­í¬ IDë“¤")
    generation_time: float = Field(..., description="ë‹µë³€ ìƒì„± ì‹œê°„ (ì´ˆ)")
    question_type: str = Field(..., description="ì§ˆë¬¸ ìœ í˜•")
    llm_model_name: str = Field(..., description="ì‚¬ìš©ëœ ëª¨ë¸ ì´ë¦„")
    pipeline_type: str = Field("basic", description="ì‚¬ìš©ëœ íŒŒì´í”„ë¼ì¸ íƒ€ì…")
    sql_query: Optional[str] = Field(None, description="ìƒì„±ëœ SQL ì¿¼ë¦¬")

class PDFUploadResponse(BaseModel):
    """PDF ì—…ë¡œë“œ ì‘ë‹µ ëª¨ë¸"""
    pdf_id: str = Field(..., description="ìƒì„±ëœ PDF ì‹ë³„ì")
    filename: str = Field(..., description="ì—…ë¡œë“œëœ íŒŒì¼ëª…")
    total_pages: int = Field(..., description="ì´ í˜ì´ì§€ ìˆ˜")
    total_chunks: int = Field(..., description="ìƒì„±ëœ ì²­í¬ ìˆ˜")
    processing_time: float = Field(..., description="ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)")

class SystemStatusResponse(BaseModel):
    """ì‹œìŠ¤í…œ ìƒíƒœ ì‘ë‹µ ëª¨ë¸"""
    status: str = Field(..., description="ì‹œìŠ¤í…œ ìƒíƒœ")
    llm_model_loaded: bool = Field(..., description="ëª¨ë¸ ë¡œë“œ ìƒíƒœ")
    total_pdfs: int = Field(..., description="ë“±ë¡ëœ PDF ìˆ˜")
    total_chunks: int = Field(..., description="ì´ ì²­í¬ ìˆ˜")
    memory_usage: Dict[str, Any] = Field(..., description="ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰")

class KeywordCacheResponse(BaseModel):
    """í‚¤ì›Œë“œ ìºì‹œ ì‘ë‹µ ëª¨ë¸"""
    total_keywords: int = Field(..., description="ì´ í‚¤ì›Œë“œ ìˆ˜")
    frequent_keywords: int = Field(..., description="ìì£¼ ì‚¬ìš©ëœ í‚¤ì›Œë“œ ìˆ˜")
    extracted_keywords: int = Field(..., description="ì¶”ì¶œëœ í‚¤ì›Œë“œ ìˆ˜")
    cache_threshold: int = Field(..., description="ìºì‹œ ì„ê³„ê°’")
    top_keywords: Dict[str, int] = Field(..., description="ìƒìœ„ í‚¤ì›Œë“œ (ìµœëŒ€ 10ê°œ)")

class KeywordPipelineResponse(BaseModel):
    """í‚¤ì›Œë“œ íŒŒì´í”„ë¼ì¸ ì¶”ê°€ ì‘ë‹µ ëª¨ë¸"""
    success: bool = Field(..., description="ì„±ê³µ ì—¬ë¶€")
    added_keywords: List[str] = Field(..., description="ì¶”ê°€ëœ í‚¤ì›Œë“œ ëª©ë¡")
    message: str = Field(..., description="ì‘ë‹µ ë©”ì‹œì§€")

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title=os.getenv("API_TITLE", "ë²”ìš© RAG ì‹œìŠ¤í…œ API"),
    description=os.getenv("API_DESCRIPTION", "ë²”ìš© ë¬¸ì„œ ê²€ìƒ‰ ë° ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì‹œìŠ¤í…œ API"),
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# UTF-8 ì¸ì½”ë”© ì„¤ì •
# ì‹œìŠ¤í…œ ì¸ì½”ë”©ì„ UTF-8ë¡œ ì„¤ì •
if sys.platform.startswith('linux'):
    locale.setlocale(locale.LC_ALL, 'C.UTF-8')
elif sys.platform.startswith('win'):
    locale.setlocale(locale.LC_ALL, 'Korean_Korea.UTF-8')
else:
    locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ê°ì²´ë“¤
pdf_processor: Optional[PDFProcessor] = None
vector_store: Optional[VectorStoreInterface] = None
question_analyzer: Optional[QuestionAnalyzer] = None
answer_generator: Optional[AnswerGenerator] = None
sql_generator: Optional[SQLGenerator] = None
query_router: Optional[QueryRouter] = None
llm_greeting_handler: Optional[GreetingHandler] = None

# PDF ë©”íƒ€ë°ì´í„° ì €ì¥ì†Œ
pdf_metadata: Dict[str, Dict] = {}

# ì˜ì¡´ì„± í•¨ìˆ˜ë“¤
def get_pdf_processor() -> PDFProcessor:
    """PDF ì²˜ë¦¬ê¸° ì˜ì¡´ì„±"""
    global pdf_processor
    if pdf_processor is None:
        pdf_processor = PDFProcessor()
    return pdf_processor

def get_vector_store() -> VectorStoreInterface:
    """ë²¡í„° ì €ì¥ì†Œ ì˜ì¡´ì„±"""
    global vector_store
    if vector_store is None:
        logger.info("ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘...")
        vector_store = HybridVectorStore()
        total_chunks = vector_store.get_total_chunks()
        logger.info(f"ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ - ì´ ì²­í¬ ìˆ˜: {total_chunks}")
    return vector_store

def get_question_analyzer() -> QuestionAnalyzer:
    """ì§ˆë¬¸ ë¶„ì„ê¸° ì˜ì¡´ì„±"""
    global question_analyzer
    if question_analyzer is None:
        question_analyzer = QuestionAnalyzer()
    return question_analyzer

def get_answer_generator() -> AnswerGenerator:
    """ë‹µë³€ ìƒì„±ê¸° ì˜ì¡´ì„±"""
    global answer_generator
    if answer_generator is None:
        answer_generator = AnswerGenerator()
    return answer_generator

def get_sql_generator() -> SQLGenerator:
    """SQL ìƒì„±ê¸° ì˜ì¡´ì„±"""
    global sql_generator
    if sql_generator is None:
        sql_generator = SQLGenerator()
    return sql_generator

def get_query_router() -> QueryRouter:
    """ì¿¼ë¦¬ ë¼ìš°í„° ì˜ì¡´ì„±"""
    global query_router
    if query_router is None:
        query_router = QueryRouter()
    return query_router

def get_llm_greeting_handler() -> GreetingHandler:
    """LLM ê¸°ë°˜ ì¸ì‚¬ë§ í•¸ë“¤ëŸ¬ ì˜ì¡´ì„±"""
    global llm_greeting_handler
    if llm_greeting_handler is None:
        # answer_generatorê°€ ì¤€ë¹„ëœ í›„ì— ì´ˆê¸°í™”
        answer_gen = get_answer_generator()
        llm_greeting_handler = GreetingHandler(answer_gen)
    return llm_greeting_handler

def initialize_system():
    """ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ìë™ PDF ì—…ë¡œë“œ"""
    global pdf_processor, vector_store, question_analyzer, answer_generator, sql_generator, query_router
    
    try:
        logger.info("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
        
        # ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”
        pdf_processor = PDFProcessor()
        vector_store = HybridVectorStore()
        question_analyzer = QuestionAnalyzer()
        answer_generator = AnswerGenerator()
        sql_generator = SQLGenerator()
        query_router = QueryRouter()
        
        logger.info("ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ê¸°ì¡´ PDF ë¬¸ì„œ ë¡œë“œ
        try:
            existing_pdfs = vector_store.get_all_pdfs()
            for pdf_info in existing_pdfs:
                pdf_metadata[pdf_info['id']] = pdf_info
            logger.info(f"ê¸°ì¡´ PDF {len(existing_pdfs)}ê°œ ë¡œë“œ ì™„ë£Œ")
            
            # ê¸°ì¡´ ì²­í¬ ìˆ˜ í™•ì¸
            total_chunks = vector_store.get_total_chunks()
            logger.info(f"ê¸°ì¡´ ì²­í¬ ìˆ˜: {total_chunks}")
            
            # ì´ë¯¸ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìë™ ì—…ë¡œë“œ ê±´ë„ˆë›°ê¸°
            if total_chunks > 0 and len(existing_pdfs) > 0:
                logger.info("ì´ë¯¸ ì¶©ë¶„í•œ PDF ë°ì´í„°ê°€ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ìë™ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                logger.info("=" * 60)
                logger.info("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
                logger.info("=" * 60)
                return
            
        except Exception as e:
            logger.warning(f"ê¸°ì¡´ PDF ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ìë™ PDF ì—…ë¡œë“œ (ë°ì´í„°ê°€ ì—†ì„ ë•Œë§Œ ì‹¤í–‰)
        logger.info("=" * 60)
        logger.info("data í´ë”ì˜ PDF íŒŒì¼ë“¤ì„ ë²¡í„° ì €ì¥ì†Œì— ì—…ë¡œë“œí•©ë‹ˆë‹¤...")
        logger.info("=" * 60)
        auto_upload_result = auto_upload_pdfs_sync()
        logger.info(f"ìë™ ì—…ë¡œë“œ ì™„ë£Œ: {auto_upload_result}")
        logger.info("=" * 60)
        logger.info("PDF ì—…ë¡œë“œ ì™„ë£Œ!")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def auto_upload_pdfs_sync():
    """ë™ê¸°ì ìœ¼ë¡œ PDF íŒŒì¼ë“¤ì„ ìë™ ì—…ë¡œë“œ"""
    try:
        import os
        
        # data í´ë”ì™€ data/pdfs í´ë” ëª¨ë‘ í™•ì¸
        data_folders = ["./data", "./data/pdfs"]
        pdf_files = []
        
        for data_folder in data_folders:
            if not os.path.exists(data_folder):
                logger.warning(f"{data_folder} í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                continue
            
            # ì¬ê·€ì ìœ¼ë¡œ PDF íŒŒì¼ ì°¾ê¸°
            for root, dirs, files in os.walk(data_folder):
                for file in files:
                    if file.lower().endswith('.pdf'):
                        pdf_path = os.path.join(root, file)
                        pdf_files.append(pdf_path)
        
        if not pdf_files:
            logger.info("data í´ë”ì—ì„œ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"message": "ì—…ë¡œë“œí•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.", "uploaded_count": 0}
        
        logger.info(f"data í´ë”ì—ì„œ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        
        uploaded_count = 0
        skipped_count = 0
        failed_count = 0
        
        for pdf_path in pdf_files:
            try:
                # ì´ë¯¸ ì²˜ë¦¬ëœ PDFì¸ì§€ í™•ì¸
                pdf_id = os.path.basename(pdf_path)
                if pdf_id in pdf_metadata:
                    logger.info(f"ì´ë¯¸ ì²˜ë¦¬ëœ PDF ê±´ë„ˆë›°ê¸°: {pdf_id}")
                    skipped_count += 1
                    continue
                
                logger.info(f"PDF ì²˜ë¦¬ ì¤‘: {pdf_id}")
                
                # PDF ì²˜ë¦¬
                chunks, metadata = pdf_processor.process_pdf(pdf_path)
                vector_store.add_chunks(chunks)
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                pdf_metadata[pdf_id] = {
                    "filename": pdf_id,
                    "total_pages": len(chunks),
                    "upload_time": datetime.now().isoformat(),
                    "total_chunks": len(chunks),
                    "file_size": os.path.getsize(pdf_path)
                }
                
                uploaded_count += 1
                logger.info(f"PDF ì²˜ë¦¬ ì™„ë£Œ: {pdf_id} ({len(chunks)}ê°œ ì²­í¬)")
                
            except Exception as e:
                logger.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨ {pdf_path}: {e}")
                failed_count += 1
        
        logger.info(f"PDF ì²˜ë¦¬ ì™„ë£Œ: {uploaded_count}ê°œ ì²˜ë¦¬ë¨, {skipped_count}ê°œ ê±´ë„ˆëœ€, {failed_count}ê°œ ì˜¤ë¥˜")
        
        return {
            "message": f"ìë™ ì—…ë¡œë“œ ì™„ë£Œ: {uploaded_count}ê°œ ì„±ê³µ, {skipped_count}ê°œ ê±´ë„ˆëœ€, {failed_count}ê°œ ì‹¤íŒ¨",
            "uploaded_count": uploaded_count,
            "skipped_count": skipped_count,
            "failed_count": failed_count,
            "total_files": len(pdf_files)
        }
        
    except Exception as e:
        logger.error(f"ìë™ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

# API ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.get("/", response_model=Dict[str, str])
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {"message": "ë²”ìš© RAG ì‹œìŠ¤í…œ API ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."}

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.post("/clear-vector-store")
async def clear_vector_store(
    vector_store: VectorStoreInterface = Depends(get_vector_store)
):
    """ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”"""
    try:
        vector_store.clear()
        return {"message": "ë²¡í„° ì €ì¥ì†Œê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        logger.error(f"ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")

@app.post("/reset-chunks")
async def reset_chunks(
    vector_store: VectorStoreInterface = Depends(get_vector_store),
    pdf_processor: PDFProcessor = Depends(get_pdf_processor)
):
    """ì²­í¬ ì´ˆê¸°í™” ë° ì¬ìƒì„±"""
    try:
        logger.info("ì²­í¬ ì´ˆê¸°í™” ë° ì¬ìƒì„± ì‹œì‘...")
        
        # 1ë‹¨ê³„: ê¸°ì¡´ ì²­í¬ ì´ˆê¸°í™”
        total_chunks = vector_store.get_total_chunks()
        logger.info(f"ê¸°ì¡´ ì²­í¬ ìˆ˜: {total_chunks}")
        
        vector_store.clear()
        logger.info("ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 2ë‹¨ê³„: PDF íŒŒì¼ ìŠ¤ìº”
        pdf_files = _find_pdf_files()
        logger.info(f"ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
        
        if not pdf_files:
            return {"message": "PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. data/pdfs í´ë”ì— PDF íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”."}
        
        # 3ë‹¨ê³„: ì²­í¬ ì¬ìƒì„±
        total_new_chunks = 0
        
        for pdf_file in pdf_files:
            try:
                logger.info(f"ì²˜ë¦¬ ì¤‘: {pdf_file}")
                
                # PDF ì²˜ë¦¬
                chunks = pdf_processor.process_pdf(pdf_file)
                
                if chunks:
                    # ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
                    vector_store.add_chunks(chunks)
                    total_new_chunks += len(chunks)
                    logger.info(f"{len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
                else:
                    logger.warning(f"ì²­í¬ ìƒì„± ì‹¤íŒ¨: {pdf_file}")
                    
            except Exception as e:
                logger.error(f"PDF ì²˜ë¦¬ ì˜¤ë¥˜ {pdf_file}: {e}")
                continue
        
        logger.info(f"ì²­í¬ ì¬ìƒì„± ì™„ë£Œ! ì´ {total_new_chunks}ê°œ ì²­í¬ ìƒì„±")
        
        return {
            "message": "ì²­í¬ ì´ˆê¸°í™” ë° ì¬ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "total_new_chunks": total_new_chunks,
            "total_chunks": vector_store.get_total_chunks()
        }
        
    except Exception as e:
        logger.error(f"ì²­í¬ ì´ˆê¸°í™” ë° ì¬ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì²­í¬ ì´ˆê¸°í™” ë° ì¬ìƒì„± ì‹¤íŒ¨: {str(e)}")

def _find_pdf_files() -> List[str]:
    """PDF íŒŒì¼ë“¤ ì°¾ê¸°"""
    pdf_files = []
    
    # data/pdfs í´ë” í™•ì¸
    pdf_dir = Path(__file__).parent.parent / "data" / "pdfs"
    if pdf_dir.exists():
        pdf_files.extend([str(f) for f in pdf_dir.glob("*.pdf")])
    
    # data í´ë” ì§ì ‘ í™•ì¸
    data_dir = Path(__file__).parent.parent / "data"
    if data_dir.exists():
        pdf_files.extend([str(f) for f in data_dir.glob("*.pdf")])
    
    return pdf_files

@app.get("/vector-store-stats")
async def get_vector_store_stats(
    vector_store: VectorStoreInterface = Depends(get_vector_store)
):
    """ë²¡í„° ì €ì¥ì†Œ í†µê³„ ì •ë³´"""
    try:
        total_chunks = vector_store.get_total_chunks()
        pdfs = vector_store.get_all_pdfs()
        
        return {
            "total_chunks": total_chunks,
            "total_pdfs": len(pdfs),
            "pdfs": pdfs
        }
    except Exception as e:
        logger.error(f"ë²¡í„° ì €ì¥ì†Œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.post("/upload-pdf", response_model=PDFUploadResponse)
async def upload_pdf(
    file: UploadFile = File(...),
    pdf_processor: PDFProcessor = Depends(get_pdf_processor),
    vector_store: VectorStoreInterface = Depends(get_vector_store)
):
    """PDF ì—…ë¡œë“œ ë° ì²˜ë¦¬"""
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(status_code=400, detail="PDF íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    try:
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        # PDF ì²˜ë¦¬
        start_time = datetime.now()
        pdf_id = str(uuid.uuid4())
        
        # PDF ì²˜ë¦¬ ë° ì²­í¬ ìƒì„±
        chunks, metadata = pdf_processor.process_pdf(temp_file_path, pdf_id)
        
        # ë²¡í„° ì €ì¥ì†Œì— ì €ì¥
        vector_store.add_chunks(chunks)
        
        # í‚¤ì›Œë“œë¥¼ íŒŒì´í”„ë¼ì¸ ì„¤ì •ì— ì¶”ê°€
        if pdf_processor.enable_keyword_extraction:
            pdf_processor.keyword_extractor.add_keywords_to_pipeline()
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        pdf_metadata[pdf_id] = {
            "filename": file.filename,
            "total_pages": len(chunks),
            "upload_time": datetime.now().isoformat(),
            "file_size": len(content)
        }
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(temp_file_path)
        
        return PDFUploadResponse(
            pdf_id=pdf_id,
            filename=file.filename,
            total_pages=len(chunks),
            total_chunks=len(chunks),
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/ask", response_model=QuestionResponse)
async def ask_question(
    request: QuestionRequest,
    vector_store: VectorStoreInterface = Depends(get_vector_store),
    question_analyzer: QuestionAnalyzer = Depends(get_question_analyzer),
    answer_generator: AnswerGenerator = Depends(get_answer_generator),
    sql_generator: SQLGenerator = Depends(get_sql_generator),
    query_router: QueryRouter = Depends(get_query_router)
):
    """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (ìµœì í™”ëœ ë²„ì „)"""
    
    # ì„±ëŠ¥ ì¸¡ì • ì‹œì‘
    start_time = time.time()
    session_id = None
    
    try:
        # ë‹¨ê³„ë³„ ë¡œê·¸ ì‹œì‘
        if chatbot_logger:
            session_id = chatbot_logger._generate_session_id()
            chatbot_logger.log_step(session_id, ProcessingStep.START, 0.0, f"ì§ˆë¬¸: {request.question[:50]}...")
        
        # SBERT ê¸°ë°˜ ì¿¼ë¦¬ ë¼ìš°íŒ…
        routing_start = time.time()
        route_result = query_router.route_query(request.question)
        routing_time = time.time() - routing_start
        
        if chatbot_logger and session_id:
            chatbot_logger.log_step(
                session_id, 
                ProcessingStep.SBERT_ROUTING, 
                routing_time, 
                f"ë¼ìš°íŒ…ê²°ê³¼: {route_result.route.value} (ì‹ ë¢°ë„: {route_result.confidence:.3f})"
            )
        
        logger.info(f"ğŸ“ ë¼ìš°íŒ… ê²°ê³¼: {route_result.route.value} (ì‹ ë¢°ë„: {route_result.confidence:.3f})")
        
        # ì¸ì‚¬ë§ ì²˜ë¦¬ (LLM ê¸°ë°˜)
        if route_result.route == QueryRoute.GREETING:
            if chatbot_logger and session_id:
                chatbot_logger.log_step(session_id, ProcessingStep.GREETING_PIPELINE, 0.0, "ì¸ì‚¬ë§ ì²˜ë¦¬ ì‹œì‘")
            
            try:
                llm_greeting_handler = get_llm_greeting_handler()
                greeting_response = llm_greeting_handler.get_greeting_response(request.question)
                
                # ì¸ì‚¬ë§ ë¡œê¹…
                try:
                    if chatbot_logger:
                        chatbot_logger.log_greeting(
                            user_question=request.question,
                            greeting_response=greeting_response["answer"],
                            processing_time=greeting_response["generation_time"],
                            confidence_score=greeting_response["confidence_score"],
                            greeting_type=greeting_response.get("greeting_type", "general")
                        )
                except Exception as log_error:
                    logger.warning(f"ì¸ì‚¬ë§ ë¡œê¹… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {log_error}")
                
                if chatbot_logger and session_id:
                    chatbot_logger.log_step(session_id, ProcessingStep.COMPLETION, greeting_response["generation_time"], "ì¸ì‚¬ë§ ì²˜ë¦¬ ì™„ë£Œ")
                
                return QuestionResponse(
                    answer=greeting_response["answer"],
                    confidence_score=greeting_response["confidence_score"],
                    used_chunks=[],
                    generation_time=greeting_response["generation_time"],
                    question_type="greeting",
                    llm_model_name=f"llm_greeting_{greeting_response.get('method', 'unknown')}",
                    pipeline_type="greeting",
                    sql_query=None
                )
            except Exception as greeting_error:
                logger.error(f"ì¸ì‚¬ë§ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {greeting_error}")
                # ê¸°ë³¸ ì¸ì‚¬ë§ (í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì • ê°€ëŠ¥)
                fallback_greeting = os.getenv("DEFAULT_GREETING", "ì•ˆë…•í•˜ì„¸ìš”! ë²”ìš© RAG ì‹œìŠ¤í…œì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! ğŸ¤–")
                
                if chatbot_logger and session_id:
                    chatbot_logger.log_step(session_id, ProcessingStep.ERROR, 0.0, f"ì¸ì‚¬ë§ ì²˜ë¦¬ ì˜¤ë¥˜: {greeting_error}")
                
                return QuestionResponse(
                    answer=fallback_greeting,
                    confidence_score=0.8,
                    used_chunks=[],
                    generation_time=0.001,
                    question_type="greeting",
                    llm_model_name="fallback_greeting",
                    pipeline_type="greeting",
                    sql_query=None
                )
        
        # SQL ì¿¼ë¦¬ ì²˜ë¦¬ (SQL ìƒì„± â†’ DB ì‹¤í–‰ â†’ LLM ìš”ì•½)
        if route_result.route == QueryRoute.SQL_QUERY:
            if chatbot_logger and session_id:
                chatbot_logger.log_step(session_id, ProcessingStep.SQL_PIPELINE, 0.0, "SQL íŒŒì´í”„ë¼ì¸ ì‹œì‘")
            
            try:
                # ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ì •ì˜
                schema = DatabaseSchema(
                    table_name="traffic_intersection",
                    columns=[
                        {"name": "id", "type": "INTEGER", "description": "êµì°¨ë¡œ ID"},
                        {"name": "name", "type": "TEXT", "description": "êµì°¨ë¡œ ì´ë¦„"},
                        {"name": "location", "type": "TEXT", "description": "ìœ„ì¹˜"},
                        {"name": "traffic_volume", "type": "INTEGER", "description": "êµí†µëŸ‰"},
                        {"name": "district", "type": "TEXT", "description": "êµ¬ì—­"}
                    ]
                )
                
                # ê·œì¹™ ê¸°ë°˜ SQL ìƒì„±
                sql_gen_start = time.time()
                sql_result = sql_generator.generate_sql(request.question, schema)
                sql_gen_time = time.time() - sql_gen_start
                
                if chatbot_logger and session_id:
                    chatbot_logger.log_step(session_id, ProcessingStep.SQL_GENERATION, sql_gen_time, f"SQLìƒì„±: {sql_result.query[:50]}...")
                
                if sql_result.is_valid:
                    # SQL ì‹¤í–‰
                    sql_exec_start = time.time()
                    execution_result = sql_generator.execute_sql(sql_result)
                    sql_exec_time = time.time() - sql_exec_start
                    
                    if chatbot_logger and session_id:
                        chatbot_logger.log_step(session_id, ProcessingStep.DATABASE_EXECUTION, sql_exec_time, f"DBì‹¤í–‰: {execution_result.get('row_count', 0)}í–‰ ë°˜í™˜")
                    
                    if execution_result['success']:
                        # LLMìœ¼ë¡œ ê²°ê³¼ ìš”ì•½ ìƒì„±
                        answer_gen_start = time.time()
                        rows = execution_result.get('data') or []
                        answer_from_sql = answer_generator.generate_from_sql_results(request.question, rows)
                        answer_gen_time = time.time() - answer_gen_start
                        
                        if chatbot_logger and session_id:
                            chatbot_logger.log_step(session_id, ProcessingStep.ANSWER_GENERATION, answer_gen_time, "SQLê²°ê³¼ ë‹µë³€ìƒì„±")
                        
                        # SQL ì§ˆë¬¸ ë¡œê¹…
                        try:
                            if chatbot_logger:
                                intent = "SQL_QUERY"
                                keywords = request.question.split()[:5]
                                
                                chatbot_logger.log_question(
                                    user_question=request.question,
                                    question_type=QuestionType.SQL,
                                    intent=intent,
                                    keywords=keywords,
                                    processing_time=answer_from_sql.generation_time,
                                    confidence_score=answer_from_sql.confidence_score,
                                    generated_sql=sql_result.query,
                                    generated_answer=answer_from_sql.content,
                                    model_name=sql_result.model_name,
                                    additional_info={
                                        "pipeline_type": "sql",
                                        "execution_success": True,
                                        "row_count": execution_result.get('row_count', 0)
                                    }
                                )
                        except Exception as log_error:
                            logger.warning(f"SQL ë¡œê¹… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {log_error}")
                        
                        if chatbot_logger and session_id:
                            total_time = time.time() - start_time
                            chatbot_logger.log_step(session_id, ProcessingStep.COMPLETION, total_time, "SQL íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
                        
                        return QuestionResponse(
                            answer=answer_from_sql.content,
                            confidence_score=answer_from_sql.confidence_score,
                            used_chunks=[],
                            generation_time=answer_from_sql.generation_time,
                            question_type="sql_query",
                            llm_model_name=answer_from_sql.model_name,
                            pipeline_type="sql",
                            sql_query=sql_result.query
                        )
                    else:
                        logger.warning(f"SQL ì‹¤í–‰ ì‹¤íŒ¨: {execution_result.get('error')}")
                        if chatbot_logger and session_id:
                            chatbot_logger.log_step(session_id, ProcessingStep.ERROR, 0.0, f"SQLì‹¤í–‰ì‹¤íŒ¨: {execution_result.get('error')}")
                        # PDF ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
                        pass
                else:
                    logger.warning(f"SQL ê²€ì¦ ì‹¤íŒ¨: {sql_result.error_message}")
                    if chatbot_logger and session_id:
                        chatbot_logger.log_step(session_id, ProcessingStep.ERROR, 0.0, f"SQLê²€ì¦ì‹¤íŒ¨: {sql_result.error_message}")
                    # PDF ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
                    pass
            except Exception as sql_error:
                logger.warning(f"SQL ì²˜ë¦¬ ì‹¤íŒ¨, PDF ê²€ìƒ‰ìœ¼ë¡œ í´ë°±: {sql_error}")
                if chatbot_logger and session_id:
                    chatbot_logger.log_step(session_id, ProcessingStep.ERROR, 0.0, f"SQLì²˜ë¦¬ì‹¤íŒ¨: {sql_error}")
                # PDF ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
                pass
        
        # PDF ê²€ìƒ‰ ì²˜ë¦¬ (ê¸°ë³¸ ëª¨ë“œ)
        logger.info("ğŸ“„ PDF ê²€ìƒ‰ ëª¨ë“œë¡œ ì²˜ë¦¬")
        
        if chatbot_logger and session_id:
            chatbot_logger.log_step(session_id, ProcessingStep.PDF_PIPELINE, 0.0, "PDF íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        # 1. ì§ˆë¬¸ ë¶„ì„
        analysis_start = time.time()
        analyzed_question = question_analyzer.analyze_question(
            request.question,
            use_conversation_context=request.use_conversation_context
        )
        analysis_time = time.time() - analysis_start
        
        if chatbot_logger and session_id:
            chatbot_logger.log_step(session_id, ProcessingStep.QUESTION_ANALYSIS, analysis_time, f"ì§ˆë¬¸ë¶„ì„: {analyzed_question.question_type.value}")
        
        # 2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (ìœ ì‚¬ë„ ì„ê³„ê°’ ì ìš©)
        search_start = time.time()
        query_embedding = analyzed_question.embedding
        
        # ë²¡í„° ìŠ¤í† ì–´ ìƒíƒœ í™•ì¸
        total_chunks = vector_store.get_total_chunks()
        logger.info(f"ë²¡í„° ìŠ¤í† ì–´ ìƒíƒœ - ì´ ì²­í¬ ìˆ˜: {total_chunks}")
        
        if total_chunks == 0:
            logger.error("âš ï¸ ë²¡í„° ìŠ¤í† ì–´ì— ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤! PDF ì—…ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return QuestionResponse(
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë¬¸ì„œ ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.",
                confidence_score=0.0,
                used_chunks=[],
                generation_time=0.001,
                question_type="error",
                llm_model_name="none",
                pipeline_type="error",
                sql_query=None
            )
        
        # ê³¼ë„í•œ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©ì„ í”¼í•˜ê¸° ìœ„í•´ ìƒí•œê³¼ ì„ê³„ê°’ì„ ì ìš©
        effective_top_k = max(3, min(request.max_chunks, 5))
        relevant_chunks = vector_store.search(
            query_embedding,
            top_k=effective_top_k,
            similarity_threshold=0.2  # ê°€ë…ì„± í–¥ìƒì„ ìœ„í•´ ì„ê³„ê°’ ìƒí–¥
        )
        search_time = time.time() - search_start
        
        if chatbot_logger and session_id:
            chatbot_logger.log_step(session_id, ProcessingStep.VECTOR_SEARCH, search_time, f"ë²¡í„°ê²€ìƒ‰: {len(relevant_chunks)}ê°œ ì²­í¬ ë°œê²¬")
        
        # ë””ë²„ê¹…: ê²€ìƒ‰ ê²°ê³¼ ë¡œê¹…
        logger.info(f"ğŸ” ê²€ìƒ‰ëœ ì²­í¬ ìˆ˜: {len(relevant_chunks)}")
        for i, (chunk, score) in enumerate(relevant_chunks[:3]):
            logger.info(f"  ğŸ“„ ì²­í¬ {i+1}: {chunk.chunk_id} (ìœ ì‚¬ë„: {score:.3f})")
            logger.info(f"    ë‚´ìš©: {chunk.content[:150]}...")
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ê²½ê³ 
        if not relevant_chunks:
            logger.warning("âš ï¸ ê²€ìƒ‰ëœ ê´€ë ¨ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤!")
        else:
            logger.info(f"âœ… {len(relevant_chunks)}ê°œì˜ ê´€ë ¨ ì²­í¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        # 3. ì»¨í…ìŠ¤íŠ¸ ê²€ì¦ ë° ë‹µë³€ ìƒì„±
        if not relevant_chunks:
            logger.warning("ğŸ” ê²€ìƒ‰ëœ ê´€ë ¨ ì²­í¬ê°€ ì—†ì–´ LLM ì§ì ‘ ë‹µë³€ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            answer_gen_start = time.time()
            answer = answer_generator.generate_direct_answer(request.question)
            answer_gen_time = time.time() - answer_gen_start
            
            if chatbot_logger and session_id:
                chatbot_logger.log_step(session_id, ProcessingStep.ANSWER_GENERATION, answer_gen_time, "ì§ì ‘ë‹µë³€ìƒì„± (ì²­í¬ì—†ìŒ)")
        else:
            # ì»¨í…ìŠ¤íŠ¸ ë‚´ìš© ë¡œê¹…
            context_content = "\n".join([chunk.content[:100] + "..." for chunk, _ in relevant_chunks[:3]])
            logger.info(f"ğŸ“„ ì»¨í…ìŠ¤íŠ¸ ë‚´ìš© (ì¼ë¶€):\n{context_content}")
            
            answer_gen_start = time.time()
            answer = answer_generator.generate_answer(
                analyzed_question,
                relevant_chunks,
                conversation_history=None,
                pdf_id=request.pdf_id
            )
            answer_gen_time = time.time() - answer_gen_start
            
            if chatbot_logger and session_id:
                chatbot_logger.log_step(session_id, ProcessingStep.ANSWER_GENERATION, answer_gen_time, f"ì»¨í…ìŠ¤íŠ¸ë‹µë³€ìƒì„±: {len(relevant_chunks)}ê°œ ì²­í¬ ì‚¬ìš©")
        
                # 4. ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        question_analyzer.add_conversation_item(
            question=request.question,
            answer=answer.content,
            used_chunks=answer.used_chunks,
            confidence_score=answer.confidence_score
        )
        
        # 5. API ë¡œê¹…
        try:
            if chatbot_logger:
                # ì§ˆë¬¸ ì˜ë„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
                intent = "PDF_QUERY"
                keywords = request.question.split()[:5]  # ì²« 5ê°œ ë‹¨ì–´ë¥¼ í‚¤ì›Œë“œë¡œ ì‚¬ìš©
                
                chatbot_logger.log_question(
                    user_question=request.question,
                    question_type=QuestionType.PDF,
                    intent=intent,
                    keywords=keywords,
                    processing_time=answer.generation_time,
                    confidence_score=answer.confidence_score,
                    generated_answer=answer.content,
                    used_chunks=answer.used_chunks,
                    model_name=answer.model_name,
                    additional_info={
                        "pipeline_type": route_result.route.value,
                        "user_id": request.user_id
                    }
                )
        except Exception as log_error:
            logger.warning(f"API ë¡œê¹… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {log_error}")
        
        # ì™„ë£Œ ë¡œê·¸
        if chatbot_logger and session_id:
            total_time = time.time() - start_time
            chatbot_logger.log_step(session_id, ProcessingStep.COMPLETION, total_time, "PDF íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        
        return QuestionResponse(
            answer=answer.content,
            confidence_score=answer.confidence_score,
            used_chunks=answer.used_chunks,
            generation_time=answer.generation_time,
            question_type=analyzed_question.question_type.value,
            llm_model_name=answer.model_name,
            pipeline_type=route_result.route.value,
            sql_query=None
        )
        
    except Exception as e:
        logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        
        # ì—ëŸ¬ ë¡œê¹…
        try:
            if chatbot_logger:
                if session_id:
                    chatbot_logger.log_step(session_id, ProcessingStep.ERROR, 0.0, f"ì „ì²´ì²˜ë¦¬ì˜¤ë¥˜: {str(e)}")
                
                chatbot_logger.log_error(
                    user_question=request.question,
                    error_message=str(e),
                    question_type=QuestionType.UNKNOWN
                )
        except Exception as log_error:
            logger.warning(f"ì—ëŸ¬ ë¡œê¹… ì‹¤íŒ¨: {log_error}")
        
        # ì‚¬ìš©ì ì¹œí™”ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€
        error_message = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        raise HTTPException(status_code=500, detail=error_message)

@app.get("/status", response_model=SystemStatusResponse)
async def get_system_status():
    """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    try:
        import psutil
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory = psutil.virtual_memory()
        memory_usage = {
            "total": memory.total,
            "available": memory.available,
            "percent": memory.percent,
            "used": memory.used
        }
        
        # ëª¨ë¸ ë¡œë“œ ìƒíƒœ
        model_loaded = (
            answer_generator is not None and 
            question_analyzer is not None and 
            vector_store is not None
        )
        
        # PDF ë° ì²­í¬ ìˆ˜
        total_pdfs = len(pdf_metadata)
        # ë²¡í„° ì €ì¥ì†Œì—ì„œ ì²­í¬ ìˆ˜ í™•ì¸ (get_all_chunks ë©”ì„œë“œê°€ ì—†ìœ¼ë¯€ë¡œ ë‹¤ë¥¸ ë°©ë²• ì‚¬ìš©)
        total_chunks = 0
        if vector_store and hasattr(vector_store, 'chunks'):
            total_chunks = len(vector_store.chunks)
        elif vector_store and hasattr(vector_store, 'faiss_store') and hasattr(vector_store.faiss_store, 'chunks'):
            total_chunks = len(vector_store.faiss_store.chunks)
        
        return SystemStatusResponse(
            status="running",
            llm_model_loaded=model_loaded,
            total_pdfs=total_pdfs,
            total_chunks=total_chunks,
            memory_usage=memory_usage
        )
        
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/router/stats")
async def get_router_stats(
    query_router: QueryRouter = Depends(get_query_router)
):
    """ì¿¼ë¦¬ ë¼ìš°í„° í†µê³„"""
    try:
        stats = query_router.get_route_statistics()
        return {
            "status": "success",
            "router_stats": stats
        }
    except Exception as e:
        logger.error(f"ë¼ìš°í„° í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë¼ìš°í„° í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.post("/router/test")
async def test_routing(
    question: str,
    query_router: QueryRouter = Depends(get_query_router)
):
    """ë¼ìš°íŒ… í…ŒìŠ¤íŠ¸"""
    try:
        result = query_router.route_query(question)
        return {
            "question": question,
            "route": result.route.value,
            "confidence": result.confidence,
            "reasoning": result.reasoning,
            "metadata": result.metadata
        }
    except Exception as e:
        logger.error(f"ë¼ìš°íŒ… í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë¼ìš°íŒ… í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

@app.get("/pdfs")
async def get_pdf_list():
    """ë“±ë¡ëœ PDF ëª©ë¡ ì¡°íšŒ"""
    try:
        pdfs = []
        for pdf_id, metadata in pdf_metadata.items():
            pdfs.append({
                "pdf_id": pdf_id,
                "filename": metadata.get("filename", "Unknown"),
                "upload_time": metadata.get("upload_time", ""),
                "total_pages": metadata.get("total_pages", 0),
                "total_chunks": metadata.get("total_chunks", 0),
                "file_size": metadata.get("file_size", 0)
            })
        
        return {"pdfs": pdfs}
        
    except Exception as e:
        logger.error(f"PDF ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"PDF ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.post("/auto-upload")
async def auto_upload_pdfs():
    """data í´ë”ì˜ PDF íŒŒì¼ë“¤ì„ ìë™ìœ¼ë¡œ ì—…ë¡œë“œ"""
    try:
        import os
        from pathlib import Path
        
        # data í´ë”ì™€ data/pdfs í´ë” ëª¨ë‘ í™•ì¸
        data_folders = ["./data", "./data/pdfs"]
        pdf_files = []
        
        for data_folder in data_folders:
            if not os.path.exists(data_folder):
                logger.warning(f"{data_folder} í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                continue
            
            # ì¬ê·€ì ìœ¼ë¡œ PDF íŒŒì¼ ì°¾ê¸°
            for root, dirs, files in os.walk(data_folder):
                for file in files:
                    if file.lower().endswith('.pdf'):
                        pdf_path = os.path.join(root, file)
                        pdf_files.append(pdf_path)
        
        if not pdf_files:
            return {"message": "ì—…ë¡œë“œí•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.", "uploaded_count": 0}
        
        logger.info(f"ìë™ ì—…ë¡œë“œ ì‹œì‘: {len(pdf_files)}ê°œì˜ PDF íŒŒì¼")
        
        uploaded_count = 0
        failed_count = 0
        
        for pdf_path in pdf_files:
            try:
                # ì´ë¯¸ ì²˜ë¦¬ëœ PDFì¸ì§€ í™•ì¸
                pdf_id = os.path.basename(pdf_path)
                if pdf_id in pdf_metadata:
                    logger.info(f"ì´ë¯¸ ì²˜ë¦¬ëœ PDF ê±´ë„ˆë›°ê¸°: {pdf_id}")
                    continue
                
                # PDF ì²˜ë¦¬
                chunks, metadata = pdf_processor.process_pdf(pdf_path)
                vector_store.add_chunks(chunks)
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                pdf_metadata[pdf_id] = {
                    "filename": pdf_id,
                    "total_pages": len(chunks),
                    "upload_time": datetime.now().isoformat(),
                    "total_chunks": len(chunks),
                    "file_size": os.path.getsize(pdf_path)
                }
                
                uploaded_count += 1
                logger.info(f"PDF ìë™ ì—…ë¡œë“œ ì™„ë£Œ: {pdf_id} ({len(chunks)}ê°œ ì²­í¬)")
                
            except Exception as e:
                logger.error(f"PDF ìë™ ì—…ë¡œë“œ ì‹¤íŒ¨ {pdf_path}: {e}")
                failed_count += 1
        
        return {
            "message": f"ìë™ ì—…ë¡œë“œ ì™„ë£Œ: {uploaded_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨",
            "uploaded_count": uploaded_count,
            "failed_count": failed_count,
            "total_files": len(pdf_files)
        }
        
    except Exception as e:
        logger.error(f"ìë™ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ìë™ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/greeting/statistics")
async def get_greeting_statistics():
    """ì¸ì‚¬ë§ ì²˜ë¦¬ í†µê³„ í™•ì¸"""
    try:
        llm_greeting_handler = get_llm_greeting_handler()
        stats = llm_greeting_handler.get_statistics()
        return {
            "status": "success",
            "data": stats
        }
    except Exception as e:
        logger.error(f"ì¸ì‚¬ë§ í†µê³„ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "status": "error",
            "message": str(e)
        }

@app.get("/api/company/info")
async def get_company_info():
    """íšŒì‚¬ ì •ë³´ ì¡°íšŒ"""
    try:
        from core.config.company_config import CompanyConfig
        company_config = CompanyConfig()
        company_info = company_config.get_company_info()
        return {
            "status": "success",
            "data": company_info
        }
    except Exception as e:
        logger.error(f"íšŒì‚¬ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "status": "error",
            "message": str(e)
        }

@app.get("/api/keywords/cache/stats", response_model=KeywordCacheResponse)
async def get_keyword_cache_stats():
    """í‚¤ì›Œë“œ ìºì‹œ í†µê³„ ì¡°íšŒ"""
    try:
        pdf_processor = get_pdf_processor()
        stats = pdf_processor.get_keyword_cache_stats()
        
        if "error" in stats:
            raise HTTPException(status_code=400, detail=stats["error"])
        
        return KeywordCacheResponse(**stats)
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ìºì‹œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/keywords/pipeline/add", response_model=KeywordPipelineResponse)
async def add_keywords_to_pipeline():
    """ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ íŒŒì´í”„ë¼ì¸ ì„¤ì •ì— ì¶”ê°€"""
    try:
        pdf_processor = get_pdf_processor()
        pdf_processor.add_keywords_to_pipeline()
        
        # ì¶”ê°€ëœ í‚¤ì›Œë“œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        stats = pdf_processor.get_keyword_cache_stats()
        frequent_keywords = pdf_processor.keyword_extractor.get_frequent_keywords()
        
        return KeywordPipelineResponse(
            success=True,
            added_keywords=frequent_keywords[:20],  # ìµœëŒ€ 20ê°œ
            message=f"íŒŒì´í”„ë¼ì¸ì— {len(frequent_keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ê°€ ì™„ë£Œ"
        )
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ íŒŒì´í”„ë¼ì¸ ì¶”ê°€ ì‹¤íŒ¨: {e}")
        return KeywordPipelineResponse(
            success=False,
            added_keywords=[],
            message=str(e)
        )

@app.delete("/api/keywords/cache/clear")
async def clear_keyword_cache():
    """í‚¤ì›Œë“œ ìºì‹œ ì´ˆê¸°í™”"""
    try:
        pdf_processor = get_pdf_processor()
        pdf_processor.clear_keyword_cache()
        return {"message": "í‚¤ì›Œë“œ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ"}
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/keywords/cache/save")
async def save_keyword_cache():
    """í‚¤ì›Œë“œ ìºì‹œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    try:
        pdf_processor = get_pdf_processor()
        pdf_processor.save_keyword_cache()
        return {"message": "í‚¤ì›Œë“œ ìºì‹œ ì €ì¥ ì™„ë£Œ"}
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/keywords/cache/load")
async def load_keyword_cache():
    """íŒŒì¼ì—ì„œ í‚¤ì›Œë“œ ìºì‹œ ë¡œë“œ"""
    try:
        pdf_processor = get_pdf_processor()
        pdf_processor.load_keyword_cache()
        return {"message": "í‚¤ì›Œë“œ ìºì‹œ ë¡œë“œ ì™„ë£Œ"}
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@app.get("/memory/status")
async def get_memory_status():
    """ë©”ëª¨ë¦¬ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
    try:
        from core.utils.memory_optimizer import memory_optimizer, model_memory_manager
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´
        memory_info = memory_optimizer.get_memory_info()
        
        # ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë³´
        model_status = model_memory_manager.get_model_status()
        
        return {
            "system_memory": {
                "total_gb": memory_info.total,
                "available_gb": memory_info.available,
                "used_gb": memory_info.used,
                "percent": memory_info.percent,
                "process_memory_gb": memory_info.process_memory
            },
            "model_memory": model_status,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

@app.post("/memory/optimize")
async def optimize_memory():
    """ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰"""
    try:
        from core.utils.memory_optimizer import memory_optimizer
        
        # ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰
        after_info = memory_optimizer.optimize_memory(aggressive=True)
        
        return {
            "message": "ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ",
            "optimized_memory_gb": after_info.used,
            "memory_percent": after_info.percent,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

@app.get("/memory/models")
async def get_loaded_models():
    """ë¡œë“œëœ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    try:
        from core.utils.memory_optimizer import model_memory_manager
        
        model_status = model_memory_manager.get_model_status()
        
        return {
            "loaded_models": model_status["loaded_models"],
            "total_memory_gb": model_status["total_memory_gb"],
            "max_memory_gb": model_status["max_memory_gb"],
            "model_details": model_status["model_details"],
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

@app.delete("/memory/models/{model_name}")
async def unload_model(model_name: str):
    """íŠ¹ì • ëª¨ë¸ ì–¸ë¡œë“œ"""
    try:
        from core.utils.memory_optimizer import model_memory_manager
        
        model_memory_manager.unload_model(model_name)
        
        return {
            "message": f"ëª¨ë¸ {model_name} ì–¸ë¡œë“œ ì™„ë£Œ",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

if __name__ == "__main__":
    # ì„œë²„ ì‹œì‘ ì „ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    initialize_system()
    uvicorn.run(app, host="0.0.0.0", port=8008)
