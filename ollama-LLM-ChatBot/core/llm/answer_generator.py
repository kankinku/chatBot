"""
KoAlpaca ê¸°ë°˜ ë‹µë³€ ìƒì„± ëª¨ë“ˆ

ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•œ ìµœì í™”ëœ ëª¨ë“ˆ
"""

import time
import logging
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import os
import json

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

# ë¡œì»¬ LLM ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì§€ì—° importë¡œ ìµœì í™”)
TRANSFORMERS_AVAILABLE = False
try:
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    logging.warning("PyTorchë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ìƒëŒ€ ê²½ë¡œ import (í•„ìš”ì‹œì—ë§Œ ë¡œë”©)
from core.document.pdf_processor import TextChunk
from core.query.question_analyzer import AnalyzedQuestion
from core.cache.fast_cache import get_question_cache
from core.config.company_config import CompanyConfig
from core.utils.memory_optimizer import model_memory_manager, memory_profiler

logger = logging.getLogger(__name__)

class ModelType(Enum):
    """ëª¨ë¸ íƒ€ì… ì—´ê±°í˜•"""
    LOCAL = "local"
    HUGGINGFACE = "huggingface"
    OLLAMA = "ollama"

@dataclass
class GenerationConfig:
    """ê²½ëŸ‰í™”ëœ ìƒì„± ì„¤ì •"""
    max_length: int = 1024
    temperature: float = 0.7
    top_p: float = 0.9
    do_sample: bool = True
    num_beams: int = 3
    early_stopping: bool = True

@dataclass
class Answer:
    """ë‹µë³€ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    confidence: float
    sources: List[TextChunk]
    generation_time: float
    model_name: str
    metadata: Optional[Dict] = None

class ModelManager:
    """ì‹±ê¸€í†¤ íŒ¨í„´ì„ ì‚¬ìš©í•œ ëª¨ë¸ ê´€ë¦¬ì (ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©)"""
    _instance = None
    _models = {}
    _tokenizers = {}
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def get_model(self, model_name: str, model_type: str = "seq2seq"):
        """ì§€ì—° ë¡œë”©ì„ í†µí•œ ëª¨ë¸ íšë“ (ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € í†µí•©)"""
        # ë¨¼ì € ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì—ì„œ í™•ì¸
        model_obj = model_memory_manager.get_model(model_name)
        if model_obj is not None:
            # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶„ë¦¬ (model_objëŠ” íŠœí”Œë¡œ ì €ì¥ë¨)
            if isinstance(model_obj, tuple) and len(model_obj) == 2:
                return model_obj[0], model_obj[1]
            return model_obj, self._tokenizers.get(model_name)
        
        # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì— ì—†ìœ¼ë©´ ìƒˆë¡œ ë¡œë“œ
        logger.info(f"ëª¨ë¸ ì§€ì—° ë¡œë”© ì¤‘: {model_name}")
        return self._load_model(model_name, model_type)
    
    def _load_model(self, model_name: str, model_type: str):
        """ì‹¤ì œ ëª¨ë¸ ë¡œë”© ë¡œì§ (ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € í†µí•©) - GPU/CPU ì§€ì›"""
        try:
            with memory_profiler(f"ëª¨ë¸ ë¡œë”©: {model_name}"):
                # í•„ìš”ì‹œì—ë§Œ transformers import
                from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM
                
                # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                logger.info(f"[DEVICE] ëª¨ë¸ ë¡œë”© ë””ë°”ì´ìŠ¤: {device}")
                
                # í† í¬ë‚˜ì´ì € ë¡œë“œ (ìºì‹œ ë””ë ‰í† ë¦¬ ìµœì í™”)
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    cache_dir="./models",
                    use_fast=True,  # ë¹ ë¥¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©
                    trust_remote_code=True
                )
                
                # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ë¡œë”© (GPU/CPU ì§€ì›)
                if model_type == "causal":
                    if device.type == "cuda":
                        model = AutoModelForCausalLM.from_pretrained(
                            model_name,
                            torch_dtype=torch.float16,  # GPU ë©”ëª¨ë¦¬ ì ˆì•½
                            cache_dir="./models",
                            low_cpu_mem_usage=True,
                            device_map="auto",  # ìë™ GPU ë§¤í•‘
                            trust_remote_code=True
                        )
                    else:
                        model = AutoModelForCausalLM.from_pretrained(
                            model_name,
                            torch_dtype=torch.float32,  # CPUëŠ” float32 ì‚¬ìš©
                            cache_dir="./models",
                            low_cpu_mem_usage=True,
                            trust_remote_code=True
                        )
                elif model_type == "seq2seq":
                    if device.type == "cuda":
                        model = AutoModelForSeq2SeqLM.from_pretrained(
                            model_name,
                            torch_dtype=torch.float16,  # GPU ë©”ëª¨ë¦¬ ì ˆì•½
                            cache_dir="./models",
                            low_cpu_mem_usage=True,
                            device_map="auto",  # ìë™ GPU ë§¤í•‘
                            trust_remote_code=True
                        )
                    else:
                        model = AutoModelForSeq2SeqLM.from_pretrained(
                            model_name,
                            torch_dtype=torch.float32,  # CPUëŠ” float32 ì‚¬ìš©
                            cache_dir="./models",
                            low_cpu_mem_usage=True,
                            trust_remote_code=True
                        )
                
                # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì— ë“±ë¡
                model_tuple = (model, tokenizer)
                estimated_memory = self._estimate_model_memory(model_name)
                model_memory_manager.register_model(model_name, model_tuple, estimated_memory)
                
                # ë¡œì»¬ ìºì‹œì—ë„ ì €ì¥ (í˜¸í™˜ì„±)
                self._models[model_name] = model
                self._tokenizers[model_name] = tokenizer
                
                logger.info(f"ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name} (ì˜ˆìƒ ë©”ëª¨ë¦¬: {estimated_memory:.2f}GB)")
                return model, tokenizer
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            # ì˜¤í”„ë¼ì¸ ëª¨ë“œ ì‹œë„
            try:
                from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM
                
                # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                logger.info(f"[DEVICE] ì˜¤í”„ë¼ì¸ ëª¨ë¸ ë¡œë”© ë””ë°”ì´ìŠ¤: {device}")
                
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    cache_dir="./models",
                    use_fast=True,
                    local_files_only=True,
                    trust_remote_code=True
                )
                
                # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ë¡œë”© (ì˜¤í”„ë¼ì¸ ëª¨ë“œ, GPU/CPU ì§€ì›)
                if "alpaca" in model_name.lower() or "polyglot" in model_name.lower():
                    if device.type == "cuda":
                        model = AutoModelForCausalLM.from_pretrained(
                            model_name,
                            torch_dtype=torch.float16,
                            cache_dir="./models",
                            low_cpu_mem_usage=True,
                            device_map="auto",
                            local_files_only=True,
                            trust_remote_code=True
                        )
                    else:
                        model = AutoModelForCausalLM.from_pretrained(
                            model_name,
                            torch_dtype=torch.float32,
                            cache_dir="./models",
                            low_cpu_mem_usage=True,
                            local_files_only=True,
                            trust_remote_code=True
                        )
                else:
                    if device.type == "cuda":
                        model = AutoModelForSeq2SeqLM.from_pretrained(
                            model_name,
                            torch_dtype=torch.float16,
                            cache_dir="./models",
                            low_cpu_mem_usage=True,
                            device_map="auto",
                            local_files_only=True,
                            trust_remote_code=True
                        )
                    else:
                        model = AutoModelForSeq2SeqLM.from_pretrained(
                            model_name,
                            torch_dtype=torch.float32,
                            cache_dir="./models",
                            low_cpu_mem_usage=True,
                            local_files_only=True,
                            trust_remote_code=True
                        )
                
                # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì— ë“±ë¡
                model_tuple = (model, tokenizer)
                estimated_memory = self._estimate_model_memory(model_name)
                model_memory_manager.register_model(model_name, model_tuple, estimated_memory)
                
                self._models[model_name] = model
                self._tokenizers[model_name] = tokenizer
                
                logger.info(f"ì˜¤í”„ë¼ì¸ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name}")
                return model, tokenizer
                
            except Exception as offline_error:
                logger.error(f"ì˜¤í”„ë¼ì¸ ëª¨ë¸ ë¡œë”©ë„ ì‹¤íŒ¨ {model_name}: {offline_error}")
                raise
    
    def _estimate_model_memory(self, model_name: str) -> float:
        """ëª¨ë¸ë³„ ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (GB)"""
        model_memory_map = {
            "beomi/KoAlpaca-Polyglot-5.8B": 3.0,
            "jhgan/ko-sroberta-multitask": 0.5,
            "defog/sqlcoder-7b-2": 4.0
        }
        return model_memory_map.get(model_name, 1.0)
    
    def clear_models(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•œ ëª¨ë¸ í•´ì œ"""
        # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì˜ ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
        model_status = model_memory_manager.get_model_status()
        for model_name in model_status["loaded_models"]:
            model_memory_manager.unload_model(model_name)
        
        # ë¡œì»¬ ìºì‹œ ì •ë¦¬
        for model_name in list(self._models.keys()):
            del self._models[model_name]
            del self._tokenizers[model_name]
        self._models.clear()
        self._tokenizers.clear()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        logger.info("ëª¨ë“  ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")

# ì „ì—­ ëª¨ë¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
model_manager = ModelManager()

class OllamaLLMInterface:
    """Ollama ë¡œì»¬ ì„œë²„ë¥¼ í†µí•œ LLM ì¸í„°í˜ì´ìŠ¤ (Qwen ë“±)"""
    def __init__(self, model_name: str = "qwen2.5:latest", base_url: str = None, config: GenerationConfig = None):
        self.model_name = model_name
        # ê¸°ë³¸ URL ì •ê·œí™” (ë§ë‹¨ ìŠ¬ë˜ì‹œ ì œê±°)
        self.base_url = (base_url or os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")).rstrip("/")
        self.config = config or GenerationConfig()
        if not REQUESTS_AVAILABLE:
            raise ImportError("requests ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install requests")
    
    def generate(self, prompt: str) -> str:
        try:
            # Qwenì€ ëŒ€í™”í˜•ì´ì§€ë§Œ, ê°„ë‹¨íˆ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ generate ì‚¬ìš©
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": self.config.temperature,
                    "top_p": self.config.top_p,
                    "num_beams": self.config.num_beams,
                    "num_predict": self.config.max_length
                }
            }
            url = f"{self.base_url}/api/generate"
            resp = requests.post(url, data=json.dumps(payload), headers={"Content-Type": "application/json"}, timeout=120)
            if resp.status_code == 404:
                # ì¼ë¶€ í™˜ê²½ì—ì„œ /api/generate ë¯¸ì§€ì› â†’ /api/chatë¡œ í´ë°±
                chat_payload = {
                    "model": self.model_name,
                    "messages": [{"role": "user", "content": prompt}],
                    "stream": False,
                    "options": {
                        "temperature": self.config.temperature,
                        "top_p": self.config.top_p,
                        "num_beams": self.config.num_beams,
                        "num_predict": self.config.max_length
                    }
                }
                chat_url = f"{self.base_url}/api/chat"
                chat_resp = requests.post(chat_url, data=json.dumps(chat_payload), headers={"Content-Type": "application/json"}, timeout=120)
                chat_resp.raise_for_status()
                chat_data = chat_resp.json()
                # /api/chatëŠ” 'message' í˜¹ì€ 'response' ì‚¬ìš©
                if isinstance(chat_data, dict):
                    if "message" in chat_data and isinstance(chat_data["message"], dict):
                        return chat_data["message"].get("content", "")
                    return chat_data.get("response", "")
                return ""
            resp.raise_for_status()
            data = resp.json()
            # Ollama /api/generate ëŠ” 'response' í•„ë“œì— í…ìŠ¤íŠ¸ë¥¼ ë‹´ìŒ
            return data.get("response", "")
        except Exception as e:
            logger.error(f"Ollama ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

class LocalLLMInterface:
    """ìµœì í™”ëœ KoAlpaca-Polyglot-5.8B ì¸í„°í˜ì´ìŠ¤ (ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©)"""
    
    def __init__(self, model_name: str = "beomi/KoAlpaca-Polyglot-5.8B", config: GenerationConfig = None):
        self.model_name = model_name
        self.config = config or GenerationConfig()
        self.model = None
        self.tokenizer = None
        self._model_loaded = False
        
    def _ensure_model_loaded(self):
        """ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë¡œë“œ"""
        if not self._model_loaded:
            logger.info(f"ì§€ì—° ë¡œë”©: {self.model_name}")
            self.model, self.tokenizer = model_manager.get_model(self.model_name, "causal")
            self._model_loaded = True
        
    def generate(self, prompt: str) -> str:
        """KoAlpaca-Polyglot-1.3B ê¸°ë°˜ ì‹¤ì œ í…ìŠ¤íŠ¸ ìƒì„± (ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ ì ìš©)"""
        try:
            with memory_profiler(f"í…ìŠ¤íŠ¸ ìƒì„±: {self.model_name}"):
                # ëª¨ë¸ ì§€ì—° ë¡œë”© í™•ì¸
                self._ensure_model_loaded()
                
                # ì…ë ¥ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
                input_text = f"### ì§ˆë¬¸: {prompt}\n\n### ë‹µë³€:"
                
                # í† í¬ë‚˜ì´ì§• (causal ëª¨ë¸ìš©) - token_type_ids ì œê±°
                inputs = self.tokenizer(
                    input_text,
                    return_tensors="pt",
                    max_length=512,
                    truncation=True,
                    padding=True
                )
                
                # token_type_idsê°€ ìˆëŠ” ê²½ìš° ì œê±° (causal ëª¨ë¸ì—ì„œëŠ” ë¶ˆí•„ìš”)
                if 'token_type_ids' in inputs:
                    inputs.pop('token_type_ids')
                
                # GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ GPUë¡œ ì´ë™ (ê°œì„ ëœ ë²„ì „)
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                if device.type == "cuda":
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    self.model = self.model.to(device)
                    logger.debug(f"[GPU] ëª¨ë¸ê³¼ ì…ë ¥ì„ GPUë¡œ ì´ë™: {device}")
                else:
                    logger.debug(f"[CPU] CPUì—ì„œ ì‹¤í–‰: {device}")
                
                # ìƒì„± (causal ëª¨ë¸ìš©)
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=self.config.max_length,
                        temperature=self.config.temperature,
                        top_p=self.config.top_p,
                        do_sample=self.config.do_sample,
                        num_beams=self.config.num_beams,
                        early_stopping=self.config.early_stopping,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                
                # ë””ì½”ë”©
                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œê±°
                if "### ë‹µë³€:" in generated_text:
                    answer = generated_text.split("### ë‹µë³€:")[-1].strip()
                else:
                    answer = generated_text.strip()
                
                return answer
                
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

class AnswerGenerator:
    """ê²½ëŸ‰í™”ëœ ë‹µë³€ ìƒì„±ê¸° (ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©)"""
    
    def __init__(self, model_name: str = "qwen2:1.5b-instruct-q4_K_M", cache_enabled: bool = True, preload_models: bool = False):
        # Qwen(Ollama) ê³ ì •
        self.model_type = ModelType.OLLAMA.value
        # í™˜ê²½ë³€ìˆ˜ë¡œ ëª¨ë¸ëª… ë®ì–´ì“°ê¸° ê°€ëŠ¥ (ê¸°ë³¸: qwen2:1.5b-instruct-q4_K_M)
        self.model_name = os.getenv("MODEL_NAME", model_name)
        self.cache_enabled = cache_enabled
        
        # LLM ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™”: Ollama(Qwen)ë§Œ ì‚¬ìš©
        logger.info(f"Ollama(Qwen) ëª¨ë“œë¡œ ì´ˆê¸°í™”: MODEL_NAME={self.model_name}")
        self.llm = OllamaLLMInterface(self.model_name)
        
        # í”„ë¦¬ë¡œë“œëŠ” ì „ë¶€ ë¹„í™œì„±í™”
        # (OllamaëŠ” í•„ìš” ì‹œ ìë™ ì œê³µ, ë¡œì»¬/ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš© ì•ˆ í•¨)
        
        # íšŒì‚¬ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
        self.company_config = CompanyConfig()
        
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.prompt_template = {
            "context": """ë‹¤ìŒì€ êµí†µì •ë³´ ê´€ë ¨ ë¬¸ì„œì™€ ë°ì´í„°ì…ë‹ˆë‹¤:

{context}

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¥´ì„¸ìš”:
- í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ, 3ë¬¸ë‹¨ ì´ë‚´
- ëª©ë¡ì€ ìµœëŒ€ 3ê°œ í•­ëª©
- ì¤‘ë³µ í‘œí˜„/ë™ì–´ë°˜ë³µ/êµ°ë”ë”ê¸° ê¸ˆì§€
- ë¶ˆí•„ìš”í•œ ì´ëª¨ì§€/ê¸°í˜¸ ì‚¬ìš© ê¸ˆì§€
ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.""",
            "no_context": """ì‚¬ìš©ì ì§ˆë¬¸: {question}

ì„¸ì¢…ëŒ€í•™êµ êµí†µì •ë³´ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. êµí†µì •ë³´ì— ëŒ€í•´ ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.""",
            "summary": """ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ìš”ì•½í•´ì£¼ì„¸ìš”(í•µì‹¬ 3ë¬¸ì¥ ì´ë‚´, ëª©ë¡ ìµœëŒ€ 3ê°œ):

{context}

ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:"""
        }
        
        logger.info(f"ê²½ëŸ‰í™”ëœ ë‹µë³€ ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ: {model_name}")
    
    def _preload_models(self):
        """í•„ìš”í•œ ëª¨ë¸ë“¤ì„ ë¯¸ë¦¬ ë¡œë”©"""
        # ì™„ì „ ë¹„í™œì„±í™” (Qwen ê³ ì •)
        logger.info("í”„ë¦¬ë¡œë“œ ë¹„í™œì„±í™”: Qwen(Ollama)ë§Œ ì‚¬ìš©")
    
    def load_model(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ (ë¡œì»¬ LLMì€ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŒ)"""
        try:
            # ë¡œì»¬ LLMì€ ì´ë¯¸ ì´ˆê¸°í™” ì‹œ ë¡œë“œë˜ë¯€ë¡œ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
            logger.info(f"ë¡œì»¬ LLM ëª¨ë¸ {self.model_name} ë¡œë“œ ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def unload_model(self):
        """ê²½ëŸ‰í™”ëœ ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            if hasattr(self.llm, 'model'):
                del self.llm.model
            if hasattr(self.llm, 'tokenizer'):
                del self.llm.tokenizer
            import gc
            gc.collect()
            logger.info(f"ê²½ëŸ‰í™”ëœ ëª¨ë¸ {self.model_name} ì–¸ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def generate_direct_answer(self, question: str) -> Answer:
        """ë¬¸ì„œ/ë°ì´í„° ì—†ì´ íšŒì‚¬ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì§ì ‘ ë‹µë³€ ìƒì„±"""
        start_time = time.time()
        
        try:
            # íšŒì‚¬ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
            context_prompt = self.prompt_template["no_context"].format(question=question)
            
            # ë‹µë³€ ìƒì„±
            answer_text = self.llm.generate(context_prompt)
            
            generation_time = time.time() - start_time
            
            return Answer(
                content=answer_text,
                confidence=0.8,  # ì§ì ‘ ë‹µë³€ì€ ì‹ ë¢°ë„ê°€ ë‚®ìŒ
                sources=[],
                generation_time=generation_time,
                model_name=self.model_name,
                metadata={"answer_type": "direct", "context_used": False}
            )
            
        except Exception as e:
            logger.error(f"ì§ì ‘ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return Answer(
                content=f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                confidence=0.0,
                sources=[],
                generation_time=time.time() - start_time,
                model_name=self.model_name,
                metadata={"error": str(e)}
            )
    
    def generate_context_answer(self, question: str, context_chunks: List[TextChunk]) -> Answer:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        start_time = time.time()
        
        try:
            # ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ìƒì„± (ê°€ë…ì„± ìœ„í•´ ìƒìœ„ 3ê°œë¡œ ì œí•œ)
            trimmed_chunks = context_chunks[:3]
            context_text = "\n\n".join([chunk.content for chunk in trimmed_chunks])
            
            # ì‚¬ìš©ëœ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¡œê¹…
            used_keywords = self._extract_used_keywords(trimmed_chunks)
            if used_keywords:
                logger.info(f"ğŸ“š ë‹µë³€ ìƒì„±ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œ/ìë£Œ:")
                for keyword in used_keywords:
                    logger.info(f"  - {keyword}")
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.prompt_template["context"].format(
                context=context_text,
                question=question
            )
            
            # ë‹µë³€ ìƒì„±
            answer_text = self.llm.generate(prompt)
            
            generation_time = time.time() - start_time
            
            # ë‹µë³€ ê¸¸ì´ì™€ í¬ë§· ê°„ê²°í™”: ë¶ˆí•„ìš”í•œ ë¦¬ìŠ¤íŠ¸/ì¤‘ë³µ ì œê±°ëŠ” í”„ë¡¬í”„íŠ¸ ì„¤ê³„ë¡œ ìœ ë„ë¨
            return Answer(
                content=answer_text,
                confidence=0.9,  # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ì€ ì‹ ë¢°ë„ê°€ ë†’ìŒ
                sources=trimmed_chunks,
                generation_time=generation_time,
                model_name=self.model_name,
                metadata={"answer_type": "context", "context_used": True, "chunks_used": len(trimmed_chunks), "used_keywords": used_keywords}
            )
            
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return Answer(
                content=f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                confidence=0.0,
                sources=context_chunks,
                generation_time=time.time() - start_time,
                model_name=self.model_name,
                metadata={"error": str(e)}
            )
    
    def _extract_used_keywords(self, context_chunks: List[TextChunk]) -> List[str]:
        """ì»¨í…ìŠ¤íŠ¸ ì²­í¬ì—ì„œ ì‚¬ìš©ëœ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = set()
        
        for chunk in context_chunks:
            # ë©”íƒ€ë°ì´í„°ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            if chunk.metadata and "keywords" in chunk.metadata:
                chunk_keywords = chunk.metadata["keywords"]
                if isinstance(chunk_keywords, list):
                    keywords.update(chunk_keywords)
            
            # PDF IDì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
            if chunk.pdf_id:
                keywords.add(f"ë¬¸ì„œ: {chunk.pdf_id}")
            
            # ì²­í¬ ë‚´ìš©ì—ì„œ ì£¼ìš” ìš©ì–´ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
            content = chunk.content.lower()
            important_terms = [
                "êµí†µì‚¬ê³ ", "êµí†µëŸ‰", "ì‚¬ê³ ", "í†µê³„", "ë¶„ì„", "ë³´ê³ ì„œ", "ë°ì´í„°",
                "êµí†µ", "ì•ˆì „", "ì •ì±…", "ë²•ê·œ", "ê·œì •", "ì‹œìŠ¤í…œ", "ê´€ë¦¬"
            ]
            
            for term in important_terms:
                if term in content:
                    keywords.add(term)
        
        return sorted(list(keywords))
    
    def generate_summary(self, chunks: List[TextChunk]) -> Answer:
        start_time = time.time()
        
        try:
            # ìš”ì•½ìš© ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context_text = "\n\n".join([chunk.content for chunk in chunks])
            
            # ì‚¬ìš©ëœ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¡œê¹…
            used_keywords = self._extract_used_keywords(chunks)
            if used_keywords:
                logger.info(f"ğŸ“š ìš”ì•½ ìƒì„±ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œ/ìë£Œ:")
                for keyword in used_keywords:
                    logger.info(f"  - {keyword}")
            
            # ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.prompt_template["summary"].format(context=context_text)
            
            # ìš”ì•½ ìƒì„±
            summary_text = self.llm.generate(prompt)
            
            generation_time = time.time() - start_time
            
            return Answer(
                content=summary_text,
                confidence=0.85,
                sources=chunks,
                generation_time=generation_time,
                model_name=self.model_name,
                metadata={"answer_type": "summary", "chunks_summarized": len(chunks), "used_keywords": used_keywords}
            )
            
        except Exception as e:
            logger.error(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return Answer(
                content=f"ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                confidence=0.0,
                sources=chunks,
                generation_time=time.time() - start_time,
                model_name=self.model_name,
                metadata={"error": str(e)}
            )
